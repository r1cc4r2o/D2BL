{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / total\n",
    "    return train_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def evaluate(device, model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinW layers:\n",
      "\n",
      "Depth 0: LinW(in_features=256, out_features=256, bias=True)\n",
      "Depth 1: LinW(in_features=256, out_features=256, bias=True)\n",
      "Epoch 1/10, Training Loss: 1.3389, Training Accuracy: 53.04%, Test accuracy: 65.71%\n",
      "Epoch 2/10, Training Loss: 1.0873, Training Accuracy: 63.03%, Test accuracy: 63.16%\n",
      "Epoch 3/10, Training Loss: 1.0985, Training Accuracy: 62.67%, Test accuracy: 62.11%\n",
      "Epoch 4/10, Training Loss: 1.1364, Training Accuracy: 61.78%, Test accuracy: 61.58%\n",
      "Epoch 5/10, Training Loss: 1.1638, Training Accuracy: 61.03%, Test accuracy: 59.98%\n",
      "Epoch 6/10, Training Loss: 1.1696, Training Accuracy: 60.92%, Test accuracy: 60.02%\n",
      "Epoch 7/10, Training Loss: 1.1096, Training Accuracy: 63.38%, Test accuracy: 63.92%\n",
      "Epoch 8/10, Training Loss: 1.0814, Training Accuracy: 64.90%, Test accuracy: 65.40%\n",
      "Epoch 9/10, Training Loss: 1.0776, Training Accuracy: 64.94%, Test accuracy: 65.47%\n",
      "Epoch 10/10, Training Loss: 1.0470, Training Accuracy: 66.40%, Test accuracy: 63.66%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "def wd(layers: list()):\n",
    "    layers = np.array([layers]).flatten().reshape(-1, 1)\n",
    "    mask = layers != 0\n",
    "    layers = layers[mask].reshape(-1, 1)\n",
    "    #print(layers.shape)\n",
    "    # layers = np.array([layer*np.sqrt(depth+1) for depth, layer in enumerate(layers)]).flatten().reshape(-1, 1)\n",
    "    res = KernelDensity(kernel=\"gaussian\", bandwidth=0.2).fit(layers).sample([256])\n",
    "    # return torch.from_numpy(np.array(res, dtype=\"float32\"))\n",
    "    # return torch.from_numpy(np.array(np.array([1]), dtype=\"float32\"))\n",
    "    return torch.from_numpy(np.array(res, dtype=\"float32\"))\n",
    "    # return 1 - len(layers)/10 if len(layers)>0 else 1\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(784, 256)\n",
    "        self.l2 = LinW(in_features=256, out_features=256, depth=0)\n",
    "        self.l3 = LinW(in_features=256, out_features=256, depth=1, layers=[self.l2])\n",
    "        self.l4 = nn.Linear(256, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l2(x, repr))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, depth, layers=[]):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.depth = depth\n",
    "        self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "\n",
    "    def forward(self, input, prev=[]):\n",
    "        return F.linear(input * wd(prev).squeeze(1).to('cuda:0'), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinW layers:\n",
      "\n",
      "Depth 0: LinW(in_features=256, out_features=256, bias=True)\n",
      "Depth 1: LinW(in_features=256, out_features=256, bias=True)\n",
      "Epoch 1/10, Training Loss: 0.5012, Training Accuracy: 84.04%, Test accuracy: 93.12%\n",
      "Epoch 2/10, Training Loss: 0.2055, Training Accuracy: 93.80%, Test accuracy: 94.64%\n",
      "Epoch 3/10, Training Loss: 0.1557, Training Accuracy: 95.39%, Test accuracy: 95.81%\n",
      "Epoch 4/10, Training Loss: 0.1290, Training Accuracy: 96.19%, Test accuracy: 95.94%\n",
      "Epoch 5/10, Training Loss: 0.1170, Training Accuracy: 96.42%, Test accuracy: 96.36%\n",
      "Epoch 6/10, Training Loss: 0.1069, Training Accuracy: 96.82%, Test accuracy: 96.30%\n",
      "Epoch 7/10, Training Loss: 0.0939, Training Accuracy: 97.19%, Test accuracy: 96.57%\n",
      "Epoch 8/10, Training Loss: 0.0876, Training Accuracy: 97.41%, Test accuracy: 96.86%\n",
      "Epoch 9/10, Training Loss: 0.0845, Training Accuracy: 97.49%, Test accuracy: 96.58%\n",
      "Epoch 10/10, Training Loss: 0.0782, Training Accuracy: 97.69%, Test accuracy: 96.66%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "def wd(layers: list()):\n",
    "    layers = np.array([np.array(i.flatten()) for i in layers]).flatten()\n",
    "    mask = layers > 0\n",
    "    layers = layers[mask].reshape(-1, 1)\n",
    "    res = KernelDensity(kernel=\"gaussian\", bandwidth=0.2).fit(layers).sample([256])\n",
    "    return torch.from_numpy(np.array(res, dtype=\"float32\"))\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(784, 256)\n",
    "        self.l2 = LinW(in_features=256, out_features=256, depth=0)\n",
    "        self.l3 = LinW(in_features=256, out_features=256, depth=1, layers=[self.l2])\n",
    "        self.l4 = nn.Linear(256, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l2(x, repr))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, depth, layers=[]):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.depth = depth\n",
    "        self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "\n",
    "    def forward(self, input, prev=[]):\n",
    "        return F.linear(input * wd(prev).squeeze(1).to('cuda:0'), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinW layers:\n",
      "\n",
      "Depth 0: LinW(in_features=256, out_features=256, bias=True)\n",
      "Depth 1: LinW(in_features=256, out_features=256, bias=True)\n",
      "Epoch 1/10, Training Loss: 0.5038, Training Accuracy: 83.66%, Test accuracy: 92.97%\n",
      "Epoch 2/10, Training Loss: 0.1993, Training Accuracy: 94.02%, Test accuracy: 94.85%\n",
      "Epoch 3/10, Training Loss: 0.1565, Training Accuracy: 95.28%, Test accuracy: 95.44%\n",
      "Epoch 4/10, Training Loss: 0.1311, Training Accuracy: 96.10%, Test accuracy: 95.69%\n",
      "Epoch 5/10, Training Loss: 0.1141, Training Accuracy: 96.61%, Test accuracy: 96.53%\n",
      "Epoch 6/10, Training Loss: 0.1022, Training Accuracy: 96.98%, Test accuracy: 96.21%\n",
      "Epoch 7/10, Training Loss: 0.0972, Training Accuracy: 97.12%, Test accuracy: 96.91%\n",
      "Epoch 8/10, Training Loss: 0.0899, Training Accuracy: 97.29%, Test accuracy: 96.68%\n",
      "Epoch 9/10, Training Loss: 0.0782, Training Accuracy: 97.64%, Test accuracy: 96.65%\n",
      "Epoch 10/10, Training Loss: 0.0819, Training Accuracy: 97.50%, Test accuracy: 96.86%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "def wd(layers: list()):\n",
    "    layers = np.array([np.array(i.flatten()) for i in layers]).flatten()\n",
    "    mask = layers > 0\n",
    "    layers = layers[mask].reshape(-1, 1)\n",
    "    res = KernelDensity(kernel=\"gaussian\", bandwidth=0.2).fit(layers).sample([256])\n",
    "    return torch.from_numpy(np.array(res, dtype=\"float32\"))\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(784, 256)\n",
    "        self.l2 = LinW(in_features=256, out_features=256, depth=0)\n",
    "        self.l3 = LinW(in_features=256, out_features=256, depth=1, layers=[self.l2])\n",
    "        self.l4 = nn.Linear(256, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l2(x, repr))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, depth, layers=[]):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.depth = depth\n",
    "        self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "\n",
    "    def forward(self, input, prev=[]):\n",
    "        return F.linear(input, self.weight * wd(prev).squeeze(1).to('cuda:0'), self.bias)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinW layers:\n",
      "\n",
      "Depth 0: LinW(in_features=256, out_features=256, bias=True)\n",
      "Depth 1: LinW(in_features=256, out_features=256, bias=True)\n",
      "Epoch 1/10, Training Loss: 0.6480, Training Accuracy: 78.64%, Test accuracy: 89.76%\n",
      "Epoch 2/10, Training Loss: 0.2932, Training Accuracy: 91.19%, Test accuracy: 92.56%\n",
      "Epoch 3/10, Training Loss: 0.2182, Training Accuracy: 93.32%, Test accuracy: 94.18%\n",
      "Epoch 4/10, Training Loss: 0.1826, Training Accuracy: 94.45%, Test accuracy: 95.05%\n",
      "Epoch 5/10, Training Loss: 0.1542, Training Accuracy: 95.28%, Test accuracy: 95.39%\n",
      "Epoch 6/10, Training Loss: 0.1376, Training Accuracy: 95.93%, Test accuracy: 95.44%\n",
      "Epoch 7/10, Training Loss: 0.1241, Training Accuracy: 96.13%, Test accuracy: 95.70%\n",
      "Epoch 8/10, Training Loss: 0.1157, Training Accuracy: 96.50%, Test accuracy: 96.21%\n",
      "Epoch 9/10, Training Loss: 0.1034, Training Accuracy: 96.84%, Test accuracy: 95.83%\n",
      "Epoch 10/10, Training Loss: 0.0970, Training Accuracy: 96.93%, Test accuracy: 96.42%\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Selecting randomly from the layers activations \n",
    "    Filter out the positive values\n",
    "\n",
    "    The network converge but still reach better \n",
    "    accuracy compared to the one with \n",
    "    kernel density estimation\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "def wd(layers: list()):\n",
    "    layers = np.array([np.array(i.flatten()) for i in layers]).flatten()\n",
    "    mask = layers < 0\n",
    "    layers = layers[mask]\n",
    "    res = layers[np.random.randint(0, high = layers.shape[0]-1, size = 256)].reshape(-1, 1)\n",
    "    return torch.from_numpy(np.array(res, dtype=\"float32\"))\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(784, 256)\n",
    "        self.l2 = LinW(in_features=256, out_features=256, depth=0)\n",
    "        self.l3 = LinW(in_features=256, out_features=256, depth=1, layers=[self.l2])\n",
    "        self.l4 = nn.Linear(256, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l2(x, repr))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, depth, layers=[]):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.depth = depth\n",
    "        self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "\n",
    "    def forward(self, input, prev=[]):\n",
    "        return F.linear(input, self.weight * wd(prev).squeeze(1).to('cuda:0'), self.bias)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinW layers:\n",
      "\n",
      "Depth 0: LinW(in_features=256, out_features=256, bias=True)\n",
      "Depth 1: LinW(in_features=256, out_features=256, bias=True)\n",
      "Epoch 1/10, Training Loss: 0.4526, Training Accuracy: 85.59%, Test accuracy: 92.92%\n",
      "Epoch 2/10, Training Loss: 0.1994, Training Accuracy: 94.16%, Test accuracy: 95.12%\n",
      "Epoch 3/10, Training Loss: 0.1511, Training Accuracy: 95.48%, Test accuracy: 96.00%\n",
      "Epoch 4/10, Training Loss: 0.1255, Training Accuracy: 96.24%, Test accuracy: 96.37%\n",
      "Epoch 5/10, Training Loss: 0.1119, Training Accuracy: 96.67%, Test accuracy: 96.21%\n",
      "Epoch 6/10, Training Loss: 0.0980, Training Accuracy: 97.03%, Test accuracy: 96.57%\n",
      "Epoch 7/10, Training Loss: 0.0951, Training Accuracy: 97.18%, Test accuracy: 96.40%\n",
      "Epoch 8/10, Training Loss: 0.0854, Training Accuracy: 97.45%, Test accuracy: 96.67%\n",
      "Epoch 9/10, Training Loss: 0.0796, Training Accuracy: 97.63%, Test accuracy: 96.71%\n",
      "Epoch 10/10, Training Loss: 0.0776, Training Accuracy: 97.64%, Test accuracy: 96.83%\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Selecting randomly from the layers activations \n",
    "    Filter out the negative values\n",
    "\n",
    "    The network converge, however, the accuracy is\n",
    "    lower than the one with kernel density estimation\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "def wd(layers: list()):\n",
    "    layers = np.array([np.array(i.flatten()) for i in layers]).flatten()\n",
    "    mask = layers > 0\n",
    "    layers = layers[mask]\n",
    "    res = layers[np.random.randint(0, high = layers.shape[0]-1, size = 256)].reshape(-1, 1)\n",
    "    return torch.from_numpy(np.array(res, dtype=\"float32\"))\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(784, 256)\n",
    "        self.l2 = LinW(in_features=256, out_features=256, depth=0)\n",
    "        self.l3 = LinW(in_features=256, out_features=256, depth=1, layers=[self.l2])\n",
    "        self.l4 = nn.Linear(256, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l2(x, repr))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, depth, layers=[]):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.depth = depth\n",
    "        self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "\n",
    "    def forward(self, input, prev=[]):\n",
    "        return F.linear(input, self.weight * wd(prev).squeeze(1).to('cuda:0'), self.bias)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "LinW layers:\n",
      "\n",
      "Depth 0: LinW(in_features=256, out_features=256, bias=True)\n",
      "Depth 1: LinW(in_features=256, out_features=256, bias=True)\n",
      "Epoch 1/10, Training Loss: 2.3640, Training Accuracy: 10.02%, Test accuracy: 10.00%\n",
      "Epoch 2/10, Training Loss: 2.3029, Training Accuracy: 10.03%, Test accuracy: 10.00%\n",
      "Epoch 3/10, Training Loss: 2.3029, Training Accuracy: 9.88%, Test accuracy: 9.99%\n",
      "Epoch 4/10, Training Loss: 2.3028, Training Accuracy: 9.97%, Test accuracy: 10.00%\n",
      "Epoch 5/10, Training Loss: 2.3029, Training Accuracy: 9.78%, Test accuracy: 9.94%\n",
      "Epoch 6/10, Training Loss: 2.3029, Training Accuracy: 9.79%, Test accuracy: 10.00%\n",
      "Epoch 7/10, Training Loss: 2.3028, Training Accuracy: 10.04%, Test accuracy: 10.00%\n",
      "Epoch 8/10, Training Loss: 2.3029, Training Accuracy: 9.94%, Test accuracy: 10.00%\n",
      "Epoch 9/10, Training Loss: 2.3029, Training Accuracy: 9.92%, Test accuracy: 10.00%\n",
      "Epoch 10/10, Training Loss: 2.3029, Training Accuracy: 9.60%, Test accuracy: 10.00%\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Selecting randomly from the layers activations \n",
    "    without any activation filtering approach\n",
    "\n",
    "    The evidence show that the model is not learning\n",
    "    and the accuracy is not improving\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "def wd(layers: list()):\n",
    "    layers = np.array([np.array(i.flatten()) for i in layers]).flatten()\n",
    "    # mask = layers > 0\n",
    "    # layers = layers[mask]\n",
    "    res = layers[np.random.randint(0, high = layers.shape[0]-2, size = 256)].reshape(-1, 1)\n",
    "    return torch.from_numpy(np.array(res, dtype=\"float32\"))\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, 256)\n",
    "        self.l2 = LinW(in_features=256, out_features=256, depth=0)\n",
    "        self.l3 = LinW(in_features=256, out_features=256, depth=1, layers=[self.l2])\n",
    "        self.l4 = nn.Linear(256, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l2(x, repr))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, depth, layers=[]):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.depth = depth\n",
    "        self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "\n",
    "    def forward(self, input, prev=[]):\n",
    "        return F.linear(input * wd(prev).squeeze(1).to('cuda:0'), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "LinW layers:\n",
      "\n",
      "Depth 0: LinW(in_features=256, out_features=256, bias=True)\n",
      "Depth 1: LinW(in_features=256, out_features=256, bias=True)\n",
      "Epoch 1/10, Training Loss: 2.1189, Training Accuracy: 19.49%, Test accuracy: 24.11%\n",
      "Epoch 2/10, Training Loss: 1.9967, Training Accuracy: 24.88%, Test accuracy: 26.07%\n",
      "Epoch 3/10, Training Loss: 1.9454, Training Accuracy: 27.92%, Test accuracy: 29.07%\n",
      "Epoch 4/10, Training Loss: 1.9000, Training Accuracy: 30.44%, Test accuracy: 31.48%\n",
      "Epoch 5/10, Training Loss: 1.8667, Training Accuracy: 31.60%, Test accuracy: 32.62%\n",
      "Epoch 6/10, Training Loss: 1.8423, Training Accuracy: 32.65%, Test accuracy: 34.03%\n",
      "Epoch 7/10, Training Loss: 1.8235, Training Accuracy: 33.38%, Test accuracy: 33.37%\n",
      "Epoch 8/10, Training Loss: 1.8031, Training Accuracy: 34.34%, Test accuracy: 35.02%\n",
      "Epoch 9/10, Training Loss: 1.7879, Training Accuracy: 34.94%, Test accuracy: 35.54%\n",
      "Epoch 10/10, Training Loss: 1.7748, Training Accuracy: 35.24%, Test accuracy: 36.15%\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Selecting randomly from the layers activations \n",
    "    filtering the negative activations\n",
    "\n",
    "    It seems that the network still converge but the\n",
    "    accuracy isn't better than the backprop approch\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "def wd(layers: list()):\n",
    "    layers = np.array([np.array(i.flatten()) for i in layers]).flatten()\n",
    "    mask = layers < 0\n",
    "    layers = layers[mask]\n",
    "    res = layers[np.random.randint(0, high = layers.shape[0]-1, size = 256)].reshape(-1, 1)\n",
    "    return torch.from_numpy(np.array(res, dtype=\"float32\"))\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, 256)\n",
    "        self.l2 = LinW(in_features=256, out_features=256, depth=0)\n",
    "        self.l3 = LinW(in_features=256, out_features=256, depth=1, layers=[self.l2])\n",
    "        self.l4 = nn.Linear(256, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l2(x, repr))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, depth, layers=[]):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.depth = depth\n",
    "        self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "\n",
    "    def forward(self, input, prev=[]):\n",
    "        return F.linear(input * wd(prev).squeeze(1).to('cuda:0'), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "def wd(layers: list()):\n",
    "    layers = np.array([np.array(i.flatten()) for i in layers]).flatten()\n",
    "    mask = layers > 0\n",
    "    layers = layers[mask].reshape(-1, 1)\n",
    "    #print(layers.shape)\n",
    "    # layers = np.array([layer*np.sqrt(depth+1) for depth, layer in enumerate(layers)]).flatten().reshape(-1, 1)\n",
    "    res = KernelDensity(kernel=\"gaussian\", bandwidth=0.2).fit(layers).sample([256])\n",
    "    # return torch.from_numpy(np.array(res, dtype=\"float32\"))\n",
    "    # return torch.from_numpy(np.array(np.array([1]), dtype=\"float32\"))\n",
    "    return torch.from_numpy(np.array(res, dtype=\"float32\"))\n",
    "    # return 1 - len(layers)/10 if len(layers)>0 else 1\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, 256)\n",
    "        self.l2 = LinW(in_features=256, out_features=256, depth=0)\n",
    "        self.l3 = LinW(in_features=256, out_features=256, depth=1, layers=[self.l2])\n",
    "        self.l4 = nn.Linear(256, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l2(x, repr))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, depth, layers=[]):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.depth = depth\n",
    "        self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "\n",
    "    def forward(self, input, prev=[]):\n",
    "        return F.linear(input, self.weight * wd(prev).squeeze(1).to('cuda:0'), self.bias)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 0.2664, Training Accuracy: 91.96%, Test accuracy: 95.56%\n",
      "Epoch 2/10, Training Loss: 0.1017, Training Accuracy: 96.92%, Test accuracy: 97.26%\n",
      "Epoch 3/10, Training Loss: 0.0686, Training Accuracy: 97.84%, Test accuracy: 97.71%\n",
      "Epoch 4/10, Training Loss: 0.0496, Training Accuracy: 98.42%, Test accuracy: 97.56%\n",
      "Epoch 5/10, Training Loss: 0.0399, Training Accuracy: 98.70%, Test accuracy: 97.83%\n",
      "Epoch 6/10, Training Loss: 0.0308, Training Accuracy: 98.99%, Test accuracy: 97.61%\n",
      "Epoch 7/10, Training Loss: 0.0244, Training Accuracy: 99.17%, Test accuracy: 97.47%\n",
      "Epoch 8/10, Training Loss: 0.0233, Training Accuracy: 99.24%, Test accuracy: 97.71%\n",
      "Epoch 9/10, Training Loss: 0.0194, Training Accuracy: 99.39%, Test accuracy: 97.82%\n",
      "Epoch 10/10, Training Loss: 0.0171, Training Accuracy: 99.43%, Test accuracy: 98.06%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(784, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, 256)\n",
    "        self.l4 = nn.Linear(256, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        x = self.gelu(self.l2(x))\n",
    "        x = self.gelu(self.l3(x))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLP().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/10, Training Loss: 1.8242, Training Accuracy: 33.66%, Test accuracy: 38.77%\n",
      "Epoch 2/10, Training Loss: 1.6521, Training Accuracy: 40.46%, Test accuracy: 42.90%\n",
      "Epoch 3/10, Training Loss: 1.5589, Training Accuracy: 43.71%, Test accuracy: 43.64%\n",
      "Epoch 4/10, Training Loss: 1.4902, Training Accuracy: 46.38%, Test accuracy: 46.61%\n",
      "Epoch 5/10, Training Loss: 1.4421, Training Accuracy: 48.10%, Test accuracy: 47.41%\n",
      "Epoch 6/10, Training Loss: 1.4022, Training Accuracy: 49.61%, Test accuracy: 48.92%\n",
      "Epoch 7/10, Training Loss: 1.3659, Training Accuracy: 50.90%, Test accuracy: 50.17%\n",
      "Epoch 8/10, Training Loss: 1.3299, Training Accuracy: 52.06%, Test accuracy: 49.90%\n",
      "Epoch 9/10, Training Loss: 1.2966, Training Accuracy: 53.28%, Test accuracy: 49.69%\n",
      "Epoch 10/10, Training Loss: 1.2629, Training Accuracy: 54.69%, Test accuracy: 50.50%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, 256)\n",
    "        self.l4 = nn.Linear(256, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        x = self.gelu(self.l2(x))\n",
    "        x = self.gelu(self.l3(x))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLP().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
