{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MHA attention mechanism applied over the activations of the previous layer\n",
    "\n",
    "For the seek of those experiment I assume that all the layer have the same number of neurons. In this way we can apply directly the MHA without sampling the activations. This assumption simplify the implementation and the understanding of the results. However, it would be possible to apply this mechanism even if in between layers the number of neurons changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Without softmax: \n",
      "tensor([-2.8145,  9.0366,  3.9772,  3.2464,  1.7831,  4.6573,  4.3377,  1.5610])\n",
      "tensor([-2.8145,  9.0366,  3.9772,  3.2464,  1.7831,  4.6573,  4.3377,  1.5610])\n",
      "\n",
      "With softmax: \n",
      "tensor([0.3660, 1.0815, 1.0566, 1.1317, 0.9139, 0.5136, 0.7670, 0.5848])\n",
      "tensor([0.3660, 1.0815, 1.0566, 1.1317, 0.9139, 0.5136, 0.7670, 0.5848])\n"
     ]
    }
   ],
   "source": [
    "# As we could see here we parallelize over the number of heads\n",
    "\n",
    "# shape: (batch_size, num_heads, num_layer, activation_size)\n",
    "attn = torch.randn(16, 10, 8, 8)\n",
    "\n",
    "# activations in the current layer\n",
    "a = torch.randn(16, 10, 8, 1)\n",
    "\n",
    "print()\n",
    "print('Without softmax: ')\n",
    "# check that the matrix multiplication is correct\n",
    "print(((torch.matmul( \n",
    "                    attn , \n",
    "                    attn.transpose(-1,-2)\n",
    "            )/torch.sqrt(torch.tensor(8))    \n",
    "        ) @ a).squeeze(-1)[0][0])\n",
    "\n",
    "\n",
    "print(((attn[0][0]@attn[0][0].T/torch.sqrt(torch.tensor(8))) @ a[0][0]).squeeze(-1))\n",
    "\n",
    "\n",
    "print()\n",
    "print('With softmax: ')\n",
    "# check that the matrix multiplication is correct\n",
    "# with the softmax on the last dimension\n",
    "print((nn.Softmax(dim=-1)(\n",
    "                torch.matmul( \n",
    "                            attn , \n",
    "                            attn.transpose(-1,-2)\n",
    "                )/torch.sqrt(torch.tensor(8))    \n",
    "            ) @ a).squeeze(-1)[0][0])\n",
    "\n",
    "print((nn.Softmax(dim=-1)(\n",
    "                torch.matmul( \n",
    "                            attn[0][0] , \n",
    "                            attn[0][0].transpose(-1,-2)\n",
    "                )/torch.sqrt(torch.tensor(8))    \n",
    "            ) @ a[0][0]).squeeze(-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.1409, -1.6327],\n",
      "         [-0.9834, -1.2864],\n",
      "         [-0.5916,  0.3915],\n",
      "         [-0.3678,  0.4640]]])\n",
      "torch.Size([1, 4, 2])\n",
      "tensor([[ 1.1409, -1.6327, -0.9834, -1.2864, -0.5916,  0.3915, -0.3678,  0.4640]])\n"
     ]
    }
   ],
   "source": [
    "# reshape to finally flatten over the number of heads\n",
    "# and get the final result of the attention\n",
    "a = torch.randn(1, 4, 2)\n",
    "\n",
    "print(a)\n",
    "print(a.shape)\n",
    "\n",
    "print(a.reshape(1, 2*4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def head_batched_attention_mechanism(Q, K, V):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Q: (batch_size, num_heads, num_layer, activation_size)\n",
    "        K: (batch_size, num_heads, num_layer, activation_size)\n",
    "        V: (batch_size, num_heads, activation_size, 1) # activations in the current layer\n",
    "\n",
    "    Returns:\n",
    "        attention: (batch_size, num_heads, activation_size)\n",
    "\n",
    "        # attention mechanism\n",
    "        # # (batch_size, num_heads, activation_size, activation_size)\n",
    "        # attention = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        # attention = attention / torch.sqrt(torch.tensor(activation_size).float())\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = nn.Softmax(dim=-1)(attention)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = torch.matmul(attention, V)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size)\n",
    "        # attention = attention.squeeze(-1)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return (nn.Softmax(dim=-1)(\n",
    "                torch.matmul( \n",
    "                            Q , \n",
    "                            K.transpose(-1,-2)\n",
    "                )/torch.sqrt(torch.tensor(8))    \n",
    "            ) @ V).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "# shape: (batch_size, num_heads, num_layer, activation_size)\n",
    "attn = torch.randn(16, 10, 8, 8)\n",
    "\n",
    "# activations in the current layer\n",
    "a = torch.randn(16, 10, 8, 1)\n",
    "\n",
    "nn.Linear(80,8)(head_batched_attention_mechanism(attn, attn, a).reshape(16, 10*8)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def head_batched_attention_mechanism(Q, K, V):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Q: (batch_size, num_heads, num_layer, activation_size)\n",
    "        K: (batch_size, num_heads, num_layer, activation_size)\n",
    "        V: (batch_size, num_heads, activation_size, 1) # activations in the current layer\n",
    "\n",
    "    Returns:\n",
    "        attention: (batch_size, num_heads, activation_size)\n",
    "\n",
    "        # attention mechanism\n",
    "        # # (batch_size, num_heads, activation_size, activation_size)\n",
    "        # attention = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        # attention = attention / torch.sqrt(torch.tensor(activation_size).float())\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = nn.Softmax(dim=-1)(attention)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = torch.matmul(attention, V)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size)\n",
    "        # attention = attention.squeeze(-1)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return (nn.Softmax(dim=-1)(\n",
    "                torch.matmul( \n",
    "                            Q , \n",
    "                            K.transpose(-1,-2)\n",
    "                )/torch.sqrt(torch.tensor(8))    \n",
    "            ) @ V).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_emb, n_head) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert dim_emb % n_head == 0, 'dim_emb must be divisible by n_head'\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.W_Q = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W_K = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W_V = nn.Linear(dim_emb, dim_emb)\n",
    "\n",
    "        self.W_O = nn.Linear(dim_emb*n_head, dim_emb)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # get the shape of the input\n",
    "        batch_size, activation_size, activation_size = Q.size()\n",
    "        \n",
    "        # check the device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # move to device\n",
    "        Q.to(device)\n",
    "        K.to(device)\n",
    "        V.to(device)\n",
    "        \n",
    "        # reshape Q, K, V\n",
    "        # parallelize over the number of heads\n",
    "        # (batch_size, num_heads, num_layer, activation_size)\n",
    "        Q = torch.stack([Q for _ in range(self.n_head)], 1)\n",
    "        K = torch.stack([K for _ in range(self.n_head)], 1)\n",
    "        V = torch.stack([V for _ in range(self.n_head)], 1)\n",
    "\n",
    "        # apply linear transformation\n",
    "        Q = self.W_Q(Q)\n",
    "        K = self.W_K(K)\n",
    "        V = self.W_V(V).reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        \n",
    "        # apply attention mechanism\n",
    "        out_attention = head_batched_attention_mechanism(Q, K, V).reshape(batch_size, self.n_head*activation_size)\n",
    "\n",
    "        # apply linear transformation\n",
    "        return self.W_O(out_attention)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MultiHeadAttention(8, 4)(torch.randn(16, 8, 8), torch.randn(16, 8, 8), torch.randn(16, 8)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 10, 8, 8])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2142, -0.2232],\n",
      "         [ 0.4802, -0.6151],\n",
      "         [ 1.0031,  0.8155],\n",
      "         [ 1.4394,  0.9198]]])\n",
      "torch.Size([1, 4, 2])\n",
      "tensor([[-0.2142, -0.2232,  0.4802, -0.6151,  1.0031,  0.8155,  1.4394,  0.9198]])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def head_batched_attention_mechanism(Q, K, V):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Q: (batch_size, num_heads, num_layer, activation_size)\n",
    "        K: (batch_size, num_heads, num_layer, activation_size)\n",
    "        V: (batch_size, num_heads, activation_size, 1) # activations in the current layer\n",
    "\n",
    "    Returns:\n",
    "        attention: (batch_size, num_heads, activation_size)\n",
    "\n",
    "        # attention mechanism\n",
    "        # # (batch_size, num_heads, activation_size, activation_size)\n",
    "        # attention = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        # attention = attention / torch.sqrt(torch.tensor(activation_size).float())\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = nn.Softmax(dim=-1)(attention)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = torch.matmul(attention, V)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size)\n",
    "        # attention = attention.squeeze(-1)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return (nn.Softmax(dim=-1)(\n",
    "                torch.matmul( \n",
    "                            Q.transpose(-1,-2) , \n",
    "                            K\n",
    "                )/torch.sqrt(torch.tensor(8))    \n",
    "            ) @ V).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_emb, n_head) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert dim_emb % n_head == 0, 'dim_emb must be divisible by n_head'\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.W_Q = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W_K = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W_V = nn.Linear(dim_emb, dim_emb)\n",
    "\n",
    "        self.W_O = nn.Linear(dim_emb*n_head, dim_emb)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # get the shape of the input\n",
    "        batch_size, activation_size, activation_size = Q.size()\n",
    "        \n",
    "        # check the device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # move to device\n",
    "        Q.to(device)\n",
    "        K.to(device)\n",
    "        V.to(device)\n",
    "        \n",
    "        # reshape Q, K, V\n",
    "        # parallelize over the number of heads\n",
    "        # (batch_size, num_heads, num_layer, activation_size)\n",
    "        Q = torch.stack([Q for _ in range(self.n_head)], 1)\n",
    "        K = torch.stack([K for _ in range(self.n_head)], 1)\n",
    "        V = torch.stack([V for _ in range(self.n_head)], 1)\n",
    "\n",
    "        # apply linear transformation\n",
    "        Q = self.W_Q(Q)\n",
    "        K = self.W_K(K)\n",
    "        V = self.W_V(V).reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        \n",
    "        # apply attention mechanism\n",
    "        out_attention = head_batched_attention_mechanism(Q, K, V).reshape(batch_size, self.n_head*activation_size)\n",
    "\n",
    "        # apply linear transformation\n",
    "        return self.W_O(out_attention)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "\n",
    "# # I have the batched activations for each layer for each sample\n",
    "# layers = torch.stack([torch.randn(batch_size, number_activations) for _ in range(10)])\n",
    "# print(layers.shape, '(num_layers, batch_size, number_activations)')\n",
    "\n",
    "# # I get the activations for each layer for each sample\n",
    "# obj_activations = torch.stack([layers[:,i,:] for i in range(layers.shape[1])])\n",
    "# print(obj_activations.shape, '(nr_object, num_layers, activation_for_each_layer)')\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "number_activations = 8\n",
    "\n",
    "a = torch.randn(batch_size, number_activations)\n",
    "\n",
    "\n",
    "attn = get_layer_activations([torch.randn(batch_size, number_activations) for _ in range(10)])\n",
    "\n",
    "\n",
    "MultiHeadAttention(8, 4)(attn, attn, a)\n",
    "\n",
    "# list(extract_activations_per_sample(\n",
    "#             extract_activations_layers(layers), \n",
    "#             mask=False\n",
    "#         )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations = [torch.randn(batch_size, number_activations) for _ in range(10)]\n",
    "\n",
    "activations[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinW layers:\n",
      "\n",
      "Depth 0: LinW(\n",
      "  in_features=8, out_features=8, bias=True\n",
      "  (mha): MultiHeadAttention(\n",
      "    (W_Q): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (W_K): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (W_V): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (W_O): Linear(in_features=32, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "Depth 1: LinW(\n",
      "  in_features=8, out_features=8, bias=True\n",
      "  (mha): MultiHeadAttention(\n",
      "    (W_Q): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (W_K): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (W_V): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (W_O): Linear(in_features=32, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1/10, Training Loss: 0.8250, Training Accuracy: 71.66%, Test accuracy: 85.08%\n",
      "Epoch 2/10, Training Loss: 0.4606, Training Accuracy: 86.76%, Test accuracy: 87.75%\n",
      "Epoch 3/10, Training Loss: 0.3847, Training Accuracy: 89.09%, Test accuracy: 89.36%\n",
      "Epoch 4/10, Training Loss: 0.3413, Training Accuracy: 90.31%, Test accuracy: 90.35%\n",
      "Epoch 5/10, Training Loss: 0.3099, Training Accuracy: 91.29%, Test accuracy: 90.98%\n",
      "Epoch 6/10, Training Loss: 0.2909, Training Accuracy: 91.78%, Test accuracy: 91.74%\n",
      "Epoch 7/10, Training Loss: 0.2741, Training Accuracy: 92.22%, Test accuracy: 92.02%\n",
      "Epoch 8/10, Training Loss: 0.2630, Training Accuracy: 92.59%, Test accuracy: 92.19%\n",
      "Epoch 9/10, Training Loss: 0.2527, Training Accuracy: 92.87%, Test accuracy: 92.36%\n",
      "Epoch 10/10, Training Loss: 0.2471, Training Accuracy: 93.00%, Test accuracy: 92.57%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(784, 8)\n",
    "        self.l2 = LinW(in_features=8, out_features=8, depth=0)\n",
    "        self.l3 = LinW(in_features=8, out_features=8, depth=1, layers=[self.l2])\n",
    "        self.l4 = nn.Linear(8, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.l1(x)\n",
    "        repr.append(x)\n",
    "        x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        # x = self.gelu(self.l2(x, repr))\n",
    "        x = self.l2(x, repr)\n",
    "        repr.append(x)\n",
    "        x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, depth, layers=[]):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.depth = depth\n",
    "        self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "        self.mha = MultiHeadAttention(in_features, 4)\n",
    "\n",
    "    def forward(self, input, activations=[]):\n",
    "        activations = get_layer_activations(activations)\n",
    "        return F.linear(self.mha(activations, activations, input), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinW layers:\n",
      "\n",
      "Depth 0: LinW(\n",
      "  in_features=8, out_features=8, bias=True\n",
      "  (mha): MultiHeadAttention(\n",
      "    (W_Q): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (W_K): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (W_V): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (W_O): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "Depth 1: LinW(\n",
      "  in_features=8, out_features=8, bias=True\n",
      "  (mha): MultiHeadAttention(\n",
      "    (W_Q): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (W_K): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (W_V): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (W_O): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1/5, Training Loss: 0.9245, Training Accuracy: 69.55%, Test accuracy: 83.89%\n",
      "Epoch 2/5, Training Loss: 0.4443, Training Accuracy: 87.33%, Test accuracy: 88.97%\n",
      "Epoch 3/5, Training Loss: 0.3844, Training Accuracy: 89.08%, Test accuracy: 90.16%\n",
      "Epoch 4/5, Training Loss: 0.3439, Training Accuracy: 90.35%, Test accuracy: 95.19%\n",
      "Epoch 5/5, Training Loss: 0.0488, Training Accuracy: 91.01%, Test accuracy: 98.43%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(784, 8)\n",
    "        self.l2 = LinW(in_features=8, out_features=8, depth=0)\n",
    "        self.l3 = LinW(in_features=8, out_features=8, depth=1, layers=[self.l2])\n",
    "        self.l4 = nn.Linear(8, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.l1(x)\n",
    "        repr.append(x)\n",
    "        x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        # x = self.gelu(self.l2(x, repr))\n",
    "        x = self.l2(x, repr)\n",
    "        repr.append(x)\n",
    "        x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, depth, layers=[]):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.depth = depth\n",
    "        self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "        self.mha = MultiHeadAttention(in_features, 2)\n",
    "\n",
    "    def forward(self, input, activations=[]):\n",
    "        activations = get_layer_activations(activations)\n",
    "        return F.linear(self.mha(activations, activations, input), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b75c26b910ba516c930507b4337d6cf30fdc3d06c5392aedae0a97adcea4b39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
