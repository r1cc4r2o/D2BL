{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing a mechanism of mutations and crossover only on the attention maps \n",
    "\n",
    "Here we introduce a mechanism of mutations and crossover only on the attention maps. The idea is to propose a mechanism which evolve individually the attention map of each head obtained in the previous notebook by using crossover and mutations. The key idea is that we use those attention maps to compute the activations of the next layer. The operation is made by making a weighted average of the activatition in the previous layer where the weights are the attention maps (where we applied mutations and crossover). The objective is to evolve the attention maps in order to obtain a better representation of the input, exploiting faster weights that lead to speeding up the convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def head_batched_attention_mechanism(Q, K, V):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Q: (batch_size, num_heads, num_layer, activation_size)\n",
    "        K: (batch_size, num_heads, num_layer, activation_size)\n",
    "        V: (batch_size, num_heads, activation_size, 1) # activations in the current layer\n",
    "\n",
    "    Returns:\n",
    "        attention: (batch_size, num_heads, activation_size)\n",
    "\n",
    "        # attention mechanism\n",
    "        # # (batch_size, num_heads, activation_size, activation_size)\n",
    "        # attention = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        # attention = attention / torch.sqrt(torch.tensor(activation_size).float())\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = nn.Softmax(dim=-1)(attention)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = torch.matmul(attention, V)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size)\n",
    "        # attention = attention.squeeze(-1)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return (nn.Softmax(dim=-1)(\n",
    "                torch.matmul( \n",
    "                            Q , \n",
    "                            K.transpose(-1,-2)\n",
    "                )/torch.sqrt(torch.tensor(8))    \n",
    "            ) @ V).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_emb, n_head) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert dim_emb % n_head == 0, 'dim_emb must be divisible by n_head'\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.W_Q = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W_K = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W_V = nn.Linear(dim_emb, dim_emb)\n",
    "\n",
    "        self.W_O = nn.Linear(dim_emb*n_head, dim_emb)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # get the shape of the input\n",
    "        batch_size, activation_size, activation_size = Q.size()\n",
    "        \n",
    "        # check the device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # move to device\n",
    "        Q.to(device)\n",
    "        K.to(device)\n",
    "        V.to(device)\n",
    "        \n",
    "        # reshape Q, K, V\n",
    "        # parallelize over the number of heads\n",
    "        # (batch_size, num_heads, num_layer, activation_size)\n",
    "        Q = torch.stack([Q for _ in range(self.n_head)], 1)\n",
    "        K = torch.stack([K for _ in range(self.n_head)], 1)\n",
    "        V = torch.stack([V for _ in range(self.n_head)], 1)\n",
    "\n",
    "        # apply linear transformation\n",
    "        Q = self.W_Q(Q)\n",
    "        K = self.W_K(K)\n",
    "        V = self.W_V(V).reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        \n",
    "        # apply attention mechanism\n",
    "        out_attention = head_batched_attention_mechanism(Q, K, V).reshape(batch_size, self.n_head*activation_size)\n",
    "\n",
    "        # apply linear transformation\n",
    "        return self.W_O(out_attention)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "\n",
    "# # I have the batched activations for each layer for each sample\n",
    "# layers = torch.stack([torch.randn(batch_size, number_activations) for _ in range(10)])\n",
    "# print(layers.shape, '(num_layers, batch_size, number_activations)')\n",
    "\n",
    "# # I get the activations for each layer for each sample\n",
    "# obj_activations = torch.stack([layers[:,i,:] for i in range(layers.shape[1])])\n",
    "# print(obj_activations.shape, '(nr_object, num_layers, activation_for_each_layer)')\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "number_activations = 8\n",
    "\n",
    "a = torch.randn(batch_size, number_activations)\n",
    "\n",
    "\n",
    "attn = get_layer_activations([torch.randn(batch_size, number_activations) for _ in range(10)])\n",
    "\n",
    "\n",
    "MultiHeadAttention(8, 4)(attn, attn, a).shape\n",
    "\n",
    "# list(extract_activations_per_sample(\n",
    "#             extract_activations_layers(layers), \n",
    "#             mask=False\n",
    "#         )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 torch.Size([16, 8]) (num_layers, batch_size, number_activations)\n",
      "torch.Size([16, 8])\n",
      "torch.Size([16, 10, 8])\n",
      "torch.Size([8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modified attention mechanism\n",
    "\n",
    "### GLOBAL VARIABLES\n",
    "CROSSOVER_MAGNITUDE = 0.3\n",
    "MUTATION_FACTOR = 0.3\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "\n",
    "def attention_map_crossover(attention_map):\n",
    "    \"\"\" Apply the crossover over the attention maps of each head.\n",
    "    The crosseover consists in picking a random index in the maxtrix over\n",
    "    the columns and swapping the values in between the columns of the\n",
    "    attention map.\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # get the crossover magnitude\n",
    "    crossover_magnitude = CROSSOVER_MAGNITUDE\n",
    "    \n",
    "    # get the batch size\n",
    "    dim_batch = attention_map.shape[0]\n",
    "    \n",
    "    # get the number of heads\n",
    "    number_of_heads = attention_map.shape[1]\n",
    "    \n",
    "    for idx_batch in range(dim_batch):\n",
    "        for idx_head in range(number_of_heads):\n",
    "            \n",
    "            # get the crossover index\n",
    "            crossover_index = attention_map.shape[2] - int(attention_map.shape[2]*crossover_magnitude)\n",
    "            \n",
    "            # get two random indexes\n",
    "            random_index_1 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            random_index_2 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            \n",
    "            # swap the values in that position over the columns\n",
    "            for idx, (x_1, x_2) in enumerate(zip(attention_map[idx_batch][idx_head][random_index_1][crossover_index:].detach(), attention_map[idx_batch][idx_head][random_index_2][crossover_index:].detach())):\n",
    "                \n",
    "                # debug\n",
    "                # print(attention_map[idx_batch][idx_head].shape, random_index_1, random_index_2, crossover_index)         \n",
    "                # print(x_1, x_2,idx_batch, idx_head, idx)\n",
    "                \n",
    "                # swap the values in that position over the columns\n",
    "                attention_map[idx_batch][idx_head][random_index_1][crossover_index+idx] = x_2 # make crossover\n",
    "                attention_map[idx_batch][idx_head][random_index_2][crossover_index+idx] = x_1 # make crossover\n",
    "    \n",
    "    return attention_map\n",
    "\n",
    "################################################################\n",
    "\n",
    "def mutate_attention_map(attention_map):\n",
    "    \"\"\" Mutate the attention map by making an elementwise multiplication with \n",
    "    a random tensor with values between 1-mutation_factor and 1+mutation_factor\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, num_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:    \n",
    "        torch.Tensor: shape (batch_size, num_heads, activation_size, activation_size)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # get the mutation factor\n",
    "    mutation_factor = MUTATION_FACTOR\n",
    "    # return the mutated attention map\n",
    "    # multiplied elementwise with a \n",
    "    # random matrix with values between \n",
    "    # 1-mutation_factor and 1+mutation_factor\n",
    "    return torch.mul(attention_map, torch.randn(attention_map.shape).uniform_(1-mutation_factor,1+mutation_factor))\n",
    "    \n",
    "    \n",
    "################################################################\n",
    "\n",
    "\n",
    "def head_batched_attention_mechanism(Q, K, V):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Q: (batch_size, num_heads, num_layer, activation_size)\n",
    "        K: (batch_size, num_heads, num_layer, activation_size)\n",
    "        V: (batch_size, num_heads, activation_size, 1) # activations in the current layer\n",
    "\n",
    "    Returns:\n",
    "        attention: (batch_size, num_heads, activation_size)\n",
    "\n",
    "        # attention mechanism\n",
    "        # # (batch_size, num_heads, activation_size, activation_size)\n",
    "        # attention = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        # attention = attention / torch.sqrt(torch.tensor(activation_size).float())\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = nn.Softmax(dim=-1)(attention)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = torch.matmul(attention, V)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size)\n",
    "        # attention = attention.squeeze(-1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # with probability p\n",
    "    p = torch.rand(1)\n",
    "    \n",
    "    # p <= 0.6 apply the mutation only\n",
    "    if p <= 0.6:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                    mutate_attention_map(torch.matmul( \n",
    "                                Q.transpose(-1,-2) , \n",
    "                                K\n",
    "                    )/torch.sqrt(torch.tensor(8)))    \n",
    "                ) @ V).squeeze(-1)\n",
    "        \n",
    "    # p > 0.6 apply the crossover only\n",
    "    else:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                attention_map_crossover(torch.matmul( \n",
    "                            Q.transpose(-1,-2) , \n",
    "                            K\n",
    "                )/torch.sqrt(torch.tensor(8)))    \n",
    "            ) @ V).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_emb, n_head) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert dim_emb % n_head == 0, 'dim_emb must be divisible by n_head'\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.W_Q = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W_K = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W_V = nn.Linear(dim_emb, dim_emb)\n",
    "\n",
    "        self.W_O = nn.Linear(dim_emb*n_head, dim_emb)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # get the shape of the input\n",
    "        batch_size, activation_size, activation_size = Q.size()\n",
    "        \n",
    "        # check the device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # move to device\n",
    "        Q.to(device)\n",
    "        K.to(device)\n",
    "        V.to(device)\n",
    "        \n",
    "        # reshape Q, K, V\n",
    "        # parallelize over the number of heads\n",
    "        # (batch_size, num_heads, num_layer, activation_size)\n",
    "        Q = torch.stack([Q for _ in range(self.n_head)], 1)\n",
    "        K = torch.stack([K for _ in range(self.n_head)], 1)\n",
    "        V = torch.stack([V for _ in range(self.n_head)], 1)\n",
    "\n",
    "        # apply linear transformation\n",
    "        Q = self.W_Q(Q)\n",
    "        K = self.W_K(K)\n",
    "        V = self.W_V(V).reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        \n",
    "        # apply attention mechanism\n",
    "        out_attention = head_batched_attention_mechanism(Q, K, V).reshape(batch_size, self.n_head*activation_size)\n",
    "\n",
    "        # apply linear transformation\n",
    "        return self.W_O(out_attention)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "\n",
    "\n",
    "# EXAMPLE \n",
    "#######################################################################\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "\n",
    "# inputs\n",
    "batch_size = 16\n",
    "number_activations = 8\n",
    "\n",
    "#######################################################################\n",
    "number_layers = 10\n",
    "\n",
    "activations = [torch.randn(batch_size, number_activations) for _ in range(number_layers)]\n",
    "# get layered activations\n",
    "attn = get_layer_activations(activations)\n",
    "print(len(activations), activations[0].size(),'(num_layers, batch_size, number_activations)')\n",
    "\n",
    "print(activations[0].size()) # (batch_size, number_activations)\n",
    "\n",
    "print(attn.shape) # (batch_size, number_layers, number_activations)\n",
    "\n",
    "# get shape of the attention map\n",
    "# shape: (batch_size, num_heads, activation_size, activation_size)\n",
    "print(MultiHeadAttention(8, 4)(attn, attn, a)[1].shape)\n",
    "\n",
    "# extract attention map for each head\n",
    "out = MultiHeadAttention(8, 4)(attn, attn, activations[-1])\n",
    "\n",
    "out.shape # (batch_size, activation_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # EXAMPLE element-wise multiplication\n",
    "# # mat = torch.stack([torch.Tensor([i for i in range(3)]) for _ in range(3)], 1) \n",
    "# # print(mat)\n",
    "# # print(torch.mul(mat, mat)) # element-wise multiplication\n",
    "\n",
    "# # # multiply two torch matrix element-wise\n",
    "# # mutation_factor = 0.03\n",
    "\n",
    "# # # how to mutate the attention map\n",
    "# # mutated_attention_map = torch.mul(attention_map, torch.randn(attention_map.shape).uniform_(1-mutation_factor,1+mutation_factor))\n",
    "# # # get the difference between the two matrices mutated_attention_map and attention_map\n",
    "# # print('mutation magnitude:',abs(torch.sum(mutated_attention_map-attention_map).detach().numpy()))\n",
    "\n",
    "\n",
    "# # indexing over the columns of the attention map\n",
    "\n",
    "# # def crossover_attention_map(attention_map, crossover_magnitude):\n",
    "\n",
    "\n",
    "# import multiprocessing\n",
    "\n",
    "# print(multiprocessing.cpu_count())\n",
    "\n",
    "# def attention_map_crossover(attention_map):\n",
    "    \n",
    "#     crossover_magnitude = 0.3\n",
    "    \n",
    "#     dim_batch = attention_map.shape[0]\n",
    "#     number_of_heads = attention_map.shape[1]\n",
    "    \n",
    "#     for idx_batch in range(dim_batch):\n",
    "#         for idx_head in range(number_of_heads):\n",
    "            \n",
    "#             print(idx_head)\n",
    "            \n",
    "#             crossover_index = attention_map.shape[2] - int(attention_map.shape[2]*crossover_magnitude)\n",
    "                \n",
    "#             random_index_1 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "#             random_index_2 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            \n",
    "#             for idx, (x_1, x_2) in enumerate(zip(attention_map[idx_batch][idx_head][random_index_1][crossover_index:].detach(), attention_map[idx_batch][idx_head][random_index_2][crossover_index:].detach())):\n",
    "                \n",
    "#                 print(attention_map[idx_batch][idx_head].shape, random_index_1, random_index_2, crossover_index)\n",
    "                            \n",
    "#                 print(x_1, x_2,idx_batch, idx_head, idx)\n",
    "                \n",
    "#                 attention_map[idx_batch][idx_head][random_index_1][crossover_index+idx] = x_2 # make crossover\n",
    "#                 attention_map[idx_batch][idx_head][random_index_2][crossover_index+idx] = x_1 # make crossover\n",
    "    \n",
    "#     return attention_map\n",
    "    \n",
    "\n",
    "# import time\n",
    "\n",
    "# start = time.time()\n",
    "# attention_map_crossover(attention_map).shape\n",
    "# end = time.time()\n",
    "\n",
    "# print('time required to perform crossover:',end-start)\n",
    "\n",
    "# torch.rand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Evaluate the layer with few neurons on the MNIST dataset. The results are good even with few neurons. The evidence shows that introducing mutations and crossover on the attention map used to weight the activation of the next layer makes the netwoek converge. It would be intresting to see the evaluation with more neurons and on more complex dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinW layers:\n",
      "\n",
      "Depth 0: LinW(\n",
      "  in_features=8, out_features=8, bias=True\n",
      "  (mha): MultiHeadAttention(\n",
      "    (W_Q): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (W_K): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (W_V): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (W_O): Linear(in_features=32, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "Depth 1: LinW(\n",
      "  in_features=8, out_features=8, bias=True\n",
      "  (mha): MultiHeadAttention(\n",
      "    (W_Q): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (W_K): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (W_V): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (W_O): Linear(in_features=32, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1/10, Training Loss: 0.9341, Training Accuracy: 68.37%, Test accuracy: 83.95%\n",
      "Epoch 2/10, Training Loss: 0.4378, Training Accuracy: 87.18%, Test accuracy: 89.38%\n",
      "Epoch 3/10, Training Loss: 0.3512, Training Accuracy: 89.89%, Test accuracy: 89.93%\n",
      "Epoch 4/10, Training Loss: 0.3149, Training Accuracy: 90.98%, Test accuracy: 91.60%\n",
      "Epoch 5/10, Training Loss: 0.2906, Training Accuracy: 91.63%, Test accuracy: 92.18%\n",
      "Epoch 6/10, Training Loss: 0.2740, Training Accuracy: 92.15%, Test accuracy: 92.10%\n",
      "Epoch 7/10, Training Loss: 0.2648, Training Accuracy: 92.50%, Test accuracy: 92.06%\n",
      "Epoch 8/10, Training Loss: 0.2585, Training Accuracy: 92.60%, Test accuracy: 92.44%\n",
      "Epoch 9/10, Training Loss: 0.2508, Training Accuracy: 92.83%, Test accuracy: 92.65%\n",
      "Epoch 10/10, Training Loss: 0.2460, Training Accuracy: 92.96%, Test accuracy: 92.60%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def evaluate(device, model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / total\n",
    "    return train_loss, accuracy\n",
    "\n",
    "### GLOBAL VARIABLES\n",
    "CROSSOVER_MAGNITUDE = 0.3\n",
    "MUTATION_FACTOR = 0.3\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "\n",
    "def attention_map_crossover(attention_map):\n",
    "    \"\"\" Apply the crossover over the attention maps of each head.\n",
    "    The crosseover consists in picking a random index in the maxtrix over\n",
    "    the columns and swapping the values in between the columns of the\n",
    "    attention map.\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # get the crossover magnitude\n",
    "    crossover_magnitude = CROSSOVER_MAGNITUDE\n",
    "    \n",
    "    # get the batch size\n",
    "    dim_batch = attention_map.shape[0]\n",
    "    \n",
    "    # get the number of heads\n",
    "    number_of_heads = attention_map.shape[1]\n",
    "    \n",
    "    for idx_batch in range(dim_batch):\n",
    "        for idx_head in range(number_of_heads):\n",
    "            \n",
    "            # get the crossover index\n",
    "            crossover_index = attention_map.shape[2] - int(attention_map.shape[2]*crossover_magnitude)\n",
    "            \n",
    "            # get two random indexes\n",
    "            random_index_1 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            random_index_2 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            \n",
    "            # swap the values in that position over the columns\n",
    "            for idx, (x_1, x_2) in enumerate(zip(attention_map[idx_batch][idx_head][random_index_1][crossover_index:].detach(), attention_map[idx_batch][idx_head][random_index_2][crossover_index:].detach())):\n",
    "                \n",
    "                # debug\n",
    "                # print(attention_map[idx_batch][idx_head].shape, random_index_1, random_index_2, crossover_index)         \n",
    "                # print(x_1, x_2,idx_batch, idx_head, idx)\n",
    "                \n",
    "                # swap the values in that position over the columns\n",
    "                attention_map[idx_batch][idx_head][random_index_1][crossover_index+idx] = x_2 # make crossover\n",
    "                attention_map[idx_batch][idx_head][random_index_2][crossover_index+idx] = x_1 # make crossover\n",
    "    \n",
    "    return attention_map\n",
    "\n",
    "################################################################\n",
    "\n",
    "def mutate_attention_map(attention_map):\n",
    "    \"\"\" Mutate the attention map by making an elementwise multiplication with \n",
    "    a random tensor with values between 1-mutation_factor and 1+mutation_factor\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, num_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:    \n",
    "        torch.Tensor: shape (batch_size, num_heads, activation_size, activation_size)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # get the mutation factor\n",
    "    mutation_factor = MUTATION_FACTOR\n",
    "    # return the mutated attention map\n",
    "    # multiplied elementwise with a \n",
    "    # random matrix with values between \n",
    "    # 1-mutation_factor and 1+mutation_factor\n",
    "    return torch.mul(attention_map, torch.randn(attention_map.shape).uniform_(1-mutation_factor,1+mutation_factor))\n",
    "    \n",
    "    \n",
    "################################################################\n",
    "\n",
    "\n",
    "def head_batched_attention_mechanism(Q, K, V):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Q: (batch_size, num_heads, num_layer, activation_size)\n",
    "        K: (batch_size, num_heads, num_layer, activation_size)\n",
    "        V: (batch_size, num_heads, activation_size, 1) # activations in the current layer\n",
    "\n",
    "    Returns:\n",
    "        attention: (batch_size, num_heads, activation_size)\n",
    "\n",
    "        # attention mechanism\n",
    "        # # (batch_size, num_heads, activation_size, activation_size)\n",
    "        # attention = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        # attention = attention / torch.sqrt(torch.tensor(activation_size).float())\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = nn.Softmax(dim=-1)(attention)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = torch.matmul(attention, V)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size)\n",
    "        # attention = attention.squeeze(-1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # with probability p\n",
    "    p = torch.rand(1)\n",
    "    \n",
    "    # p <= 0.6 apply the mutation only\n",
    "    if p <= 0.6:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                    mutate_attention_map(torch.matmul( \n",
    "                                Q.transpose(-1,-2) , \n",
    "                                K\n",
    "                    )/torch.sqrt(torch.tensor(8)))    \n",
    "                ) @ V).squeeze(-1)\n",
    "        \n",
    "    # p > 0.6 apply the crossover only\n",
    "    else:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                attention_map_crossover(torch.matmul( \n",
    "                            Q.transpose(-1,-2) , \n",
    "                            K\n",
    "                )/torch.sqrt(torch.tensor(8)))    \n",
    "            ) @ V).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_emb, n_head) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert dim_emb % n_head == 0, 'dim_emb must be divisible by n_head'\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.W_Q = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W_K = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W_V = nn.Linear(dim_emb, dim_emb)\n",
    "\n",
    "        self.W_O = nn.Linear(dim_emb*n_head, dim_emb)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # get the shape of the input\n",
    "        batch_size, activation_size, activation_size = Q.size()\n",
    "        \n",
    "        # check the device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # move to device\n",
    "        Q.to(device)\n",
    "        K.to(device)\n",
    "        V.to(device)\n",
    "        \n",
    "        # reshape Q, K, V\n",
    "        # parallelize over the number of heads\n",
    "        # (batch_size, num_heads, num_layer, activation_size)\n",
    "        Q = torch.stack([Q for _ in range(self.n_head)], 1)\n",
    "        K = torch.stack([K for _ in range(self.n_head)], 1)\n",
    "        V = torch.stack([V for _ in range(self.n_head)], 1)\n",
    "\n",
    "        # apply linear transformation\n",
    "        Q = self.W_Q(Q)\n",
    "        K = self.W_K(K)\n",
    "        V = self.W_V(V).reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        \n",
    "        # apply attention mechanism\n",
    "        out_attention = head_batched_attention_mechanism(Q, K, V).reshape(batch_size, self.n_head*activation_size)\n",
    "\n",
    "        # apply linear transformation\n",
    "        return self.W_O(out_attention)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "####################################################################\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(784, 8)\n",
    "        self.l2 = LinW(in_features=8, out_features=8, depth=0)\n",
    "        self.l3 = LinW(in_features=8, out_features=8, depth=1, layers=[self.l2])\n",
    "        self.l4 = nn.Linear(8, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.l1(x)\n",
    "        repr.append(x)\n",
    "        x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        # x = self.gelu(self.l2(x, repr))\n",
    "        x = self.l2(x, repr)\n",
    "        repr.append(x)\n",
    "        x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, depth, layers=[]):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.depth = depth\n",
    "        self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "        self.mha = MultiHeadAttention(in_features, 4)\n",
    "\n",
    "    def forward(self, input, activations=[]):\n",
    "        activations = get_layer_activations(activations)\n",
    "        return F.linear(self.mha(activations, activations, input), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "LinW layers:\n",
      "\n",
      "Depth 0: LinW(\n",
      "  in_features=32, out_features=32, bias=True\n",
      "  (mha): MultiHeadAttention(\n",
      "    (W_Q): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (W_K): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (W_V): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (W_O): Linear(in_features=128, out_features=32, bias=True)\n",
      "  )\n",
      ")\n",
      "Depth 1: LinW(\n",
      "  in_features=32, out_features=32, bias=True\n",
      "  (mha): MultiHeadAttention(\n",
      "    (W_Q): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (W_K): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (W_V): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (W_O): Linear(in_features=128, out_features=32, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1/10, Training Loss: 1.9940, Training Accuracy: 25.86%, Test accuracy: 32.41%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 348\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLinW layers:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDepth \u001b[39m\u001b[39m{\u001b[39;00mmodel[i]\u001b[39m.\u001b[39mdepth\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mmodel[i]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(model))]), sep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    347\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m--> 348\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(model, train_loader, criterion, optimizer, device)\n\u001b[1;32m    349\u001b[0m     test_accuracy \u001b[39m=\u001b[39m evaluate(device, model, test_loader)\n\u001b[1;32m    350\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mEPOCHS\u001b[39m}\u001b[39;00m\u001b[39m, Training Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Training Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%, Test accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtest_accuracy\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 36\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m images, labels \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     35\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 36\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     37\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     38\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[6], line 307\u001b[0m, in \u001b[0;36mMLPWD.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    305\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgelu(x)\n\u001b[1;32m    306\u001b[0m \u001b[39m# repr.append(x.detach().cpu().numpy())\u001b[39;00m\n\u001b[0;32m--> 307\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml3(x, \u001b[39mrepr\u001b[39;49m))\n\u001b[1;32m    308\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml4(x)\n\u001b[1;32m    309\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[6], line 327\u001b[0m, in \u001b[0;36mLinW.forward\u001b[0;34m(self, input, activations)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, activations\u001b[39m=\u001b[39m[]):\n\u001b[1;32m    326\u001b[0m     activations \u001b[39m=\u001b[39m get_layer_activations(activations)\n\u001b[0;32m--> 327\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmha(activations, activations, \u001b[39minput\u001b[39;49m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[6], line 218\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, Q, K, V)\u001b[0m\n\u001b[1;32m    215\u001b[0m V \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_V(V)\u001b[39m.\u001b[39mreshape(batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_head, activation_size, \u001b[39m1\u001b[39m)\n\u001b[1;32m    217\u001b[0m \u001b[39m# apply attention mechanism\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m out_attention \u001b[39m=\u001b[39m head_batched_attention_mechanism(Q, K, V)\u001b[39m.\u001b[39mreshape(batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_head\u001b[39m*\u001b[39mactivation_size)\n\u001b[1;32m    220\u001b[0m \u001b[39m# apply linear transformation\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_O(out_attention)\n",
      "Cell \u001b[0;32mIn[6], line 170\u001b[0m, in \u001b[0;36mhead_batched_attention_mechanism\u001b[0;34m(Q, K, V)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[39mreturn\u001b[39;00m (nn\u001b[39m.\u001b[39mSoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)(\n\u001b[1;32m    161\u001b[0m                 mutate_attention_map(torch\u001b[39m.\u001b[39mmatmul( \n\u001b[1;32m    162\u001b[0m                             Q\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m) , \n\u001b[1;32m    163\u001b[0m                             K\n\u001b[1;32m    164\u001b[0m                 )\u001b[39m/\u001b[39mtorch\u001b[39m.\u001b[39msqrt(torch\u001b[39m.\u001b[39mtensor(\u001b[39m8\u001b[39m)))    \n\u001b[1;32m    165\u001b[0m             ) \u001b[39m@\u001b[39m V)\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    167\u001b[0m \u001b[39m# p > 0.6 apply the crossover only\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m (nn\u001b[39m.\u001b[39mSoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)(\n\u001b[0;32m--> 170\u001b[0m             attention_map_crossover(torch\u001b[39m.\u001b[39;49mmatmul( \n\u001b[1;32m    171\u001b[0m                         Q\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m) , \n\u001b[1;32m    172\u001b[0m                         K\n\u001b[1;32m    173\u001b[0m             )\u001b[39m/\u001b[39;49mtorch\u001b[39m.\u001b[39;49msqrt(torch\u001b[39m.\u001b[39;49mtensor(\u001b[39m8\u001b[39;49m)))    \n\u001b[1;32m    174\u001b[0m         ) \u001b[39m@\u001b[39m V)\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 99\u001b[0m, in \u001b[0;36mattention_map_crossover\u001b[0;34m(attention_map)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[39mfor\u001b[39;00m idx, (x_1, x_2) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(attention_map[idx_batch][idx_head][random_index_1][crossover_index:]\u001b[39m.\u001b[39mdetach(), attention_map[idx_batch][idx_head][random_index_2][crossover_index:]\u001b[39m.\u001b[39mdetach())):\n\u001b[1;32m     92\u001b[0m             \n\u001b[1;32m     93\u001b[0m             \u001b[39m# debug\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \n\u001b[1;32m     97\u001b[0m             \u001b[39m# swap the values in that position over the columns\u001b[39;00m\n\u001b[1;32m     98\u001b[0m             attention_map[idx_batch][idx_head][random_index_1][crossover_index\u001b[39m+\u001b[39midx] \u001b[39m=\u001b[39m x_2 \u001b[39m# make crossover\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m             attention_map[idx_batch][idx_head][random_index_2][crossover_index\u001b[39m+\u001b[39midx] \u001b[39m=\u001b[39m x_1 \u001b[39m# make crossover\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39mreturn\u001b[39;00m attention_map\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def evaluate(device, model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / total\n",
    "    return train_loss, accuracy\n",
    "\n",
    "### GLOBAL VARIABLES\n",
    "CROSSOVER_MAGNITUDE = 0.3\n",
    "MUTATION_FACTOR = 0.3\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "\n",
    "def attention_map_crossover(attention_map):\n",
    "    \"\"\" Apply the crossover over the attention maps of each head.\n",
    "    The crosseover consists in picking a random index in the maxtrix over\n",
    "    the columns and swapping the values in between the columns of the\n",
    "    attention map.\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # get the crossover magnitude\n",
    "    crossover_magnitude = CROSSOVER_MAGNITUDE\n",
    "    \n",
    "    # get the batch size\n",
    "    dim_batch = attention_map.shape[0]\n",
    "    \n",
    "    # get the number of heads\n",
    "    number_of_heads = attention_map.shape[1]\n",
    "    \n",
    "    for idx_batch in range(dim_batch):\n",
    "        for idx_head in range(number_of_heads):\n",
    "            \n",
    "            # get the crossover index\n",
    "            crossover_index = attention_map.shape[2] - int(attention_map.shape[2]*crossover_magnitude)\n",
    "            \n",
    "            # get two random indexes\n",
    "            random_index_1 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            random_index_2 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            \n",
    "            # swap the values in that position over the columns\n",
    "            for idx, (x_1, x_2) in enumerate(zip(attention_map[idx_batch][idx_head][random_index_1][crossover_index:].detach(), attention_map[idx_batch][idx_head][random_index_2][crossover_index:].detach())):\n",
    "                \n",
    "                # debug\n",
    "                # print(attention_map[idx_batch][idx_head].shape, random_index_1, random_index_2, crossover_index)         \n",
    "                # print(x_1, x_2,idx_batch, idx_head, idx)\n",
    "                \n",
    "                # swap the values in that position over the columns\n",
    "                attention_map[idx_batch][idx_head][random_index_1][crossover_index+idx] = x_2 # make crossover\n",
    "                attention_map[idx_batch][idx_head][random_index_2][crossover_index+idx] = x_1 # make crossover\n",
    "    \n",
    "    return attention_map\n",
    "\n",
    "################################################################\n",
    "\n",
    "def mutate_attention_map(attention_map):\n",
    "    \"\"\" Mutate the attention map by making an elementwise multiplication with \n",
    "    a random tensor with values between 1-mutation_factor and 1+mutation_factor\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, num_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:    \n",
    "        torch.Tensor: shape (batch_size, num_heads, activation_size, activation_size)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # get the mutation factor\n",
    "    mutation_factor = MUTATION_FACTOR\n",
    "    # return the mutated attention map\n",
    "    # multiplied elementwise with a \n",
    "    # random matrix with values between \n",
    "    # 1-mutation_factor and 1+mutation_factor\n",
    "    return torch.mul(attention_map, torch.randn(attention_map.shape).uniform_(1-mutation_factor,1+mutation_factor).to(attention_map.device))\n",
    "    \n",
    "    \n",
    "################################################################\n",
    "\n",
    "\n",
    "def head_batched_attention_mechanism(Q, K, V):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Q: (batch_size, num_heads, num_layer, activation_size)\n",
    "        K: (batch_size, num_heads, num_layer, activation_size)\n",
    "        V: (batch_size, num_heads, activation_size, 1) # activations in the current layer\n",
    "\n",
    "    Returns:\n",
    "        attention: (batch_size, num_heads, activation_size)\n",
    "\n",
    "        # attention mechanism\n",
    "        # # (batch_size, num_heads, activation_size, activation_size)\n",
    "        # attention = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        # attention = attention / torch.sqrt(torch.tensor(activation_size).float())\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = nn.Softmax(dim=-1)(attention)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = torch.matmul(attention, V)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size)\n",
    "        # attention = attention.squeeze(-1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # with probability p\n",
    "    p = torch.rand(1)\n",
    "    \n",
    "    # p <= 0.6 apply the mutation only\n",
    "    if p <= 0.6:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                    mutate_attention_map(torch.matmul( \n",
    "                                Q.transpose(-1,-2) , \n",
    "                                K\n",
    "                    )/torch.sqrt(torch.tensor(8)))    \n",
    "                ) @ V).squeeze(-1)\n",
    "        \n",
    "    # p > 0.6 apply the crossover only\n",
    "    else:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                attention_map_crossover(torch.matmul( \n",
    "                            Q.transpose(-1,-2) , \n",
    "                            K\n",
    "                )/torch.sqrt(torch.tensor(8)))    \n",
    "            ) @ V).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_emb, n_head) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert dim_emb % n_head == 0, 'dim_emb must be divisible by n_head'\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.W_Q = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W_K = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W_V = nn.Linear(dim_emb, dim_emb)\n",
    "\n",
    "        self.W_O = nn.Linear(dim_emb*n_head, dim_emb)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # get the shape of the input\n",
    "        batch_size, activation_size, activation_size = Q.size()\n",
    "        \n",
    "        # check the device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # move to device\n",
    "        Q.to(device)\n",
    "        K.to(device)\n",
    "        V.to(device)\n",
    "        \n",
    "        # reshape Q, K, V\n",
    "        # parallelize over the number of heads\n",
    "        # (batch_size, num_heads, num_layer, activation_size)\n",
    "        Q = torch.stack([Q for _ in range(self.n_head)], 1)\n",
    "        K = torch.stack([K for _ in range(self.n_head)], 1)\n",
    "        V = torch.stack([V for _ in range(self.n_head)], 1)\n",
    "\n",
    "        # apply linear transformation\n",
    "        Q = self.W_Q(Q)\n",
    "        K = self.W_K(K)\n",
    "        V = self.W_V(V).reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        \n",
    "        # apply attention mechanism\n",
    "        out_attention = head_batched_attention_mechanism(Q, K, V).reshape(batch_size, self.n_head*activation_size)\n",
    "\n",
    "        # apply linear transformation\n",
    "        return self.W_O(out_attention)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, 32)\n",
    "        self.l2 = LinW(in_features=32, out_features=32, depth=0)\n",
    "        self.l3 = LinW(in_features=32, out_features=32, depth=1, layers=[self.l2])\n",
    "        self.l4 = nn.Linear(32, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.l1(x)\n",
    "        repr.append(x)\n",
    "        x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        # x = self.gelu(self.l2(x, repr))\n",
    "        x = self.l2(x, repr)\n",
    "        repr.append(x)\n",
    "        x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, depth, layers=[]):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.depth = depth\n",
    "        self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "        self.mha = MultiHeadAttention(in_features, 4)\n",
    "\n",
    "    def forward(self, input, activations=[]):\n",
    "        activations = get_layer_activations(activations)\n",
    "        return F.linear(self.mha(activations, activations, input), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 120\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "LinW layers:\n",
      "\n",
      "Depth 0: LinW(\n",
      "  in_features=16, out_features=16, bias=True\n",
      "  (mha): MultiHeadAttention(\n",
      "    (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (W_O): Linear(in_features=16, out_features=16, bias=True)\n",
      "  )\n",
      ")\n",
      "Depth 1: LinW(\n",
      "  in_features=16, out_features=16, bias=True\n",
      "  (mha): MultiHeadAttention(\n",
      "    (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (W_O): Linear(in_features=16, out_features=16, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1/10, Training Loss: 2.0611, Training Accuracy: 21.91%, Test accuracy: 27.32%\n",
      "Epoch 2/10, Training Loss: 1.9188, Training Accuracy: 27.91%, Test accuracy: 29.00%\n",
      "Epoch 3/10, Training Loss: 1.8632, Training Accuracy: 30.66%, Test accuracy: 32.41%\n",
      "Epoch 4/10, Training Loss: 1.8198, Training Accuracy: 32.62%, Test accuracy: 33.07%\n",
      "Epoch 5/10, Training Loss: 1.7955, Training Accuracy: 33.50%, Test accuracy: 34.24%\n",
      "Epoch 6/10, Training Loss: 1.7529, Training Accuracy: 35.15%, Test accuracy: 37.09%\n",
      "Epoch 7/10, Training Loss: 1.7308, Training Accuracy: 36.60%, Test accuracy: 37.12%\n",
      "Epoch 8/10, Training Loss: 1.7026, Training Accuracy: 37.84%, Test accuracy: 38.25%\n",
      "Epoch 9/10, Training Loss: 1.6713, Training Accuracy: 39.26%, Test accuracy: 38.95%\n",
      "Epoch 10/10, Training Loss: 1.6575, Training Accuracy: 39.58%, Test accuracy: 38.83%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def evaluate(device, model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / total\n",
    "    return train_loss, accuracy\n",
    "\n",
    "### GLOBAL VARIABLES\n",
    "CROSSOVER_MAGNITUDE = 0.3\n",
    "MUTATION_FACTOR = 0.3\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "\n",
    "def attention_map_crossover(attention_map):\n",
    "    \"\"\" Apply the crossover over the attention maps of each head.\n",
    "    The crosseover consists in picking a random index in the maxtrix over\n",
    "    the columns and swapping the values in between the columns of the\n",
    "    attention map.\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # get the crossover magnitude\n",
    "    crossover_magnitude = CROSSOVER_MAGNITUDE\n",
    "    \n",
    "    # get the batch size\n",
    "    dim_batch = attention_map.shape[0]\n",
    "    \n",
    "    # get the number of heads\n",
    "    number_of_heads = attention_map.shape[1]\n",
    "    \n",
    "    for idx_batch in range(dim_batch):\n",
    "        for idx_head in range(number_of_heads):\n",
    "            \n",
    "            # get the crossover index\n",
    "            crossover_index = attention_map.shape[2] - int(attention_map.shape[2]*crossover_magnitude)\n",
    "            \n",
    "            # get two random indexes\n",
    "            random_index_1 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            random_index_2 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            \n",
    "            # swap the values in that position over the columns\n",
    "            for idx, (x_1, x_2) in enumerate(zip(attention_map[idx_batch][idx_head][random_index_1][crossover_index:].detach(), attention_map[idx_batch][idx_head][random_index_2][crossover_index:].detach())):\n",
    "                \n",
    "                # debug\n",
    "                # print(attention_map[idx_batch][idx_head].shape, random_index_1, random_index_2, crossover_index)         \n",
    "                # print(x_1, x_2,idx_batch, idx_head, idx)\n",
    "                \n",
    "                # swap the values in that position over the columns\n",
    "                attention_map[idx_batch][idx_head][random_index_1][crossover_index+idx] = x_2 # make crossover\n",
    "                attention_map[idx_batch][idx_head][random_index_2][crossover_index+idx] = x_1 # make crossover\n",
    "    \n",
    "    return attention_map\n",
    "\n",
    "################################################################\n",
    "\n",
    "def mutate_attention_map(attention_map):\n",
    "    \"\"\" Mutate the attention map by making an elementwise multiplication with \n",
    "    a random tensor with values between 1-mutation_factor and 1+mutation_factor\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, num_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:    \n",
    "        torch.Tensor: shape (batch_size, num_heads, activation_size, activation_size)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # get the mutation factor\n",
    "    mutation_factor = MUTATION_FACTOR\n",
    "    # return the mutated attention map\n",
    "    # multiplied elementwise with a \n",
    "    # random matrix with values between \n",
    "    # 1-mutation_factor and 1+mutation_factor\n",
    "    return torch.mul(attention_map, torch.randn(attention_map.shape).uniform_(1-mutation_factor,1+mutation_factor).to(attention_map.device))\n",
    "    \n",
    "    \n",
    "################################################################\n",
    "\n",
    "\n",
    "def head_batched_attention_mechanism(Q, K, V):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Q: (batch_size, num_heads, num_layer, activation_size)\n",
    "        K: (batch_size, num_heads, num_layer, activation_size)\n",
    "        V: (batch_size, num_heads, activation_size, 1) # activations in the current layer\n",
    "\n",
    "    Returns:\n",
    "        attention: (batch_size, num_heads, activation_size)\n",
    "\n",
    "        # attention mechanism\n",
    "        # # (batch_size, num_heads, activation_size, activation_size)\n",
    "        # attention = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        # attention = attention / torch.sqrt(torch.tensor(activation_size).float())\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = nn.Softmax(dim=-1)(attention)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = torch.matmul(attention, V)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size)\n",
    "        # attention = attention.squeeze(-1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # with probability p\n",
    "    p = torch.rand(1)\n",
    "    \n",
    "    # p <= 0.6 apply the mutation only\n",
    "    if p <= 0.6:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                    mutate_attention_map(torch.matmul( \n",
    "                                Q.transpose(-1,-2) , \n",
    "                                K\n",
    "                    )/torch.sqrt(torch.tensor(8)))    \n",
    "                ) @ V).squeeze(-1)\n",
    "        \n",
    "    # p > 0.6 apply the crossover only\n",
    "    else:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                attention_map_crossover(torch.matmul( \n",
    "                            Q.transpose(-1,-2) , \n",
    "                            K\n",
    "                )/torch.sqrt(torch.tensor(8)))    \n",
    "            ) @ V).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_emb, n_head) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert dim_emb % n_head == 0, 'dim_emb must be divisible by n_head'\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.W_Q = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W_K = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W_V = nn.Linear(dim_emb, dim_emb)\n",
    "\n",
    "        self.W_O = nn.Linear(dim_emb*n_head, dim_emb)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # get the shape of the input\n",
    "        batch_size, activation_size, activation_size = Q.size()\n",
    "        \n",
    "        # check the device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # move to device\n",
    "        Q.to(device)\n",
    "        K.to(device)\n",
    "        V.to(device)\n",
    "        \n",
    "        # reshape Q, K, V\n",
    "        # parallelize over the number of heads\n",
    "        # (batch_size, num_heads, num_layer, activation_size)\n",
    "        Q = torch.stack([Q for _ in range(self.n_head)], 1)\n",
    "        K = torch.stack([K for _ in range(self.n_head)], 1)\n",
    "        V = torch.stack([V for _ in range(self.n_head)], 1)\n",
    "\n",
    "        # apply linear transformation\n",
    "        Q = self.W_Q(Q)\n",
    "        K = self.W_K(K)\n",
    "        V = self.W_V(V).reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        \n",
    "        # apply attention mechanism\n",
    "        out_attention = head_batched_attention_mechanism(Q, K, V).reshape(batch_size, self.n_head*activation_size)\n",
    "\n",
    "        # apply linear transformation\n",
    "        return self.W_O(out_attention)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, 16)\n",
    "        self.l2 = LinW(in_features=16, out_features=16, depth=0)\n",
    "        self.l3 = LinW(in_features=16, out_features=16, depth=1, layers=[self.l2])\n",
    "        self.l4 = nn.Linear(16, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.l1(x)\n",
    "        repr.append(x)\n",
    "        x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        # x = self.gelu(self.l2(x, repr))\n",
    "        x = self.l2(x, repr)\n",
    "        repr.append(x)\n",
    "        x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, depth, layers=[]):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.depth = depth\n",
    "        self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "        self.mha = MultiHeadAttention(in_features, 1)\n",
    "\n",
    "    def forward(self, input, activations=[]):\n",
    "        activations = get_layer_activations(activations)\n",
    "        return F.linear(self.mha(activations, activations, input), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 120\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "LinW layers:\n",
      "\n",
      "Depth 0: LinW(\n",
      "  in_features=16, out_features=16, bias=True\n",
      "  (mha): MultiHeadAttention(\n",
      "    (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (W_O): Linear(in_features=32, out_features=16, bias=True)\n",
      "  )\n",
      ")\n",
      "Depth 1: LinW(\n",
      "  in_features=16, out_features=16, bias=True\n",
      "  (mha): MultiHeadAttention(\n",
      "    (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (W_O): Linear(in_features=32, out_features=16, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1/7, Training Loss: 2.0838, Training Accuracy: 18.03%, Test accuracy: 19.83%\n",
      "Epoch 2/7, Training Loss: 1.9959, Training Accuracy: 22.95%, Test accuracy: 28.68%\n",
      "Epoch 3/7, Training Loss: 1.8880, Training Accuracy: 28.63%, Test accuracy: 29.46%\n",
      "Epoch 4/7, Training Loss: 1.8560, Training Accuracy: 29.87%, Test accuracy: 30.79%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 348\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLinW layers:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDepth \u001b[39m\u001b[39m{\u001b[39;00mmodel[i]\u001b[39m.\u001b[39mdepth\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mmodel[i]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(model))]), sep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    347\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m--> 348\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(model, train_loader, criterion, optimizer, device)\n\u001b[1;32m    349\u001b[0m     test_accuracy \u001b[39m=\u001b[39m evaluate(device, model, test_loader)\n\u001b[1;32m    350\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mEPOCHS\u001b[39m}\u001b[39;00m\u001b[39m, Training Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Training Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%, Test accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtest_accuracy\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 36\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m images, labels \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     35\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 36\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     37\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     38\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[8], line 307\u001b[0m, in \u001b[0;36mMLPWD.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    305\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgelu(x)\n\u001b[1;32m    306\u001b[0m \u001b[39m# repr.append(x.detach().cpu().numpy())\u001b[39;00m\n\u001b[0;32m--> 307\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml3(x, \u001b[39mrepr\u001b[39;49m))\n\u001b[1;32m    308\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml4(x)\n\u001b[1;32m    309\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[8], line 326\u001b[0m, in \u001b[0;36mLinW.forward\u001b[0;34m(self, input, activations)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, activations\u001b[39m=\u001b[39m[]):\n\u001b[0;32m--> 326\u001b[0m     activations \u001b[39m=\u001b[39m get_layer_activations(activations)\n\u001b[1;32m    327\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmha(activations, activations, \u001b[39minput\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "Cell \u001b[0;32mIn[8], line 279\u001b[0m, in \u001b[0;36mget_layer_activations\u001b[0;34m(activations)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_layer_activations\u001b[39m(activations):\n\u001b[1;32m    268\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Get the activations for each layer for each sample\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \n\u001b[1;32m    270\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    277\u001b[0m \n\u001b[1;32m    278\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m     \u001b[39mreturn\u001b[39;00m get_activations_per_object(torch\u001b[39m.\u001b[39;49mstack(activations))\n",
      "Cell \u001b[0;32mIn[8], line 265\u001b[0m, in \u001b[0;36mget_activations_per_object\u001b[0;34m(activations)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_activations_per_object\u001b[39m(activations):\n\u001b[1;32m    256\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Get the activations for each object per layer\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \n\u001b[1;32m    258\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack([activations[:,i,:] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(activations\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])])\n",
      "Cell \u001b[0;32mIn[8], line 265\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_activations_per_object\u001b[39m(activations):\n\u001b[1;32m    256\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Get the activations for each object per layer\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \n\u001b[1;32m    258\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack([activations[:,i,:] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(activations\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def evaluate(device, model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / total\n",
    "    return train_loss, accuracy\n",
    "\n",
    "### GLOBAL VARIABLES\n",
    "CROSSOVER_MAGNITUDE = 0.3\n",
    "MUTATION_FACTOR = 0.3\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "\n",
    "def attention_map_crossover(attention_map):\n",
    "    \"\"\" Apply the crossover over the attention maps of each head.\n",
    "    The crosseover consists in picking a random index in the maxtrix over\n",
    "    the columns and swapping the values in between the columns of the\n",
    "    attention map.\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # get the crossover magnitude\n",
    "    crossover_magnitude = CROSSOVER_MAGNITUDE\n",
    "    \n",
    "    # get the batch size\n",
    "    dim_batch = attention_map.shape[0]\n",
    "    \n",
    "    # get the number of heads\n",
    "    number_of_heads = attention_map.shape[1]\n",
    "    \n",
    "    for idx_batch in range(dim_batch):\n",
    "        for idx_head in range(number_of_heads):\n",
    "            \n",
    "            # get the crossover index\n",
    "            crossover_index = attention_map.shape[2] - int(attention_map.shape[2]*crossover_magnitude)\n",
    "            \n",
    "            # get two random indexes\n",
    "            random_index_1 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            random_index_2 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            \n",
    "            # swap the values in that position over the columns\n",
    "            for idx, (x_1, x_2) in enumerate(zip(attention_map[idx_batch][idx_head][random_index_1][crossover_index:].detach(), attention_map[idx_batch][idx_head][random_index_2][crossover_index:].detach())):\n",
    "                \n",
    "                # debug\n",
    "                # print(attention_map[idx_batch][idx_head].shape, random_index_1, random_index_2, crossover_index)         \n",
    "                # print(x_1, x_2,idx_batch, idx_head, idx)\n",
    "                \n",
    "                # swap the values in that position over the columns\n",
    "                attention_map[idx_batch][idx_head][random_index_1][crossover_index+idx] = x_2 # make crossover\n",
    "                attention_map[idx_batch][idx_head][random_index_2][crossover_index+idx] = x_1 # make crossover\n",
    "    \n",
    "    return attention_map\n",
    "\n",
    "################################################################\n",
    "\n",
    "def mutate_attention_map(attention_map):\n",
    "    \"\"\" Mutate the attention map by making an elementwise multiplication with \n",
    "    a random tensor with values between 1-mutation_factor and 1+mutation_factor\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, num_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:    \n",
    "        torch.Tensor: shape (batch_size, num_heads, activation_size, activation_size)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # get the mutation factor\n",
    "    mutation_factor = MUTATION_FACTOR\n",
    "    # return the mutated attention map\n",
    "    # multiplied elementwise with a \n",
    "    # random matrix with values between \n",
    "    # 1-mutation_factor and 1+mutation_factor\n",
    "    return torch.mul(attention_map, torch.randn(attention_map.shape).uniform_(1-mutation_factor,1+mutation_factor).to(attention_map.device))\n",
    "    \n",
    "    \n",
    "################################################################\n",
    "\n",
    "\n",
    "def head_batched_attention_mechanism(Q, K, V):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Q: (batch_size, num_heads, num_layer, activation_size)\n",
    "        K: (batch_size, num_heads, num_layer, activation_size)\n",
    "        V: (batch_size, num_heads, activation_size, 1) # activations in the current layer\n",
    "\n",
    "    Returns:\n",
    "        attention: (batch_size, num_heads, activation_size)\n",
    "\n",
    "        # attention mechanism\n",
    "        # # (batch_size, num_heads, activation_size, activation_size)\n",
    "        # attention = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        # attention = attention / torch.sqrt(torch.tensor(activation_size).float())\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = nn.Softmax(dim=-1)(attention)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = torch.matmul(attention, V)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size)\n",
    "        # attention = attention.squeeze(-1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # with probability p\n",
    "    p = torch.rand(1)\n",
    "    \n",
    "    # p <= 0.6 apply the mutation only\n",
    "    if p <= 0.6:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                    mutate_attention_map(torch.matmul( \n",
    "                                Q , \n",
    "                                K.transpose(-1,-2)\n",
    "                    )/torch.sqrt(torch.tensor(8)))    \n",
    "                ) @ V).squeeze(-1)\n",
    "        \n",
    "    # p > 0.6 apply the crossover only\n",
    "    else:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                attention_map_crossover(torch.matmul( \n",
    "                            Q , \n",
    "                            K.transpose(-1,-2)\n",
    "                )/torch.sqrt(torch.tensor(8)))    \n",
    "            ) @ V).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_emb, n_head) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert dim_emb % n_head == 0, 'dim_emb must be divisible by n_head'\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.W_Q = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W_K = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W_V = nn.Linear(dim_emb, dim_emb)\n",
    "\n",
    "        self.W_O = nn.Linear(dim_emb*n_head, dim_emb)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # get the shape of the input\n",
    "        batch_size, activation_size, activation_size = Q.size()\n",
    "        \n",
    "        # check the device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # move to device\n",
    "        Q.to(device)\n",
    "        K.to(device)\n",
    "        V.to(device)\n",
    "        \n",
    "        # reshape Q, K, V\n",
    "        # parallelize over the number of heads\n",
    "        # (batch_size, num_heads, num_layer, activation_size)\n",
    "        Q = torch.stack([Q for _ in range(self.n_head)], 1)\n",
    "        K = torch.stack([K for _ in range(self.n_head)], 1)\n",
    "        V = torch.stack([V for _ in range(self.n_head)], 1)\n",
    "\n",
    "        # apply linear transformation\n",
    "        Q = self.W_Q(Q)\n",
    "        K = self.W_K(K)\n",
    "        V = self.W_V(V).reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        \n",
    "        # apply attention mechanism\n",
    "        out_attention = head_batched_attention_mechanism(Q, K, V).reshape(batch_size, self.n_head*activation_size)\n",
    "\n",
    "        # apply linear transformation\n",
    "        return self.W_O(out_attention)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, 16)\n",
    "        self.l2 = LinW(in_features=16, out_features=16, depth=0)\n",
    "        self.l3 = LinW(in_features=16, out_features=16, depth=1, layers=[self.l2])\n",
    "        self.l4 = nn.Linear(16, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.l1(x)\n",
    "        repr.append(x)\n",
    "        x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        # x = self.gelu(self.l2(x, repr))\n",
    "        x = self.l2(x, repr)\n",
    "        repr.append(x)\n",
    "        x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, depth, layers=[]):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.depth = depth\n",
    "        self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "        self.mha = MultiHeadAttention(in_features, 2)\n",
    "\n",
    "    def forward(self, input, activations=[]):\n",
    "        activations = get_layer_activations(activations)\n",
    "        return F.linear(self.mha(activations, activations, input), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 7\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I would like to test if adding ADD & NORM when we perform the crossover improve the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "MLPWD(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (l1): Linear(in_features=3072, out_features=64, bias=True)\n",
      "  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  (l3): LinW(\n",
      "    in_features=64, out_features=64, bias=True\n",
      "    (mha): MultiHeadAttention(\n",
      "      (W_O): Linear(in_features=128, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (l4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (gelu): GELU(approximate='none')\n",
      ")\n",
      "Epoch 1/7, Training Loss: 1.9327, Training Accuracy: 28.63%, Test accuracy: 36.79%\n",
      "Epoch 2/7, Training Loss: 1.6924, Training Accuracy: 38.79%, Test accuracy: 40.88%\n",
      "Epoch 3/7, Training Loss: 1.6092, Training Accuracy: 42.26%, Test accuracy: 42.21%\n",
      "Epoch 4/7, Training Loss: 1.5496, Training Accuracy: 44.27%, Test accuracy: 44.44%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 357\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mprint\u001b[39m(model)\n\u001b[1;32m    356\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m--> 357\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(model, train_loader, criterion, optimizer, device)\n\u001b[1;32m    358\u001b[0m     test_accuracy \u001b[39m=\u001b[39m evaluate(device, model, test_loader)\n\u001b[1;32m    359\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mEPOCHS\u001b[39m}\u001b[39;00m\u001b[39m, Training Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Training Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%, Test accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtest_accuracy\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 38\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     36\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     37\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 38\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     39\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     40\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m images\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def evaluate(device, model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / total\n",
    "    return train_loss, accuracy\n",
    "\n",
    "### GLOBAL VARIABLES\n",
    "CROSSOVER_MAGNITUDE = 0.3\n",
    "MUTATION_FACTOR = 0.3\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "\n",
    "def attention_map_crossover(attention_map):\n",
    "    \"\"\" Apply the crossover over the attention maps of each head.\n",
    "    The crosseover consists in picking a random index in the maxtrix over\n",
    "    the columns and swapping the values in between the columns of the\n",
    "    attention map.\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # get the crossover magnitude\n",
    "    crossover_magnitude = CROSSOVER_MAGNITUDE\n",
    "    \n",
    "    # get the batch size\n",
    "    dim_batch = attention_map.shape[0]\n",
    "    \n",
    "    # get the number of heads\n",
    "    number_of_heads = attention_map.shape[1]\n",
    "    \n",
    "    for idx_batch in range(dim_batch):\n",
    "        for idx_head in range(number_of_heads):\n",
    "            \n",
    "            # get the crossover index\n",
    "            crossover_index = attention_map.shape[2] - int(attention_map.shape[2]*crossover_magnitude)\n",
    "            \n",
    "            # get two random indexes\n",
    "            random_index_1 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            random_index_2 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            \n",
    "            # swap the values in that position over the columns\n",
    "            for idx, (x_1, x_2) in enumerate(zip(attention_map[idx_batch][idx_head][random_index_1][crossover_index:].detach(), attention_map[idx_batch][idx_head][random_index_2][crossover_index:].detach())):\n",
    "                \n",
    "                # debug\n",
    "                # print(attention_map[idx_batch][idx_head].shape, random_index_1, random_index_2, crossover_index)         \n",
    "                # print(x_1, x_2,idx_batch, idx_head, idx)\n",
    "                \n",
    "                # swap the values in that position over the columns\n",
    "                attention_map[idx_batch][idx_head][random_index_1][crossover_index+idx] = x_2 # make crossover\n",
    "                attention_map[idx_batch][idx_head][random_index_2][crossover_index+idx] = x_1 # make crossover\n",
    "    \n",
    "    return attention_map\n",
    "\n",
    "################################################################\n",
    "\n",
    "def mutate_attention_map(attention_map):\n",
    "    \"\"\" Mutate the attention map by making an elementwise multiplication with \n",
    "    a random tensor with values between 1-mutation_factor and 1+mutation_factor\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, num_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:    \n",
    "        torch.Tensor: shape (batch_size, num_heads, activation_size, activation_size)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # get the mutation factor\n",
    "    mutation_factor = MUTATION_FACTOR\n",
    "    # return the mutated attention map\n",
    "    # multiplied elementwise with a \n",
    "    # random matrix with values between \n",
    "    # 1-mutation_factor and 1+mutation_factor\n",
    "    return torch.mul(attention_map, torch.randn(attention_map.shape).uniform_(1-mutation_factor,1+mutation_factor).to(attention_map.device))\n",
    "    \n",
    "    \n",
    "################################################################\n",
    "\n",
    "\n",
    "def head_batched_attention_mechanism(Q, K, V):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Q: (batch_size, num_heads, num_layer, activation_size)\n",
    "        K: (batch_size, num_heads, num_layer, activation_size)\n",
    "        V: (batch_size, num_heads, activation_size, 1) # activations in the current layer\n",
    "\n",
    "    Returns:\n",
    "        attention: (batch_size, num_heads, activation_size)\n",
    "\n",
    "        # attention mechanism\n",
    "        # # (batch_size, num_heads, activation_size, activation_size)\n",
    "        # attention = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        # attention = attention / torch.sqrt(torch.tensor(activation_size).float())\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = nn.Softmax(dim=-1)(attention)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = torch.matmul(attention, V)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size)\n",
    "        # attention = attention.squeeze(-1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # with probability p\n",
    "    p = torch.rand(1)\n",
    "    \n",
    "    # p <= 0.6 apply the mutation only\n",
    "    if p <= 0.6:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                    mutate_attention_map(torch.matmul( \n",
    "                                Q.transpose(-1,-2) , \n",
    "                                K\n",
    "                    )/torch.sqrt(torch.tensor(8)))    \n",
    "                ) @ V).squeeze(-1)\n",
    "        \n",
    "    # p > 0.6 apply the crossover only\n",
    "    else:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                attention_map_crossover(torch.matmul( \n",
    "                            Q.transpose(-1,-2) , \n",
    "                            K\n",
    "                )/torch.sqrt(torch.tensor(8)))    \n",
    "            ) @ V).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_emb, n_head) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert dim_emb % n_head == 0, 'dim_emb must be divisible by n_head'\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.n_head = n_head\n",
    "\n",
    "        # self.W_Q = nn.Linear(dim_emb, dim_emb)\n",
    "        # self.W_K = nn.Linear(dim_emb, dim_emb)\n",
    "        # self.W_V = nn.Linear(dim_emb, dim_emb)\n",
    "\n",
    "        self.W_O = nn.Linear(dim_emb*n_head, dim_emb)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # get the shape of the input\n",
    "        batch_size, activation_size, activation_size = Q.size()\n",
    "        \n",
    "        # check the device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # move to device\n",
    "        Q.to(device)\n",
    "        K.to(device)\n",
    "        V.to(device)\n",
    "        \n",
    "        # reshape Q, K, V\n",
    "        # parallelize over the number of heads\n",
    "        # (batch_size, num_heads, num_layer, activation_size)\n",
    "        Q = torch.stack([Q for _ in range(self.n_head)], 1)\n",
    "        K = torch.stack([K for _ in range(self.n_head)], 1)\n",
    "        V = torch.stack([V for _ in range(self.n_head)], 1)\n",
    "\n",
    "        # apply linear transformation\n",
    "        # Q = self.W_Q(Q)\n",
    "        # K = self.W_K(K)\n",
    "        # V = self.W_V(V).reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        V = V.reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        \n",
    "        # apply attention mechanism\n",
    "        out_attention = head_batched_attention_mechanism(Q, K, V).reshape(batch_size, self.n_head*activation_size)\n",
    "\n",
    "        # apply linear transformation\n",
    "        return self.W_O(out_attention)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, 64)\n",
    "        self.layer_norm = nn.LayerNorm(64)\n",
    "        # self.l2 = LinW(in_features=64, out_features=64, depth=0)\n",
    "        # self.l2 = nn.Linear(64, 64)\n",
    "        # self.l3 = LinW(in_features=64, out_features=64, depth=1, layers=[self.l2])\n",
    "        self.l3 = LinW(in_features=64, out_features=64)\n",
    "        self.l4 = nn.Linear(64, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        # self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x)\n",
    "        x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        # x = self.gelu(self.l2(x, repr))\n",
    "        # x = self.l2(x, repr)\n",
    "        # x = self.l2(x)\n",
    "        # x = self.layer_norm(x)\n",
    "        # repr.append(x)\n",
    "        # x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        # self.depth = depth\n",
    "        # self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "        self.mha = MultiHeadAttention(in_features, 2)\n",
    "\n",
    "    def forward(self, input, activations=[]):\n",
    "        activations = get_layer_activations(activations)\n",
    "        return F.linear(self.mha(activations, activations, input), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 7\n",
    "BATCH_SIZE = 120\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "print(model)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "MLPWD(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (l1): Linear(in_features=3072, out_features=8, bias=True)\n",
      "  (layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "  (l3): LinW(\n",
      "    in_features=8, out_features=8, bias=True\n",
      "    (mha): MultiHeadAttention(\n",
      "      (W_O): Linear(in_features=16, out_features=8, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (l4): Linear(in_features=8, out_features=10, bias=True)\n",
      "  (gelu): GELU(approximate='none')\n",
      ")\n",
      "Epoch 1/7, Training Loss: 2.1721, Training Accuracy: 15.29%, Test accuracy: 21.48%\n",
      "Epoch 2/7, Training Loss: 1.9987, Training Accuracy: 24.03%, Test accuracy: 25.82%\n",
      "Epoch 3/7, Training Loss: 1.9060, Training Accuracy: 28.45%, Test accuracy: 30.82%\n",
      "Epoch 4/7, Training Loss: 1.8440, Training Accuracy: 31.72%, Test accuracy: 31.34%\n",
      "Epoch 5/7, Training Loss: 1.8081, Training Accuracy: 33.39%, Test accuracy: 32.09%\n",
      "Epoch 6/7, Training Loss: 1.7944, Training Accuracy: 33.57%, Test accuracy: 33.25%\n",
      "Epoch 7/7, Training Loss: 1.7726, Training Accuracy: 34.57%, Test accuracy: 34.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def evaluate(device, model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / total\n",
    "    return train_loss, accuracy\n",
    "\n",
    "### GLOBAL VARIABLES\n",
    "CROSSOVER_MAGNITUDE = 0.3\n",
    "MUTATION_FACTOR = 0.3\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "\n",
    "def attention_map_crossover(attention_map):\n",
    "    \"\"\" Apply the crossover over the attention maps of each head.\n",
    "    The crosseover consists in picking a random index in the maxtrix over\n",
    "    the columns and swapping the values in between the columns of the\n",
    "    attention map.\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # get the crossover magnitude\n",
    "    crossover_magnitude = CROSSOVER_MAGNITUDE\n",
    "    \n",
    "    # get the batch size\n",
    "    dim_batch = attention_map.shape[0]\n",
    "    \n",
    "    # get the number of heads\n",
    "    number_of_heads = attention_map.shape[1]\n",
    "    \n",
    "    for idx_batch in range(dim_batch):\n",
    "        for idx_head in range(number_of_heads):\n",
    "            \n",
    "            # get the crossover index\n",
    "            crossover_index = attention_map.shape[2] - int(attention_map.shape[2]*crossover_magnitude)\n",
    "            \n",
    "            # get two random indexes\n",
    "            random_index_1 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            random_index_2 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            \n",
    "            # swap the values in that position over the columns\n",
    "            for idx, (x_1, x_2) in enumerate(zip(attention_map[idx_batch][idx_head][random_index_1][crossover_index:].detach(), attention_map[idx_batch][idx_head][random_index_2][crossover_index:].detach())):\n",
    "                \n",
    "                # debug\n",
    "                # print(attention_map[idx_batch][idx_head].shape, random_index_1, random_index_2, crossover_index)         \n",
    "                # print(x_1, x_2,idx_batch, idx_head, idx)\n",
    "                \n",
    "                # swap the values in that position over the columns\n",
    "                attention_map[idx_batch][idx_head][random_index_1][crossover_index+idx] = x_2 # make crossover\n",
    "                attention_map[idx_batch][idx_head][random_index_2][crossover_index+idx] = x_1 # make crossover\n",
    "    \n",
    "    return attention_map\n",
    "\n",
    "################################################################\n",
    "\n",
    "def mutate_attention_map(attention_map):\n",
    "    \"\"\" Mutate the attention map by making an elementwise multiplication with \n",
    "    a random tensor with values between 1-mutation_factor and 1+mutation_factor\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, num_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:    \n",
    "        torch.Tensor: shape (batch_size, num_heads, activation_size, activation_size)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # get the mutation factor\n",
    "    mutation_factor = MUTATION_FACTOR\n",
    "    # return the mutated attention map\n",
    "    # multiplied elementwise with a \n",
    "    # random matrix with values between \n",
    "    # 1-mutation_factor and 1+mutation_factor\n",
    "    return torch.mul(attention_map, torch.randn(attention_map.shape).uniform_(1-mutation_factor,1+mutation_factor).to(attention_map.device))\n",
    "    \n",
    "    \n",
    "################################################################\n",
    "\n",
    "\n",
    "def head_batched_attention_mechanism(Q, K, V):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Q: (batch_size, num_heads, num_layer, activation_size)\n",
    "        K: (batch_size, num_heads, num_layer, activation_size)\n",
    "        V: (batch_size, num_heads, activation_size, 1) # activations in the current layer\n",
    "\n",
    "    Returns:\n",
    "        attention: (batch_size, num_heads, activation_size)\n",
    "\n",
    "        # attention mechanism\n",
    "        # # (batch_size, num_heads, activation_size, activation_size)\n",
    "        # attention = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        # attention = attention / torch.sqrt(torch.tensor(activation_size).float())\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = nn.Softmax(dim=-1)(attention)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = torch.matmul(attention, V)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size)\n",
    "        # attention = attention.squeeze(-1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # with probability p\n",
    "    p = torch.rand(1)\n",
    "    \n",
    "    # p <= 0.6 apply the mutation only\n",
    "    if p <= 0.6:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                    mutate_attention_map(torch.matmul( \n",
    "                                Q , \n",
    "                                K.transpose(-1,-2)\n",
    "                    )/torch.sqrt(torch.tensor(8)))    \n",
    "                ) @ V).squeeze(-1)\n",
    "        \n",
    "    # p > 0.6 apply the crossover only\n",
    "    else:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                attention_map_crossover(torch.matmul( \n",
    "                            Q , \n",
    "                            K.transpose(-1,-2)\n",
    "                )/torch.sqrt(torch.tensor(8)))    \n",
    "            ) @ V).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_emb, n_head) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert dim_emb % n_head == 0, 'dim_emb must be divisible by n_head'\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.n_head = n_head\n",
    "\n",
    "        # self.W_Q = nn.Linear(dim_emb, dim_emb)\n",
    "        # self.W_K = nn.Linear(dim_emb, dim_emb)\n",
    "        # self.W_V = nn.Linear(dim_emb, dim_emb)\n",
    "\n",
    "        self.W_O = nn.Linear(dim_emb*n_head, dim_emb)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # get the shape of the input\n",
    "        batch_size, activation_size, activation_size = Q.size()\n",
    "        \n",
    "        # check the device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # move to device\n",
    "        Q.to(device)\n",
    "        K.to(device)\n",
    "        V.to(device)\n",
    "        \n",
    "        # reshape Q, K, V\n",
    "        # parallelize over the number of heads\n",
    "        # (batch_size, num_heads, num_layer, activation_size)\n",
    "        Q = torch.stack([Q for _ in range(self.n_head)], 1)\n",
    "        K = torch.stack([K for _ in range(self.n_head)], 1)\n",
    "        V = torch.stack([V for _ in range(self.n_head)], 1)\n",
    "\n",
    "        # apply linear transformation\n",
    "        # Q = self.W_Q(Q)\n",
    "        # K = self.W_K(K)\n",
    "        # V = self.W_V(V).reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        V = V.reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        \n",
    "        # apply attention mechanism\n",
    "        out_attention = head_batched_attention_mechanism(Q, K, V).reshape(batch_size, self.n_head*activation_size)\n",
    "\n",
    "        # apply linear transformation\n",
    "        return self.W_O(out_attention)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, 8)\n",
    "        self.layer_norm = nn.LayerNorm(8)\n",
    "        # self.l2 = LinW(in_features=64, out_features=64, depth=0)\n",
    "        # self.l2 = nn.Linear(64, 64)\n",
    "        # self.l3 = LinW(in_features=64, out_features=64, depth=1, layers=[self.l2])\n",
    "        self.l3 = LinW(in_features=8, out_features=8)\n",
    "        self.l4 = nn.Linear(8, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        # self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x)\n",
    "        x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        # x = self.gelu(self.l2(x, repr))\n",
    "        # x = self.l2(x, repr)\n",
    "        # x = self.l2(x)\n",
    "        # x = self.layer_norm(x)\n",
    "        # repr.append(x)\n",
    "        # x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        # self.depth = depth\n",
    "        # self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "        self.mha = MultiHeadAttention(in_features, 2)\n",
    "\n",
    "    def forward(self, input, activations=[]):\n",
    "        activations = get_layer_activations(activations)\n",
    "        return F.linear(self.mha(activations, activations, input), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 7\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "print(model)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "MLPWD(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (l1): Linear(in_features=3072, out_features=8, bias=True)\n",
      "  (l4): Linear(in_features=8, out_features=10, bias=True)\n",
      "  (gelu): GELU(approximate='none')\n",
      ")\n",
      "Epoch 1/7, Training Loss: 2.1802, Training Accuracy: 16.14%, Test accuracy: 14.07%\n",
      "Epoch 2/7, Training Loss: 2.1021, Training Accuracy: 17.15%, Test accuracy: 17.51%\n",
      "Epoch 3/7, Training Loss: 2.0839, Training Accuracy: 17.97%, Test accuracy: 18.44%\n",
      "Epoch 4/7, Training Loss: 2.0747, Training Accuracy: 17.81%, Test accuracy: 18.64%\n",
      "Epoch 5/7, Training Loss: 2.0711, Training Accuracy: 18.03%, Test accuracy: 17.86%\n",
      "Epoch 6/7, Training Loss: 2.0672, Training Accuracy: 18.27%, Test accuracy: 18.45%\n",
      "Epoch 7/7, Training Loss: 2.0661, Training Accuracy: 18.37%, Test accuracy: 18.07%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def evaluate(device, model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / total\n",
    "    return train_loss, accuracy\n",
    "\n",
    "### GLOBAL VARIABLES\n",
    "CROSSOVER_MAGNITUDE = 0.3\n",
    "MUTATION_FACTOR = 0.3\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "\n",
    "def attention_map_crossover(attention_map):\n",
    "    \"\"\" Apply the crossover over the attention maps of each head.\n",
    "    The crosseover consists in picking a random index in the maxtrix over\n",
    "    the columns and swapping the values in between the columns of the\n",
    "    attention map.\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # get the crossover magnitude\n",
    "    crossover_magnitude = CROSSOVER_MAGNITUDE\n",
    "    \n",
    "    # get the batch size\n",
    "    dim_batch = attention_map.shape[0]\n",
    "    \n",
    "    # get the number of heads\n",
    "    number_of_heads = attention_map.shape[1]\n",
    "    \n",
    "    for idx_batch in range(dim_batch):\n",
    "        for idx_head in range(number_of_heads):\n",
    "            \n",
    "            # get the crossover index\n",
    "            crossover_index = attention_map.shape[2] - int(attention_map.shape[2]*crossover_magnitude)\n",
    "            \n",
    "            # get two random indexes\n",
    "            random_index_1 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            random_index_2 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            \n",
    "            # swap the values in that position over the columns\n",
    "            for idx, (x_1, x_2) in enumerate(zip(attention_map[idx_batch][idx_head][random_index_1][crossover_index:].detach(), attention_map[idx_batch][idx_head][random_index_2][crossover_index:].detach())):\n",
    "                \n",
    "                # debug\n",
    "                # print(attention_map[idx_batch][idx_head].shape, random_index_1, random_index_2, crossover_index)         \n",
    "                # print(x_1, x_2,idx_batch, idx_head, idx)\n",
    "                \n",
    "                # swap the values in that position over the columns\n",
    "                attention_map[idx_batch][idx_head][random_index_1][crossover_index+idx] = x_2 # make crossover\n",
    "                attention_map[idx_batch][idx_head][random_index_2][crossover_index+idx] = x_1 # make crossover\n",
    "    \n",
    "    return attention_map\n",
    "\n",
    "################################################################\n",
    "\n",
    "def mutate_attention_map(attention_map):\n",
    "    \"\"\" Mutate the attention map by making an elementwise multiplication with \n",
    "    a random tensor with values between 1-mutation_factor and 1+mutation_factor\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, num_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:    \n",
    "        torch.Tensor: shape (batch_size, num_heads, activation_size, activation_size)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # get the mutation factor\n",
    "    mutation_factor = MUTATION_FACTOR\n",
    "    # return the mutated attention map\n",
    "    # multiplied elementwise with a \n",
    "    # random matrix with values between \n",
    "    # 1-mutation_factor and 1+mutation_factor\n",
    "    return torch.mul(attention_map, torch.randn(attention_map.shape).uniform_(1-mutation_factor,1+mutation_factor).to(attention_map.device))\n",
    "    \n",
    "    \n",
    "################################################################\n",
    "\n",
    "\n",
    "def head_batched_attention_mechanism(Q, K, V):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Q: (batch_size, num_heads, num_layer, activation_size)\n",
    "        K: (batch_size, num_heads, num_layer, activation_size)\n",
    "        V: (batch_size, num_heads, activation_size, 1) # activations in the current layer\n",
    "\n",
    "    Returns:\n",
    "        attention: (batch_size, num_heads, activation_size)\n",
    "\n",
    "        # attention mechanism\n",
    "        # # (batch_size, num_heads, activation_size, activation_size)\n",
    "        # attention = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        # attention = attention / torch.sqrt(torch.tensor(activation_size).float())\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = nn.Softmax(dim=-1)(attention)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = torch.matmul(attention, V)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size)\n",
    "        # attention = attention.squeeze(-1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # with probability p\n",
    "    p = torch.rand(1)\n",
    "    \n",
    "    # p <= 0.6 apply the mutation only\n",
    "    if p <= 0.6:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                    mutate_attention_map(torch.matmul( \n",
    "                                Q.transpose(-1,-2) , \n",
    "                                K\n",
    "                    )/torch.sqrt(torch.tensor(8)))    \n",
    "                ) @ V).squeeze(-1)\n",
    "        \n",
    "    # p > 0.6 apply the crossover only\n",
    "    else:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                attention_map_crossover(torch.matmul( \n",
    "                            Q.transpose(-1,-2) , \n",
    "                            K\n",
    "                )/torch.sqrt(torch.tensor(8)))    \n",
    "            ) @ V).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_emb, n_head) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert dim_emb % n_head == 0, 'dim_emb must be divisible by n_head'\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.n_head = n_head\n",
    "\n",
    "        # self.W_Q = nn.Linear(dim_emb, dim_emb)\n",
    "        # self.W_K = nn.Linear(dim_emb, dim_emb)\n",
    "        # self.W_V = nn.Linear(dim_emb, dim_emb)\n",
    "\n",
    "        self.W_O = nn.Linear(dim_emb*n_head, dim_emb)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # get the shape of the input\n",
    "        batch_size, activation_size, activation_size = Q.size()\n",
    "        \n",
    "        # check the device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # move to device\n",
    "        Q.to(device)\n",
    "        K.to(device)\n",
    "        V.to(device)\n",
    "        \n",
    "        # reshape Q, K, V\n",
    "        # parallelize over the number of heads\n",
    "        # (batch_size, num_heads, num_layer, activation_size)\n",
    "        Q = torch.stack([Q for _ in range(self.n_head)], 1)\n",
    "        K = torch.stack([K for _ in range(self.n_head)], 1)\n",
    "        V = torch.stack([V for _ in range(self.n_head)], 1)\n",
    "\n",
    "        # apply linear transformation\n",
    "        # Q = self.W_Q(Q)\n",
    "        # K = self.W_K(K)\n",
    "        # V = self.W_V(V).reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        V = V.reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        \n",
    "        # apply attention mechanism\n",
    "        out_attention = head_batched_attention_mechanism(Q, K, V).reshape(batch_size, self.n_head*activation_size)\n",
    "\n",
    "        # apply linear transformation\n",
    "        return self.W_O(out_attention)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, 8)\n",
    "        # self.layer_norm = nn.LayerNorm(8)\n",
    "        # self.l2 = LinW(in_features=64, out_features=64, depth=0)\n",
    "        # self.l2 = nn.Linear(64, 64)\n",
    "        # self.l3 = LinW(in_features=64, out_features=64, depth=1, layers=[self.l2])\n",
    "        # self.l3 = LinW(in_features=8, out_features=8)\n",
    "        self.l4 = nn.Linear(8, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        # self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.l1(x)\n",
    "        # x = self.layer_norm(x)\n",
    "        # repr.append(x)\n",
    "        x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        # x = self.gelu(self.l2(x, repr))\n",
    "        # x = self.l2(x, repr)\n",
    "        # x = self.l2(x)\n",
    "        # x = self.layer_norm(x)\n",
    "        # repr.append(x)\n",
    "        # x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        # x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        # self.depth = depth\n",
    "        # self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "        self.mha = MultiHeadAttention(in_features, 2)\n",
    "\n",
    "    def forward(self, input, activations=[]):\n",
    "        activations = get_layer_activations(activations)\n",
    "        return F.linear(self.mha(activations, activations, input), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 7\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "print(model)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "MLPWD(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (l1): Linear(in_features=3072, out_features=16, bias=True)\n",
      "  (layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "  (l3): LinW(\n",
      "    in_features=16, out_features=16, bias=True\n",
      "    (mha): MultiHeadAttention(\n",
      "      (W_O): Linear(in_features=32, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (l4): Linear(in_features=16, out_features=10, bias=True)\n",
      "  (gelu): GELU(approximate='none')\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n",
      "0.05017 M\n",
      "Epoch 1/7, Training Loss: 2.3024, Training Accuracy: 11.24%, Test accuracy: 11.34%\n",
      "Epoch 2/7, Training Loss: 2.3024, Training Accuracy: 11.13%, Test accuracy: 11.46%\n",
      "Epoch 3/7, Training Loss: 2.3024, Training Accuracy: 11.28%, Test accuracy: 11.50%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 360\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39msum\u001b[39m(p\u001b[39m.\u001b[39mnumel() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mparameters() \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mrequires_grad)\u001b[39m/\u001b[39m\u001b[39m1000000.0\u001b[39m , \u001b[39m\"\u001b[39m\u001b[39mM\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    359\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m--> 360\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(model, train_loader, criterion, optimizer, lr_scheduler, device)\n\u001b[1;32m    361\u001b[0m     test_accuracy \u001b[39m=\u001b[39m evaluate(device, model, test_loader)\n\u001b[1;32m    362\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mEPOCHS\u001b[39m}\u001b[39;00m\u001b[39m, Training Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Training Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%, Test accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtest_accuracy\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 36\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, lr_scheduler, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m images, labels \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     35\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 36\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     37\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     38\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 314\u001b[0m, in \u001b[0;36mMLPWD.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mrepr\u001b[39m\u001b[39m.\u001b[39mappend(x)\n\u001b[1;32m    306\u001b[0m \u001b[39m# repr.append(x.detach().cpu().numpy())\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[39m# x = self.gelu(self.l2(x, repr))\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39m# x = self.l2(x, repr)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39m# x = self.gelu(x)\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m# repr.append(x.detach().cpu().numpy())\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml3(x, \u001b[39mrepr\u001b[39;49m))\n\u001b[1;32m    315\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml4(x)\n\u001b[1;32m    316\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoftmax(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 335\u001b[0m, in \u001b[0;36mLinW.forward\u001b[0;34m(self, input, activations)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, activations\u001b[39m=\u001b[39m[]):\n\u001b[1;32m    334\u001b[0m     activations \u001b[39m=\u001b[39m get_layer_activations(activations)\n\u001b[0;32m--> 335\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmha(activations, activations, \u001b[39minput\u001b[39;49m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 220\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, Q, K, V)\u001b[0m\n\u001b[1;32m    217\u001b[0m V \u001b[39m=\u001b[39m V\u001b[39m.\u001b[39mreshape(batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_head, activation_size, \u001b[39m1\u001b[39m)\n\u001b[1;32m    219\u001b[0m \u001b[39m# apply attention mechanism\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m out_attention \u001b[39m=\u001b[39m head_batched_attention_mechanism(Q, K, V)\u001b[39m.\u001b[39mreshape(batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_head\u001b[39m*\u001b[39mactivation_size)\n\u001b[1;32m    222\u001b[0m \u001b[39m# apply linear transformation\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_O(out_attention)\n",
      "Cell \u001b[0;32mIn[3], line 171\u001b[0m, in \u001b[0;36mhead_batched_attention_mechanism\u001b[0;34m(Q, K, V)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[39mreturn\u001b[39;00m (nn\u001b[39m.\u001b[39mSoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)(\n\u001b[1;32m    162\u001b[0m                 mutate_attention_map(torch\u001b[39m.\u001b[39mmatmul( \n\u001b[1;32m    163\u001b[0m                             Q\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m) , \n\u001b[1;32m    164\u001b[0m                             K\n\u001b[1;32m    165\u001b[0m                 )\u001b[39m/\u001b[39mtorch\u001b[39m.\u001b[39msqrt(torch\u001b[39m.\u001b[39mtensor(\u001b[39m8\u001b[39m)))    \n\u001b[1;32m    166\u001b[0m             ) \u001b[39m@\u001b[39m V)\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    168\u001b[0m \u001b[39m# p > 0.6 apply the crossover only\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[39mreturn\u001b[39;00m (nn\u001b[39m.\u001b[39mSoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)(\n\u001b[0;32m--> 171\u001b[0m             attention_map_crossover(torch\u001b[39m.\u001b[39;49mmatmul( \n\u001b[1;32m    172\u001b[0m                         Q\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m) , \n\u001b[1;32m    173\u001b[0m                         K\n\u001b[1;32m    174\u001b[0m             )\u001b[39m/\u001b[39;49mtorch\u001b[39m.\u001b[39;49msqrt(torch\u001b[39m.\u001b[39;49mtensor(\u001b[39m8\u001b[39;49m)))    \n\u001b[1;32m    175\u001b[0m         ) \u001b[39m@\u001b[39m V)\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 100\u001b[0m, in \u001b[0;36mattention_map_crossover\u001b[0;34m(attention_map)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[39mfor\u001b[39;00m idx, (x_1, x_2) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(attention_map[idx_batch][idx_head][random_index_1][crossover_index:]\u001b[39m.\u001b[39mdetach(), attention_map[idx_batch][idx_head][random_index_2][crossover_index:]\u001b[39m.\u001b[39mdetach())):\n\u001b[1;32m     93\u001b[0m             \n\u001b[1;32m     94\u001b[0m             \u001b[39m# debug\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \n\u001b[1;32m     98\u001b[0m             \u001b[39m# swap the values in that position over the columns\u001b[39;00m\n\u001b[1;32m     99\u001b[0m             attention_map[idx_batch][idx_head][random_index_1][crossover_index\u001b[39m+\u001b[39midx] \u001b[39m=\u001b[39m x_2 \u001b[39m# make crossover\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m             attention_map[idx_batch][idx_head][random_index_2][crossover_index\u001b[39m+\u001b[39midx] \u001b[39m=\u001b[39m x_1 \u001b[39m# make crossover\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[39mreturn\u001b[39;00m attention_map\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def evaluate(device, model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, lr_scheduler, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / total\n",
    "    return train_loss, accuracy\n",
    "\n",
    "### GLOBAL VARIABLES\n",
    "CROSSOVER_MAGNITUDE = 0.3\n",
    "MUTATION_FACTOR = 0.3\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "\n",
    "def attention_map_crossover(attention_map):\n",
    "    \"\"\" Apply the crossover over the attention maps of each head.\n",
    "    The crosseover consists in picking a random index in the maxtrix over\n",
    "    the columns and swapping the values in between the columns of the\n",
    "    attention map.\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # get the crossover magnitude\n",
    "    crossover_magnitude = CROSSOVER_MAGNITUDE\n",
    "    \n",
    "    # get the batch size\n",
    "    dim_batch = attention_map.shape[0]\n",
    "    \n",
    "    # get the number of heads\n",
    "    number_of_heads = attention_map.shape[1]\n",
    "    \n",
    "    for idx_batch in range(dim_batch):\n",
    "        for idx_head in range(number_of_heads):\n",
    "            \n",
    "            # get the crossover index\n",
    "            crossover_index = attention_map.shape[2] - int(attention_map.shape[2]*crossover_magnitude)\n",
    "            \n",
    "            # get two random indexes\n",
    "            random_index_1 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            random_index_2 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            \n",
    "            # swap the values in that position over the columns\n",
    "            for idx, (x_1, x_2) in enumerate(zip(attention_map[idx_batch][idx_head][random_index_1][crossover_index:].detach(), attention_map[idx_batch][idx_head][random_index_2][crossover_index:].detach())):\n",
    "                \n",
    "                # debug\n",
    "                # print(attention_map[idx_batch][idx_head].shape, random_index_1, random_index_2, crossover_index)         \n",
    "                # print(x_1, x_2,idx_batch, idx_head, idx)\n",
    "                \n",
    "                # swap the values in that position over the columns\n",
    "                attention_map[idx_batch][idx_head][random_index_1][crossover_index+idx] = x_2 # make crossover\n",
    "                attention_map[idx_batch][idx_head][random_index_2][crossover_index+idx] = x_1 # make crossover\n",
    "    \n",
    "    return attention_map\n",
    "\n",
    "################################################################\n",
    "\n",
    "def mutate_attention_map(attention_map):\n",
    "    \"\"\" Mutate the attention map by making an elementwise multiplication with \n",
    "    a random tensor with values between 1-mutation_factor and 1+mutation_factor\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, num_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:    \n",
    "        torch.Tensor: shape (batch_size, num_heads, activation_size, activation_size)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # get the mutation factor\n",
    "    mutation_factor = MUTATION_FACTOR\n",
    "    # return the mutated attention map\n",
    "    # multiplied elementwise with a \n",
    "    # random matrix with values between \n",
    "    # 1-mutation_factor and 1+mutation_factor\n",
    "    return torch.mul(attention_map, torch.randn(attention_map.shape).uniform_(1-mutation_factor,1+mutation_factor).to(attention_map.device))\n",
    "    \n",
    "    \n",
    "################################################################\n",
    "\n",
    "\n",
    "def head_batched_attention_mechanism(Q, K, V):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Q: (batch_size, num_heads, num_layer, activation_size)\n",
    "        K: (batch_size, num_heads, num_layer, activation_size)\n",
    "        V: (batch_size, num_heads, activation_size, 1) # activations in the current layer\n",
    "\n",
    "    Returns:\n",
    "        attention: (batch_size, num_heads, activation_size)\n",
    "\n",
    "        # attention mechanism\n",
    "        # # (batch_size, num_heads, activation_size, activation_size)\n",
    "        # attention = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        # attention = attention / torch.sqrt(torch.tensor(activation_size).float())\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = nn.Softmax(dim=-1)(attention)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = torch.matmul(attention, V)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size)\n",
    "        # attention = attention.squeeze(-1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # with probability p\n",
    "    p = torch.rand(1)\n",
    "    \n",
    "    # p <= 0.6 apply the mutation only\n",
    "    if p <= 0.6:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                    mutate_attention_map(torch.matmul( \n",
    "                                Q.transpose(-1,-2) , \n",
    "                                K\n",
    "                    )/torch.sqrt(torch.tensor(8)))    \n",
    "                ) @ V).squeeze(-1)\n",
    "        \n",
    "    # p > 0.6 apply the crossover only\n",
    "    else:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                attention_map_crossover(torch.matmul( \n",
    "                            Q.transpose(-1,-2) , \n",
    "                            K\n",
    "                )/torch.sqrt(torch.tensor(8)))    \n",
    "            ) @ V).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_emb, n_head) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert dim_emb % n_head == 0, 'dim_emb must be divisible by n_head'\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.n_head = n_head\n",
    "\n",
    "        # self.W_Q = nn.Linear(dim_emb, dim_emb)\n",
    "        # self.W_K = nn.Linear(dim_emb, dim_emb)\n",
    "        # self.W_V = nn.Linear(dim_emb, dim_emb)\n",
    "\n",
    "        self.W_O = nn.Linear(dim_emb*n_head, dim_emb)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # get the shape of the input\n",
    "        batch_size, activation_size, activation_size = Q.size()\n",
    "        \n",
    "        # check the device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # move to device\n",
    "        Q.to(device)\n",
    "        K.to(device)\n",
    "        V.to(device)\n",
    "        \n",
    "        # reshape Q, K, V\n",
    "        # parallelize over the number of heads\n",
    "        # (batch_size, num_heads, num_layer, activation_size)\n",
    "        Q = torch.stack([Q for _ in range(self.n_head)], 1)\n",
    "        K = torch.stack([K for _ in range(self.n_head)], 1)\n",
    "        V = torch.stack([V for _ in range(self.n_head)], 1)\n",
    "\n",
    "        # apply linear transformation\n",
    "        # Q = self.W_Q(Q)\n",
    "        # K = self.W_K(K)\n",
    "        # V = self.W_V(V).reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        V = V.reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        \n",
    "        # apply attention mechanism\n",
    "        out_attention = head_batched_attention_mechanism(Q, K, V).reshape(batch_size, self.n_head*activation_size)\n",
    "\n",
    "        # apply linear transformation\n",
    "        return self.W_O(out_attention)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, 16)\n",
    "        self.layer_norm = nn.LayerNorm(16)\n",
    "        # self.l2 = LinW(in_features=64, out_features=64, depth=0)\n",
    "        # self.l2 = nn.Linear(64, 64)\n",
    "        # self.l3 = LinW(in_features=64, out_features=64, depth=1, layers=[self.l2])\n",
    "        self.l3 = LinW(in_features=16, out_features=16)\n",
    "        self.l4 = nn.Linear(16, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        # self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.layer_norm(self.gelu(self.l1(x)))\n",
    "        repr.append(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        # x = self.gelu(self.l2(x, repr))\n",
    "        # x = self.l2(x, repr)\n",
    "        # x = self.l2(x)\n",
    "        # x = self.layer_norm(x)\n",
    "        # repr.append(x)\n",
    "        # x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        # self.depth = depth\n",
    "        # self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "        self.mha = MultiHeadAttention(in_features, 2)\n",
    "\n",
    "    def forward(self, input, activations=[]):\n",
    "        activations = get_layer_activations(activations)\n",
    "        return F.linear(self.mha(activations, activations, input), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 7\n",
    "BATCH_SIZE = 120\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "# print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "print(model)\n",
    "# number of parameters\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad)/1000000.0 , \"M\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, lr_scheduler, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "MLPWD(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (l1): Linear(in_features=3072, out_features=16, bias=True)\n",
      "  (layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "  (l3): LinW(\n",
      "    in_features=16, out_features=16, bias=True\n",
      "    (mha): MultiHeadAttention(\n",
      "      (W_O): Linear(in_features=32, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (l4): Linear(in_features=16, out_features=10, bias=True)\n",
      "  (gelu): GELU(approximate='none')\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n",
      "0.05017 M\n",
      "Epoch 1/7, Training Loss: 2.3026, Training Accuracy: 10.00%, Test accuracy: 10.00%\n",
      "Epoch 2/7, Training Loss: 2.3026, Training Accuracy: 10.00%, Test accuracy: 10.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 362\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39msum\u001b[39m(p\u001b[39m.\u001b[39mnumel() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mparameters() \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mrequires_grad)\u001b[39m/\u001b[39m\u001b[39m1000000.0\u001b[39m , \u001b[39m\"\u001b[39m\u001b[39mM\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    361\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m--> 362\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(model, train_loader, criterion, optimizer, lr_scheduler, device)\n\u001b[1;32m    363\u001b[0m     test_accuracy \u001b[39m=\u001b[39m evaluate(device, model, test_loader)\n\u001b[1;32m    364\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mEPOCHS\u001b[39m}\u001b[39;00m\u001b[39m, Training Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Training Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%, Test accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtest_accuracy\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 38\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, lr_scheduler, device)\u001b[0m\n\u001b[1;32m     36\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     37\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 38\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     39\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     40\u001b[0m lr_scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def evaluate(device, model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, lr_scheduler, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / total\n",
    "    return train_loss, accuracy\n",
    "\n",
    "### GLOBAL VARIABLES\n",
    "CROSSOVER_MAGNITUDE = 0.3\n",
    "MUTATION_FACTOR = 0.3\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "\n",
    "def attention_map_crossover(attention_map):\n",
    "    \"\"\" Apply the crossover over the attention maps of each head.\n",
    "    The crosseover consists in picking a random index in the maxtrix over\n",
    "    the columns and swapping the values in between the columns of the\n",
    "    attention map.\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # get the crossover magnitude\n",
    "    crossover_magnitude = CROSSOVER_MAGNITUDE\n",
    "    \n",
    "    # get the batch size\n",
    "    dim_batch = attention_map.shape[0]\n",
    "    \n",
    "    # get the number of heads\n",
    "    number_of_heads = attention_map.shape[1]\n",
    "    \n",
    "    for idx_batch in range(dim_batch):\n",
    "        for idx_head in range(number_of_heads):\n",
    "            \n",
    "            # get the crossover index\n",
    "            crossover_index = attention_map.shape[2] - int(attention_map.shape[2]*crossover_magnitude)\n",
    "            \n",
    "            # get two random indexes\n",
    "            random_index_1 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            random_index_2 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            \n",
    "            # swap the values in that position over the columns\n",
    "            for idx, (x_1, x_2) in enumerate(zip(attention_map[idx_batch][idx_head][random_index_1][crossover_index:].detach(), attention_map[idx_batch][idx_head][random_index_2][crossover_index:].detach())):\n",
    "                \n",
    "                # debug\n",
    "                # print(attention_map[idx_batch][idx_head].shape, random_index_1, random_index_2, crossover_index)         \n",
    "                # print(x_1, x_2,idx_batch, idx_head, idx)\n",
    "                \n",
    "                # swap the values in that position over the columns\n",
    "                attention_map[idx_batch][idx_head][random_index_1][crossover_index+idx] = x_2 # make crossover\n",
    "                attention_map[idx_batch][idx_head][random_index_2][crossover_index+idx] = x_1 # make crossover\n",
    "    \n",
    "    return attention_map\n",
    "\n",
    "################################################################\n",
    "\n",
    "def mutate_attention_map(attention_map):\n",
    "    \"\"\" Mutate the attention map by making an elementwise multiplication with \n",
    "    a random tensor with values between 1-mutation_factor and 1+mutation_factor\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, num_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:    \n",
    "        torch.Tensor: shape (batch_size, num_heads, activation_size, activation_size)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # get the mutation factor\n",
    "    mutation_factor = MUTATION_FACTOR\n",
    "    # return the mutated attention map\n",
    "    # multiplied elementwise with a \n",
    "    # random matrix with values between \n",
    "    # 1-mutation_factor and 1+mutation_factor\n",
    "    return torch.mul(attention_map, torch.randn(attention_map.shape).uniform_(1-mutation_factor,1+mutation_factor).to(attention_map.device))\n",
    "    \n",
    "    \n",
    "################################################################\n",
    "\n",
    "\n",
    "def head_batched_attention_mechanism(Q, K, V):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Q: (batch_size, num_heads, num_layer, activation_size)\n",
    "        K: (batch_size, num_heads, num_layer, activation_size)\n",
    "        V: (batch_size, num_heads, activation_size, 1) # activations in the current layer\n",
    "\n",
    "    Returns:\n",
    "        attention: (batch_size, num_heads, activation_size)\n",
    "\n",
    "        # attention mechanism\n",
    "        # # (batch_size, num_heads, activation_size, activation_size)\n",
    "        # attention = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        # attention = attention / torch.sqrt(torch.tensor(activation_size).float())\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = nn.Softmax(dim=-1)(attention)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = torch.matmul(attention, V)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size)\n",
    "        # attention = attention.squeeze(-1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # with probability p\n",
    "    p = torch.rand(1)\n",
    "    \n",
    "    # p <= 0.6 apply the mutation only\n",
    "    if p <= 0.6:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                    mutate_attention_map(torch.matmul( \n",
    "                                Q.transpose(-1,-2) , \n",
    "                                K\n",
    "                    )/torch.sqrt(torch.tensor(8)))    \n",
    "                ) @ V).squeeze(-1)\n",
    "        \n",
    "    # p > 0.6 apply the crossover only\n",
    "    else:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                attention_map_crossover(torch.matmul( \n",
    "                            Q.transpose(-1,-2) , \n",
    "                            K\n",
    "                )/torch.sqrt(torch.tensor(8)))    \n",
    "            ) @ V).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_emb, n_head) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert dim_emb % n_head == 0, 'dim_emb must be divisible by n_head'\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.n_head = n_head\n",
    "\n",
    "        # self.W_Q = nn.Linear(dim_emb, dim_emb)\n",
    "        # self.W_K = nn.Linear(dim_emb, dim_emb)\n",
    "        # self.W_V = nn.Linear(dim_emb, dim_emb)\n",
    "\n",
    "        self.W_O = nn.Linear(dim_emb*n_head, dim_emb)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # get the shape of the input\n",
    "        batch_size, activation_size, activation_size = Q.size()\n",
    "        \n",
    "        # check the device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # move to device\n",
    "        Q.to(device)\n",
    "        K.to(device)\n",
    "        V.to(device)\n",
    "        \n",
    "        # reshape Q, K, V\n",
    "        # parallelize over the number of heads\n",
    "        # (batch_size, num_heads, num_layer, activation_size)\n",
    "        Q = torch.stack([Q for _ in range(self.n_head)], 1)\n",
    "        K = torch.stack([K for _ in range(self.n_head)], 1)\n",
    "        V = torch.stack([V for _ in range(self.n_head)], 1)\n",
    "\n",
    "        # apply linear transformation\n",
    "        # Q = self.W_Q(Q)\n",
    "        # K = self.W_K(K)\n",
    "        # V = self.W_V(V).reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        V = V.reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        \n",
    "        # apply attention mechanism\n",
    "        out_attention = head_batched_attention_mechanism(Q, K, V).reshape(batch_size, self.n_head*activation_size)\n",
    "\n",
    "        # apply linear transformation\n",
    "        return self.W_O(out_attention)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, 16)\n",
    "        self.layer_norm = nn.LayerNorm(16)\n",
    "        # self.l2 = LinW(in_features=64, out_features=64, depth=0)\n",
    "        # self.l2 = nn.Linear(64, 64)\n",
    "        # self.l3 = LinW(in_features=64, out_features=64, depth=1, layers=[self.l2])\n",
    "        self.l3 = LinW(in_features=16, out_features=16)\n",
    "        self.l4 = nn.Linear(16, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        # self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x)\n",
    "        x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        # x = self.gelu(self.l2(x, repr))\n",
    "        # x = self.l2(x, repr)\n",
    "        # x = self.l2(x)\n",
    "        # x = self.layer_norm(x)\n",
    "        # repr.append(x)\n",
    "        # x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        # self.depth = depth\n",
    "        # self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "        self.mha = MultiHeadAttention(in_features, 2)\n",
    "\n",
    "    def forward(self, input, activations=[]):\n",
    "        activations = get_layer_activations(activations)\n",
    "        return F.linear(self.mha(activations, activations, input), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 7\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "# print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "print(model)\n",
    "# number of parameters\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad)/1000000.0 , \"M\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, lr_scheduler, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "MLPWD(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (l1): Linear(in_features=3072, out_features=16, bias=True)\n",
      "  (layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "  (l3): LinW(\n",
      "    in_features=16, out_features=16, bias=True\n",
      "    (mha): MultiHeadAttention(\n",
      "      (W_O): Linear(in_features=32, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (l4): Linear(in_features=16, out_features=10, bias=True)\n",
      "  (gelu): GELU(approximate='none')\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n",
      "0.05017 M\n",
      "Epoch 1/7, Training Loss: 2.2586, Training Accuracy: 17.49%, Test accuracy: 24.63%\n",
      "Epoch 2/7, Training Loss: 2.1724, Training Accuracy: 28.13%, Test accuracy: 31.31%\n",
      "Epoch 3/7, Training Loss: 2.1328, Training Accuracy: 32.10%, Test accuracy: 34.01%\n",
      "Epoch 4/7, Training Loss: 2.1123, Training Accuracy: 34.08%, Test accuracy: 35.53%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 362\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39msum\u001b[39m(p\u001b[39m.\u001b[39mnumel() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mparameters() \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mrequires_grad)\u001b[39m/\u001b[39m\u001b[39m1000000.0\u001b[39m , \u001b[39m\"\u001b[39m\u001b[39mM\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    361\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m--> 362\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(model, train_loader, criterion, optimizer, lr_scheduler, device)\n\u001b[1;32m    363\u001b[0m     test_accuracy \u001b[39m=\u001b[39m evaluate(device, model, test_loader)\n\u001b[1;32m    364\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mEPOCHS\u001b[39m}\u001b[39;00m\u001b[39m, Training Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Training Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%, Test accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtest_accuracy\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 36\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, lr_scheduler, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m images, labels \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     35\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 36\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     37\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     38\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[6], line 316\u001b[0m, in \u001b[0;36mMLPWD.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    307\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgelu(x)\n\u001b[1;32m    308\u001b[0m \u001b[39m# repr.append(x.detach().cpu().numpy())\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[39m# x = self.gelu(self.l2(x, repr))\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[39m# x = self.l2(x, repr)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39m# x = self.gelu(x)\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[39m# repr.append(x.detach().cpu().numpy())\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml3(x, \u001b[39mrepr\u001b[39;49m))\n\u001b[1;32m    317\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml4(x)\n\u001b[1;32m    318\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoftmax(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[6], line 337\u001b[0m, in \u001b[0;36mLinW.forward\u001b[0;34m(self, input, activations)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, activations\u001b[39m=\u001b[39m[]):\n\u001b[1;32m    336\u001b[0m     activations \u001b[39m=\u001b[39m get_layer_activations(activations)\n\u001b[0;32m--> 337\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmha(activations, activations, \u001b[39minput\u001b[39;49m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[6], line 220\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, Q, K, V)\u001b[0m\n\u001b[1;32m    217\u001b[0m V \u001b[39m=\u001b[39m V\u001b[39m.\u001b[39mreshape(batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_head, activation_size, \u001b[39m1\u001b[39m)\n\u001b[1;32m    219\u001b[0m \u001b[39m# apply attention mechanism\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m out_attention \u001b[39m=\u001b[39m head_batched_attention_mechanism(Q, K, V)\u001b[39m.\u001b[39mreshape(batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_head\u001b[39m*\u001b[39mactivation_size)\n\u001b[1;32m    222\u001b[0m \u001b[39m# apply linear transformation\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_O(out_attention)\n",
      "Cell \u001b[0;32mIn[6], line 171\u001b[0m, in \u001b[0;36mhead_batched_attention_mechanism\u001b[0;34m(Q, K, V)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[39mreturn\u001b[39;00m (nn\u001b[39m.\u001b[39mSoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)(\n\u001b[1;32m    162\u001b[0m                 mutate_attention_map(torch\u001b[39m.\u001b[39mmatmul( \n\u001b[1;32m    163\u001b[0m                             Q\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m) , \n\u001b[1;32m    164\u001b[0m                             K\n\u001b[1;32m    165\u001b[0m                 )\u001b[39m/\u001b[39mtorch\u001b[39m.\u001b[39msqrt(torch\u001b[39m.\u001b[39mtensor(\u001b[39m8\u001b[39m)))    \n\u001b[1;32m    166\u001b[0m             ) \u001b[39m@\u001b[39m V)\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    168\u001b[0m \u001b[39m# p > 0.6 apply the crossover only\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[39mreturn\u001b[39;00m (nn\u001b[39m.\u001b[39mSoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)(\n\u001b[0;32m--> 171\u001b[0m             attention_map_crossover(torch\u001b[39m.\u001b[39;49mmatmul( \n\u001b[1;32m    172\u001b[0m                         Q\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m) , \n\u001b[1;32m    173\u001b[0m                         K\n\u001b[1;32m    174\u001b[0m             )\u001b[39m/\u001b[39;49mtorch\u001b[39m.\u001b[39;49msqrt(torch\u001b[39m.\u001b[39;49mtensor(\u001b[39m8\u001b[39;49m)))    \n\u001b[1;32m    175\u001b[0m         ) \u001b[39m@\u001b[39m V)\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 88\u001b[0m, in \u001b[0;36mattention_map_crossover\u001b[0;34m(attention_map)\u001b[0m\n\u001b[1;32m     85\u001b[0m crossover_index \u001b[39m=\u001b[39m attention_map\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m] \u001b[39m-\u001b[39m \u001b[39mint\u001b[39m(attention_map\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]\u001b[39m*\u001b[39mcrossover_magnitude)\n\u001b[1;32m     87\u001b[0m \u001b[39m# get two random indexes\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m random_index_1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrandint(\u001b[39m0\u001b[39;49m, attention_map\u001b[39m.\u001b[39;49mshape[\u001b[39m2\u001b[39;49m],(\u001b[39m1\u001b[39;49m,))[\u001b[39m0\u001b[39m]\n\u001b[1;32m     89\u001b[0m random_index_2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, attention_map\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m],(\u001b[39m1\u001b[39m,))[\u001b[39m0\u001b[39m]\n\u001b[1;32m     91\u001b[0m \u001b[39m# swap the values in that position over the columns\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def evaluate(device, model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, lr_scheduler, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # lr_scheduler.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / total\n",
    "    return train_loss, accuracy\n",
    "\n",
    "### GLOBAL VARIABLES\n",
    "CROSSOVER_MAGNITUDE = 0.3\n",
    "MUTATION_FACTOR = 0.3\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "\n",
    "def attention_map_crossover(attention_map):\n",
    "    \"\"\" Apply the crossover over the attention maps of each head.\n",
    "    The crosseover consists in picking a random index in the maxtrix over\n",
    "    the columns and swapping the values in between the columns of the\n",
    "    attention map.\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # get the crossover magnitude\n",
    "    crossover_magnitude = CROSSOVER_MAGNITUDE\n",
    "    \n",
    "    # get the batch size\n",
    "    dim_batch = attention_map.shape[0]\n",
    "    \n",
    "    # get the number of heads\n",
    "    number_of_heads = attention_map.shape[1]\n",
    "    \n",
    "    for idx_batch in range(dim_batch):\n",
    "        for idx_head in range(number_of_heads):\n",
    "            \n",
    "            # get the crossover index\n",
    "            crossover_index = attention_map.shape[2] - int(attention_map.shape[2]*crossover_magnitude)\n",
    "            \n",
    "            # get two random indexes\n",
    "            random_index_1 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            random_index_2 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            \n",
    "            # swap the values in that position over the columns\n",
    "            for idx, (x_1, x_2) in enumerate(zip(attention_map[idx_batch][idx_head][random_index_1][crossover_index:].detach(), attention_map[idx_batch][idx_head][random_index_2][crossover_index:].detach())):\n",
    "                \n",
    "                # debug\n",
    "                # print(attention_map[idx_batch][idx_head].shape, random_index_1, random_index_2, crossover_index)         \n",
    "                # print(x_1, x_2,idx_batch, idx_head, idx)\n",
    "                \n",
    "                # swap the values in that position over the columns\n",
    "                attention_map[idx_batch][idx_head][random_index_1][crossover_index+idx] = x_2 # make crossover\n",
    "                attention_map[idx_batch][idx_head][random_index_2][crossover_index+idx] = x_1 # make crossover\n",
    "    \n",
    "    return attention_map\n",
    "\n",
    "################################################################\n",
    "\n",
    "def mutate_attention_map(attention_map):\n",
    "    \"\"\" Mutate the attention map by making an elementwise multiplication with \n",
    "    a random tensor with values between 1-mutation_factor and 1+mutation_factor\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, num_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:    \n",
    "        torch.Tensor: shape (batch_size, num_heads, activation_size, activation_size)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # get the mutation factor\n",
    "    mutation_factor = MUTATION_FACTOR\n",
    "    # return the mutated attention map\n",
    "    # multiplied elementwise with a \n",
    "    # random matrix with values between \n",
    "    # 1-mutation_factor and 1+mutation_factor\n",
    "    return torch.mul(attention_map, torch.randn(attention_map.shape).uniform_(1-mutation_factor,1+mutation_factor).to(attention_map.device))\n",
    "    \n",
    "    \n",
    "################################################################\n",
    "\n",
    "\n",
    "def head_batched_attention_mechanism(Q, K, V):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Q: (batch_size, num_heads, num_layer, activation_size)\n",
    "        K: (batch_size, num_heads, num_layer, activation_size)\n",
    "        V: (batch_size, num_heads, activation_size, 1) # activations in the current layer\n",
    "\n",
    "    Returns:\n",
    "        attention: (batch_size, num_heads, activation_size)\n",
    "\n",
    "        # attention mechanism\n",
    "        # # (batch_size, num_heads, activation_size, activation_size)\n",
    "        # attention = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        # attention = attention / torch.sqrt(torch.tensor(activation_size).float())\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = nn.Softmax(dim=-1)(attention)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = torch.matmul(attention, V)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size)\n",
    "        # attention = attention.squeeze(-1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # with probability p\n",
    "    p = torch.rand(1)\n",
    "    \n",
    "    # p <= 0.6 apply the mutation only\n",
    "    if p <= 0.6:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                    mutate_attention_map(torch.matmul( \n",
    "                                Q , \n",
    "                                K.transpose(-1,-2)\n",
    "                    )/torch.sqrt(torch.tensor(8)))    \n",
    "                ) @ V).squeeze(-1)\n",
    "        \n",
    "    # p > 0.6 apply the crossover only\n",
    "    else:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                attention_map_crossover(torch.matmul( \n",
    "                            Q , \n",
    "                            K.transpose(-1,-2)\n",
    "                )/torch.sqrt(torch.tensor(8)))    \n",
    "            ) @ V).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_emb, n_head) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert dim_emb % n_head == 0, 'dim_emb must be divisible by n_head'\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.n_head = n_head\n",
    "\n",
    "        # self.W_Q = nn.Linear(dim_emb, dim_emb)\n",
    "        # self.W_K = nn.Linear(dim_emb, dim_emb)\n",
    "        # self.W_V = nn.Linear(dim_emb, dim_emb)\n",
    "\n",
    "        self.W_O = nn.Linear(dim_emb*n_head, dim_emb)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # get the shape of the input\n",
    "        batch_size, activation_size, activation_size = Q.size()\n",
    "        \n",
    "        # check the device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # move to device\n",
    "        Q.to(device)\n",
    "        K.to(device)\n",
    "        V.to(device)\n",
    "        \n",
    "        # reshape Q, K, V\n",
    "        # parallelize over the number of heads\n",
    "        # (batch_size, num_heads, num_layer, activation_size)\n",
    "        Q = torch.stack([Q for _ in range(self.n_head)], 1)\n",
    "        K = torch.stack([K for _ in range(self.n_head)], 1)\n",
    "        V = torch.stack([V for _ in range(self.n_head)], 1)\n",
    "\n",
    "        # apply linear transformation\n",
    "        # Q = self.W_Q(Q)\n",
    "        # K = self.W_K(K)\n",
    "        # V = self.W_V(V).reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        V = V.reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        \n",
    "        # apply attention mechanism\n",
    "        out_attention = head_batched_attention_mechanism(Q, K, V).reshape(batch_size, self.n_head*activation_size)\n",
    "\n",
    "        # apply linear transformation\n",
    "        return self.W_O(out_attention)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, 16)\n",
    "        self.layer_norm = nn.LayerNorm(16)\n",
    "        # self.l2 = LinW(in_features=64, out_features=64, depth=0)\n",
    "        # self.l2 = nn.Linear(64, 64)\n",
    "        # self.l3 = LinW(in_features=64, out_features=64, depth=1, layers=[self.l2])\n",
    "        self.l3 = LinW(in_features=16, out_features=16)\n",
    "        self.l4 = nn.Linear(16, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        # self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x)\n",
    "        x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        # x = self.gelu(self.l2(x, repr))\n",
    "        # x = self.l2(x, repr)\n",
    "        # x = self.l2(x)\n",
    "        # x = self.layer_norm(x)\n",
    "        # repr.append(x)\n",
    "        # x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        # self.depth = depth\n",
    "        # self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "        self.mha = MultiHeadAttention(in_features, 2)\n",
    "\n",
    "    def forward(self, input, activations=[]):\n",
    "        activations = get_layer_activations(activations)\n",
    "        return F.linear(self.mha(activations, activations, input), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 7\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "print(model)\n",
    "# number of parameters\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad)/1000000.0 , \"M\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, lr_scheduler, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "MLPWD(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (l1): Linear(in_features=3072, out_features=16, bias=True)\n",
      "  (l2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "  (l3): LinW(\n",
      "    in_features=16, out_features=16, bias=True\n",
      "    (mha): MultiHeadAttention(\n",
      "      (W_O): Linear(in_features=32, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (l4): Linear(in_features=16, out_features=10, bias=True)\n",
      "  (gelu): GELU(approximate='none')\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n",
      "0.050442 M\n",
      "Epoch 1/7, Training Loss: 2.2545, Training Accuracy: 17.30%, Test accuracy: 22.66%\n",
      "Epoch 2/7, Training Loss: 2.1897, Training Accuracy: 25.81%, Test accuracy: 29.93%\n",
      "Epoch 3/7, Training Loss: 2.1456, Training Accuracy: 30.97%, Test accuracy: 34.43%\n",
      "Epoch 4/7, Training Loss: 2.1169, Training Accuracy: 33.87%, Test accuracy: 34.76%\n",
      "Epoch 5/7, Training Loss: 2.1025, Training Accuracy: 35.31%, Test accuracy: 36.05%\n",
      "Epoch 6/7, Training Loss: 2.0847, Training Accuracy: 37.19%, Test accuracy: 36.52%\n",
      "Epoch 7/7, Training Loss: 2.0789, Training Accuracy: 37.75%, Test accuracy: 37.55%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def evaluate(device, model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, lr_scheduler, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # lr_scheduler.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / total\n",
    "    return train_loss, accuracy\n",
    "\n",
    "### GLOBAL VARIABLES\n",
    "CROSSOVER_MAGNITUDE = 0.3\n",
    "MUTATION_FACTOR = 0.3\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "\n",
    "def attention_map_crossover(attention_map):\n",
    "    \"\"\" Apply the crossover over the attention maps of each head.\n",
    "    The crosseover consists in picking a random index in the maxtrix over\n",
    "    the columns and swapping the values in between the columns of the\n",
    "    attention map.\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # get the crossover magnitude\n",
    "    crossover_magnitude = CROSSOVER_MAGNITUDE\n",
    "    \n",
    "    # get the batch size\n",
    "    dim_batch = attention_map.shape[0]\n",
    "    \n",
    "    # get the number of heads\n",
    "    number_of_heads = attention_map.shape[1]\n",
    "    \n",
    "    for idx_batch in range(dim_batch):\n",
    "        for idx_head in range(number_of_heads):\n",
    "            \n",
    "            # get the crossover index\n",
    "            crossover_index = attention_map.shape[2] - int(attention_map.shape[2]*crossover_magnitude)\n",
    "            \n",
    "            # get two random indexes\n",
    "            random_index_1 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            random_index_2 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            \n",
    "            # swap the values in that position over the columns\n",
    "            for idx, (x_1, x_2) in enumerate(zip(attention_map[idx_batch][idx_head][random_index_1][crossover_index:].detach(), attention_map[idx_batch][idx_head][random_index_2][crossover_index:].detach())):\n",
    "                \n",
    "                # debug\n",
    "                # print(attention_map[idx_batch][idx_head].shape, random_index_1, random_index_2, crossover_index)         \n",
    "                # print(x_1, x_2,idx_batch, idx_head, idx)\n",
    "                \n",
    "                # swap the values in that position over the columns\n",
    "                attention_map[idx_batch][idx_head][random_index_1][crossover_index+idx] = x_2 # make crossover\n",
    "                attention_map[idx_batch][idx_head][random_index_2][crossover_index+idx] = x_1 # make crossover\n",
    "    \n",
    "    return attention_map\n",
    "\n",
    "################################################################\n",
    "\n",
    "def mutate_attention_map(attention_map):\n",
    "    \"\"\" Mutate the attention map by making an elementwise multiplication with \n",
    "    a random tensor with values between 1-mutation_factor and 1+mutation_factor\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, num_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:    \n",
    "        torch.Tensor: shape (batch_size, num_heads, activation_size, activation_size)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # get the mutation factor\n",
    "    mutation_factor = MUTATION_FACTOR\n",
    "    # return the mutated attention map\n",
    "    # multiplied elementwise with a \n",
    "    # random matrix with values between \n",
    "    # 1-mutation_factor and 1+mutation_factor\n",
    "    return torch.mul(attention_map, torch.randn(attention_map.shape).uniform_(1-mutation_factor,1+mutation_factor).to(attention_map.device))\n",
    "    \n",
    "    \n",
    "################################################################\n",
    "\n",
    "\n",
    "def head_batched_attention_mechanism(Q, K, V):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Q: (batch_size, num_heads, num_layer, activation_size)\n",
    "        K: (batch_size, num_heads, num_layer, activation_size)\n",
    "        V: (batch_size, num_heads, activation_size, 1) # activations in the current layer\n",
    "\n",
    "    Returns:\n",
    "        attention: (batch_size, num_heads, activation_size)\n",
    "\n",
    "        # attention mechanism\n",
    "        # # (batch_size, num_heads, activation_size, activation_size)\n",
    "        # attention = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        # attention = attention / torch.sqrt(torch.tensor(activation_size).float())\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = nn.Softmax(dim=-1)(attention)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = torch.matmul(attention, V)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size)\n",
    "        # attention = attention.squeeze(-1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # with probability p\n",
    "    p = torch.rand(1)\n",
    "    \n",
    "    # p <= 0.6 apply the mutation only\n",
    "    if p <= 0.6:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                    mutate_attention_map(torch.matmul( \n",
    "                                Q.transpose(-1,-2) , \n",
    "                                K\n",
    "                    )/torch.sqrt(torch.tensor(16)))    \n",
    "                ) @ V).squeeze(-1)\n",
    "        \n",
    "    # p > 0.6 apply the crossover only\n",
    "    else:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                attention_map_crossover(torch.matmul( \n",
    "                            Q.transpose(-1,-2) , \n",
    "                            K\n",
    "                )/torch.sqrt(torch.tensor(16)))    \n",
    "            ) @ V).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_emb, n_head) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert dim_emb % n_head == 0, 'dim_emb must be divisible by n_head'\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.n_head = n_head\n",
    "\n",
    "        # self.W_Q = nn.Linear(dim_emb, dim_emb)\n",
    "        # self.W_K = nn.Linear(dim_emb, dim_emb)\n",
    "        # self.W_V = nn.Linear(dim_emb, dim_emb)\n",
    "\n",
    "        self.W_O = nn.Linear(dim_emb*n_head, dim_emb)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # get the shape of the input\n",
    "        batch_size, activation_size, activation_size = Q.size()\n",
    "        \n",
    "        # check the device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # move to device\n",
    "        Q.to(device)\n",
    "        K.to(device)\n",
    "        V.to(device)\n",
    "        \n",
    "        # reshape Q, K, V\n",
    "        # parallelize over the number of heads\n",
    "        # (batch_size, num_heads, num_layer, activation_size)\n",
    "        Q = torch.stack([Q for _ in range(self.n_head)], 1)\n",
    "        K = torch.stack([K for _ in range(self.n_head)], 1)\n",
    "        V = torch.stack([V for _ in range(self.n_head)], 1)\n",
    "\n",
    "        # apply linear transformation\n",
    "        # Q = self.W_Q(Q)\n",
    "        # K = self.W_K(K)\n",
    "        # V = self.W_V(V).reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        V = V.reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        \n",
    "        # apply attention mechanism\n",
    "        out_attention = head_batched_attention_mechanism(Q, K, V).reshape(batch_size, self.n_head*activation_size)\n",
    "\n",
    "        # apply linear transformation\n",
    "        return self.W_O(out_attention)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, 16)\n",
    "        self.l2 = nn.Linear(16, 16)\n",
    "        self.layer_norm = nn.LayerNorm(16)\n",
    "        # self.l2 = LinW(in_features=64, out_features=64, depth=0)\n",
    "        # self.l2 = nn.Linear(64, 64)\n",
    "        # self.l3 = LinW(in_features=64, out_features=64, depth=1, layers=[self.l2])\n",
    "        self.l3 = LinW(in_features=16, out_features=16)\n",
    "        self.l4 = nn.Linear(16, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        # self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x)\n",
    "        x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        # x = self.gelu(self.l2(x, repr))\n",
    "        # x = self.l2(x, repr)\n",
    "        # x = self.l2(x)\n",
    "        # x = self.layer_norm(x)\n",
    "        # repr.append(x)\n",
    "        # x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        # self.depth = depth\n",
    "        # self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "        self.mha = MultiHeadAttention(in_features, 2)\n",
    "\n",
    "    def forward(self, input, activations=[]):\n",
    "        activations = get_layer_activations(activations)\n",
    "        return F.linear(self.mha(activations, activations, input), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 7\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "print(model)\n",
    "# number of parameters\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad)/1000000.0 , \"M\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, lr_scheduler, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "MLPWD(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (l1): Linear(in_features=3072, out_features=8, bias=True)\n",
      "  (l2): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "  (l3): LinW(\n",
      "    in_features=8, out_features=8, bias=True\n",
      "    (mha): MultiHeadAttention(\n",
      "      (W_O): Linear(in_features=64, out_features=8, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (l4): Linear(in_features=8, out_features=10, bias=True)\n",
      "  (gelu): GELU(approximate='none')\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n",
      "0.025354 M\n",
      "Epoch 1/7, Training Loss: 2.2631, Training Accuracy: 16.92%, Test accuracy: 21.05%\n",
      "Epoch 2/7, Training Loss: 2.2133, Training Accuracy: 22.81%, Test accuracy: 25.70%\n",
      "Epoch 3/7, Training Loss: 2.1903, Training Accuracy: 25.87%, Test accuracy: 27.67%\n",
      "Epoch 4/7, Training Loss: 2.1747, Training Accuracy: 27.56%, Test accuracy: 28.45%\n",
      "Epoch 5/7, Training Loss: 2.1614, Training Accuracy: 29.15%, Test accuracy: 29.82%\n",
      "Epoch 6/7, Training Loss: 2.1470, Training Accuracy: 30.73%, Test accuracy: 31.59%\n",
      "Epoch 7/7, Training Loss: 2.1384, Training Accuracy: 31.71%, Test accuracy: 31.88%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def evaluate(device, model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, lr_scheduler, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # lr_scheduler.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / total\n",
    "    return train_loss, accuracy\n",
    "\n",
    "### GLOBAL VARIABLES\n",
    "CROSSOVER_MAGNITUDE = 0.3\n",
    "MUTATION_FACTOR = 0.3\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "\n",
    "def attention_map_crossover(attention_map):\n",
    "    \"\"\" Apply the crossover over the attention maps of each head.\n",
    "    The crosseover consists in picking a random index in the maxtrix over\n",
    "    the columns and swapping the values in between the columns of the\n",
    "    attention map.\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # get the crossover magnitude\n",
    "    crossover_magnitude = CROSSOVER_MAGNITUDE\n",
    "    \n",
    "    # get the batch size\n",
    "    dim_batch = attention_map.shape[0]\n",
    "    \n",
    "    # get the number of heads\n",
    "    number_of_heads = attention_map.shape[1]\n",
    "    \n",
    "    for idx_batch in range(dim_batch):\n",
    "        for idx_head in range(number_of_heads):\n",
    "            \n",
    "            # get the crossover index\n",
    "            crossover_index = attention_map.shape[2] - int(attention_map.shape[2]*crossover_magnitude)\n",
    "            \n",
    "            # get two random indexes\n",
    "            random_index_1 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            random_index_2 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            \n",
    "            # swap the values in that position over the columns\n",
    "            for idx, (x_1, x_2) in enumerate(zip(attention_map[idx_batch][idx_head][random_index_1][crossover_index:].detach(), attention_map[idx_batch][idx_head][random_index_2][crossover_index:].detach())):\n",
    "                \n",
    "                # debug\n",
    "                # print(attention_map[idx_batch][idx_head].shape, random_index_1, random_index_2, crossover_index)         \n",
    "                # print(x_1, x_2,idx_batch, idx_head, idx)\n",
    "                \n",
    "                # swap the values in that position over the columns\n",
    "                attention_map[idx_batch][idx_head][random_index_1][crossover_index+idx] = x_2 # make crossover\n",
    "                attention_map[idx_batch][idx_head][random_index_2][crossover_index+idx] = x_1 # make crossover\n",
    "    \n",
    "    return attention_map\n",
    "\n",
    "################################################################\n",
    "\n",
    "def mutate_attention_map(attention_map):\n",
    "    \"\"\" Mutate the attention map by making an elementwise multiplication with \n",
    "    a random tensor with values between 1-mutation_factor and 1+mutation_factor\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, num_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:    \n",
    "        torch.Tensor: shape (batch_size, num_heads, activation_size, activation_size)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # get the mutation factor\n",
    "    mutation_factor = MUTATION_FACTOR\n",
    "    # return the mutated attention map\n",
    "    # multiplied elementwise with a \n",
    "    # random matrix with values between \n",
    "    # 1-mutation_factor and 1+mutation_factor\n",
    "    return torch.mul(attention_map, torch.randn(attention_map.shape).uniform_(1-mutation_factor,1+mutation_factor).to(attention_map.device))\n",
    "    \n",
    "    \n",
    "################################################################\n",
    "\n",
    "\n",
    "def head_batched_attention_mechanism(Q, K, V):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Q: (batch_size, num_heads, num_layer, activation_size)\n",
    "        K: (batch_size, num_heads, num_layer, activation_size)\n",
    "        V: (batch_size, num_heads, activation_size, 1) # activations in the current layer\n",
    "\n",
    "    Returns:\n",
    "        attention: (batch_size, num_heads, activation_size)\n",
    "\n",
    "        # attention mechanism\n",
    "        # # (batch_size, num_heads, activation_size, activation_size)\n",
    "        # attention = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        # attention = attention / torch.sqrt(torch.tensor(activation_size).float())\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = nn.Softmax(dim=-1)(attention)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = torch.matmul(attention, V)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size)\n",
    "        # attention = attention.squeeze(-1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # with probability p\n",
    "    p = torch.rand(1)\n",
    "    \n",
    "    # p <= 0.6 apply the mutation only\n",
    "    if p <= 0.6:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                    mutate_attention_map(torch.matmul( \n",
    "                                Q.transpose(-1,-2) , \n",
    "                                K\n",
    "                    )/torch.sqrt(torch.tensor(16)))    \n",
    "                ) @ V).squeeze(-1)\n",
    "        \n",
    "    # p > 0.6 apply the crossover only\n",
    "    else:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                attention_map_crossover(torch.matmul( \n",
    "                            Q.transpose(-1,-2) , \n",
    "                            K\n",
    "                )/torch.sqrt(torch.tensor(16)))    \n",
    "            ) @ V).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_emb, n_head) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert dim_emb % n_head == 0, 'dim_emb must be divisible by n_head'\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.n_head = n_head\n",
    "\n",
    "        # self.W_Q = nn.Linear(dim_emb, dim_emb)\n",
    "        # self.W_K = nn.Linear(dim_emb, dim_emb)\n",
    "        # self.W_V = nn.Linear(dim_emb, dim_emb)\n",
    "\n",
    "        self.W_O = nn.Linear(dim_emb*n_head, dim_emb)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # get the shape of the input\n",
    "        batch_size, activation_size, activation_size = Q.size()\n",
    "        \n",
    "        # check the device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # move to device\n",
    "        Q.to(device)\n",
    "        K.to(device)\n",
    "        V.to(device)\n",
    "        \n",
    "        # reshape Q, K, V\n",
    "        # parallelize over the number of heads\n",
    "        # (batch_size, num_heads, num_layer, activation_size)\n",
    "        Q = torch.stack([Q for _ in range(self.n_head)], 1)\n",
    "        K = torch.stack([K for _ in range(self.n_head)], 1)\n",
    "        V = torch.stack([V for _ in range(self.n_head)], 1)\n",
    "\n",
    "        # apply linear transformation\n",
    "        # Q = self.W_Q(Q)\n",
    "        # K = self.W_K(K)\n",
    "        # V = self.W_V(V).reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        V = V.reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        \n",
    "        # apply attention mechanism\n",
    "        out_attention = head_batched_attention_mechanism(Q, K, V).reshape(batch_size, self.n_head*activation_size)\n",
    "\n",
    "        # apply linear transformation\n",
    "        return self.W_O(out_attention)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, 8)\n",
    "        self.l2 = nn.Linear(8, 8)\n",
    "        self.layer_norm = nn.LayerNorm(8)\n",
    "        # self.l2 = LinW(in_features=64, out_features=64, depth=0)\n",
    "        # self.l2 = nn.Linear(64, 64)\n",
    "        # self.l3 = LinW(in_features=64, out_features=64, depth=1, layers=[self.l2])\n",
    "        self.l3 = LinW(in_features=8, out_features=8)\n",
    "        self.l4 = nn.Linear(8, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        # self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x)\n",
    "        x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        # x = self.gelu(self.l2(x, repr))\n",
    "        # x = self.l2(x, repr)\n",
    "        # x = self.l2(x)\n",
    "        # x = self.layer_norm(x)\n",
    "        # repr.append(x)\n",
    "        # x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        # self.depth = depth\n",
    "        # self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "        self.mha = MultiHeadAttention(in_features, 8)\n",
    "\n",
    "    def forward(self, input, activations=[]):\n",
    "        activations = get_layer_activations(activations)\n",
    "        return F.linear(self.mha(activations, activations, input), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 7\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "print(model)\n",
    "# number of parameters\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad)/1000000.0 , \"M\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, lr_scheduler, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3163cfb8aa3549ad3f5400bc3427ee7a4002d2a0d6d7ead52f641c6a7636395"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
