{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig](../../img/kde_learning_single_representation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / total\n",
    "    return train_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate(device, model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment we use the representations of each sample extracted from different layer of the network. Each as point in an high-dimensional space. Each sample is figuratively represented as a point in an high-dimensional spa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_activations_layers(layers):\n",
    "    \"\"\" Extract for each layer the activations\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (layer_activation, batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array([np.array([np.array(h) for h in l]) for l in layers])\n",
    "\n",
    "def extract_activations_per_sample(layers, mask = False):\n",
    "    \"\"\" Extract for each sample the activations \n",
    "    for each layer and store them in a list.\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    if mask == True:\n",
    "        # mask the activations to remove zeros\n",
    "        mask = layers != 0\n",
    "        layers = [[np.array(h[m]) for h, m in zip(l,sm)] \n",
    "                for l, sm in zip(layers, mask)]\n",
    "        \n",
    "    return np.array([layers[:,i,:].flatten().reshape(-1, 1) for i in range(layers.shape[1])])\n",
    "\n",
    "\n",
    "def get_sampled_activations(activations, bandwidth = 0.2):\n",
    "    \"\"\" Sample the activations using KDE\n",
    "\n",
    "    Args:\n",
    "        activations (np.array): shape (batch_size, number_of_activations)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.from_numpy(np.array([KernelDensity(kernel=\"gaussian\", bandwidth=bandwidth).fit(a).sample([n_neurons]) for a in activations], dtype=\"float32\")).squeeze(2)\n",
    "\n",
    "def wd(layers: list()):\n",
    "    \"\"\" Compute the weight decay for each layer\n",
    "\n",
    "    Args:\n",
    "        layers (list): list of layers\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: weight decay\n",
    "\n",
    "    \"\"\"\n",
    "    return get_sampled_activations(\n",
    "                list(\n",
    "                    extract_activations_per_sample(\n",
    "                            extract_activations_layers(layers), \n",
    "                            mask=False\n",
    "                        )\n",
    "                ), \n",
    "                bandwidth=0.2\n",
    "            )\n",
    "\n",
    "n_neurons = 64\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(784, n_neurons)\n",
    "        self.l2 = LinW(in_features=n_neurons, out_features=n_neurons, depth=0)\n",
    "        self.l3 = LinW(in_features=n_neurons, out_features=n_neurons, depth=1, layers=[self.l2])\n",
    "        self.l4 = nn.Linear(n_neurons, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l2(x, repr))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, depth, layers=[]):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.depth = depth\n",
    "        self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "\n",
    "    def forward(self, input, prev=[]):\n",
    "        # use wd as a shift \n",
    "        return F.linear(input + wd(prev).to('cuda:0'), self.weight, self.bias)\n",
    "        # use wd as transformation\n",
    "        return F.linear(input * wd(prev).to('cuda:0'), self.weight, self.bias)\n",
    "        # use wd as the new activations\n",
    "        return F.linear(wd(prev).to('cuda:0'), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b75c26b910ba516c930507b4337d6cf30fdc3d06c5392aedae0a97adcea4b39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
