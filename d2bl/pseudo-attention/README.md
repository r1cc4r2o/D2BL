### Info

<p align="center">LinW pseudo-attention mechanism without crossing-over and mutations</p>

[pseudo-attention.ipynb](https://github.com/r1cc4r2o/D2BL/blob/main/d2bl/pseudo-attention/pseudo-attention.ipynb)


<p align="center">
 <img src="../../img/pseudo-attention.svg" width="95%" >
</p>


<p align="center">LinW pseudo-attention mechanism with crossing-over and mutations</p>

[1_pseudo-attention-mut-cross.ipynb](https://github.com/r1cc4r2o/D2BL/blob/main/d2bl/pseudo-attention/1_pseudo-attention-mut-cross.ipynb)


<p align="center">
 <img src="../../img/pseudo-attention-crossingovermutations.svg" width="95%" >
</p>

<p align="center">LinW pseudo-attention mechanism with crossing-over and mutations for fine-tuning RN50 and VGG19 </p>

[2_rn50_linW.ipynb](https://github.com/r1cc4r2o/D2BL/blob/main/d2bl/pseudo-attention/2_rn50_linW.ipynb)

[2_vgg19_linW.ipynb](https://github.com/r1cc4r2o/D2BL/blob/main/d2bl/pseudo-attention/2_vgg19_linW.ipynb)

<p align="center">
 <img src="../../img\pseudo-attention-crossingovermutations-finetuning.svg" width="95%" >
</p>
