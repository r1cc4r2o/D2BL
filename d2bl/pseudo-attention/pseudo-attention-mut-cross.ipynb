{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../../img/pseudo-attention-crossingovermutations.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "####################################################################\n",
    "### GLOBAL VARIABLES\n",
    "CROSSOVER_MAGNITUDE = 0.3\n",
    "MUTATION_FACTOR = 0.3\n",
    "\n",
    "####################################################################\n",
    "\n",
    "def evaluate(device, model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "####################################################################\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / total\n",
    "    return train_loss, accuracy\n",
    "\n",
    "################################################################\n",
    "\n",
    "def attention_map_crossover(attention_map):\n",
    "    \"\"\" Apply the crossover over the attention maps of each head.\n",
    "    The crosseover consists in picking a random index in the maxtrix over\n",
    "    the columns and swapping the values in between the columns of the\n",
    "    attention map.\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # get the crossover magnitude\n",
    "    crossover_magnitude = CROSSOVER_MAGNITUDE\n",
    "    \n",
    "    # get the batch size\n",
    "    dim_batch = attention_map.shape[0]\n",
    "    \n",
    "    # get the number of heads\n",
    "    number_of_heads = attention_map.shape[1]\n",
    "    \n",
    "    for idx_batch in range(dim_batch):\n",
    "        for idx_head in range(number_of_heads):\n",
    "            \n",
    "            # get the crossover index\n",
    "            crossover_index = attention_map.shape[2] - int(attention_map.shape[2]*crossover_magnitude)\n",
    "            \n",
    "            # get two random indexes\n",
    "            random_index_1 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            random_index_2 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            \n",
    "            # swap the values in that position over the columns\n",
    "            for idx, (x_1, x_2) in enumerate(zip(attention_map[idx_batch][idx_head][random_index_1][crossover_index:].detach(), attention_map[idx_batch][idx_head][random_index_2][crossover_index:].detach())):\n",
    "                \n",
    "                # debug\n",
    "                # print(attention_map[idx_batch][idx_head].shape, random_index_1, random_index_2, crossover_index)         \n",
    "                # print(x_1, x_2,idx_batch, idx_head, idx)\n",
    "                \n",
    "                # swap the values in that position over the columns\n",
    "                attention_map[idx_batch][idx_head][random_index_1][crossover_index+idx] = x_2 # make crossover\n",
    "                attention_map[idx_batch][idx_head][random_index_2][crossover_index+idx] = x_1 # make crossover\n",
    "    \n",
    "    return attention_map\n",
    "\n",
    "################################################################\n",
    "\n",
    "def mutate_attention_map(attention_map):\n",
    "    \"\"\" Mutate the attention map by making an elementwise multiplication with \n",
    "    a random tensor with values between 1-mutation_factor and 1+mutation_factor\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, num_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:    \n",
    "        torch.Tensor: shape (batch_size, num_heads, activation_size, activation_size)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # get the mutation factor\n",
    "    mutation_factor = MUTATION_FACTOR\n",
    "    # return the mutated attention map\n",
    "    # multiplied elementwise with a \n",
    "    # random matrix with values between \n",
    "    # 1-mutation_factor and 1+mutation_factor\n",
    "    return torch.mul(attention_map, torch.randn(attention_map.shape).uniform_(1-mutation_factor,1+mutation_factor).to(attention_map.device))\n",
    "    \n",
    "    \n",
    "################################################################\n",
    "\n",
    "\n",
    "def head_batched_attention_mechanism(Q, K, V):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Q: (batch_size, num_heads, num_layer, activation_size)\n",
    "        K: (batch_size, num_heads, num_layer, activation_size)\n",
    "        V: (batch_size, num_heads, activation_size, 1) # activations in the current layer\n",
    "\n",
    "    Returns:\n",
    "        attention: (batch_size, num_heads, activation_size)\n",
    "\n",
    "        # attention mechanism\n",
    "        # # (batch_size, num_heads, activation_size, activation_size)\n",
    "        # attention = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        # attention = attention / torch.sqrt(torch.tensor(activation_size).float())\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = nn.Softmax(dim=-1)(attention)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = torch.matmul(attention, V)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size)\n",
    "        # attention = attention.squeeze(-1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # with probability p\n",
    "    p = torch.rand(1)\n",
    "    \n",
    "    # p <= 0.6 apply the mutation only\n",
    "    if p <= 0.6:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                    mutate_attention_map(torch.matmul( \n",
    "                                Q , \n",
    "                                K.transpose(-1,-2)\n",
    "                    )/torch.sqrt(torch.tensor(8)))    \n",
    "                ) @ V).squeeze(-1)\n",
    "        \n",
    "    # p > 0.6 apply the crossover only\n",
    "    else:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                attention_map_crossover(torch.matmul( \n",
    "                            Q , \n",
    "                            K.transpose(-1,-2)\n",
    "                )/torch.sqrt(torch.tensor(8)))    \n",
    "            ) @ V).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "class LinW_Attention_Module_C_M(nn.Module):\n",
    "    def __init__(self, dim_emb, n_head) -> None:\n",
    "        super(LinW_Attention_Module_C_M, self).__init__()\n",
    "\n",
    "        assert dim_emb % n_head == 0, 'dim_emb must be divisible by n_head'\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.W_O = nn.Linear(dim_emb*n_head, dim_emb)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # get the shape of the input\n",
    "        batch_size, activation_size, activation_size = Q.size()\n",
    "        \n",
    "        # check the device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # move to device\n",
    "        Q.to(device)\n",
    "        K.to(device)\n",
    "        V.to(device)\n",
    "        \n",
    "        # reshape Q, K, V\n",
    "        # parallelize over the number of heads\n",
    "        # (batch_size, num_heads, num_layer, activation_size)\n",
    "        Q = torch.stack([Q for _ in range(self.n_head)], 1)\n",
    "        K = torch.stack([K for _ in range(self.n_head)], 1)\n",
    "        V = torch.stack([V for _ in range(self.n_head)], 1)\n",
    "\n",
    "        V = V.reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        \n",
    "        # apply attention mechanism\n",
    "        out_attention = head_batched_attention_mechanism(Q, K, V).reshape(batch_size, self.n_head*activation_size)\n",
    "\n",
    "        # apply linear transformation\n",
    "        return self.W_O(out_attention)\n",
    "    \n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, 8)\n",
    "        self.layer_norm = nn.LayerNorm(8)\n",
    "        self.l2 = LinW(in_features=8, out_features=8)\n",
    "        self.l3 = nn.Linear(8, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.gelu(self.l2(x, repr))\n",
    "        x = self.l3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.mha = LinW_Attention_Module_C_M(in_features, 2)\n",
    "\n",
    "    def forward(self, input, activations=[]):\n",
    "        activations = get_layer_activations(activations)\n",
    "        return F.linear(self.mha(activations, activations, input), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 7\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "# train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "# test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10)\n",
    "\n",
    "\n",
    "print(model)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    scheduler.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b75c26b910ba516c930507b4337d6cf30fdc3d06c5392aedae0a97adcea4b39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
