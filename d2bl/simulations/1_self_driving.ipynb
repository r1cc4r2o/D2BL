{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting panda3d==1.10.5\n",
      "  Downloading panda3d-1.10.5-cp37-cp37m-manylinux1_x86_64.whl (54.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: panda3d\n",
      "Successfully installed panda3d-1.10.5\n",
      "Collecting cython==0.29.21\n",
      "  Downloading Cython-0.29.21-cp37-cp37m-manylinux1_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: cython\n",
      "Successfully installed cython-0.29.21\n",
      "Collecting gym==0.17.3\n",
      "  Using cached gym-0.17.3.tar.gz (1.6 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting scipy (from gym==0.17.3)\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.1/38.1 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.10.4 (from gym==0.17.3)\n",
      "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyglet<=1.5.0,>=1.4.0 (from gym==0.17.3)\n",
      "  Using cached pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
      "Collecting cloudpickle<1.7.0,>=1.2.0 (from gym==0.17.3)\n",
      "  Using cached cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Collecting future (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3)\n",
      "  Using cached future-0.18.3.tar.gz (840 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: gym, future\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654622 sha256=6fc2b9f87afb029a2160c31f4ef720470e831465233eaad89a317fe1e61bf685\n",
      "  Stored in directory: /home/rickbook/.cache/pip/wheels/d1/81/4b/dd9c029691022cb957398d1f015e66b75e37637dda61abdf58\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492022 sha256=ed7a64da74b6a6630e831dad9e183bd2fe1b9504217cf2c58f56f1f39efc845c\n",
      "  Stored in directory: /home/rickbook/.cache/pip/wheels/fa/cd/1f/c6b7b50b564983bf3011e8fc75d06047ddc50c07f6e3660b00\n",
      "Successfully built gym future\n",
      "Installing collected packages: numpy, future, cloudpickle, scipy, pyglet, gym\n",
      "Successfully installed cloudpickle-1.6.0 future-0.18.3 gym-0.17.3 numpy-1.21.6 pyglet-1.5.0 scipy-1.7.3\n",
      "Collecting pgdrive==0.1.1\n",
      "  Using cached pgdrive-0.1.1-py3-none-any.whl (30.3 MB)\n",
      "Requirement already satisfied: gym in /home/rickbook/mambaforge/envs/pgdrive/lib/python3.7/site-packages (from pgdrive==0.1.1) (0.17.3)\n",
      "Collecting numpy<=1.19.3 (from pgdrive==0.1.1)\n",
      "  Downloading numpy-1.19.3-cp37-cp37m-manylinux2010_x86_64.whl (14.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting matplotlib (from pgdrive==0.1.1)\n",
      "  Downloading matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pandas (from pgdrive==0.1.1)\n",
      "  Downloading pandas-1.1.5-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pygame (from pgdrive==0.1.1)\n",
      "  Downloading pygame-2.4.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm0:01\u001b[0mm\n",
      "\u001b[?25hCollecting yapf==0.30.0 (from pgdrive==0.1.1)\n",
      "  Using cached yapf-0.30.0-py2.py3-none-any.whl (190 kB)\n",
      "Collecting seaborn (from pgdrive==0.1.1)\n",
      "  Using cached seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "Requirement already satisfied: panda3d==1.10.5 in /home/rickbook/mambaforge/envs/pgdrive/lib/python3.7/site-packages (from pgdrive==0.1.1) (1.10.5)\n",
      "Collecting panda3d-gltf (from pgdrive==0.1.1)\n",
      "  Downloading panda3d_gltf-0.14-py3-none-any.whl (25 kB)\n",
      "Collecting panda3d-simplepbr (from pgdrive==0.1.1)\n",
      "  Downloading panda3d_simplepbr-0.10-py3-none-any.whl (10 kB)\n",
      "Collecting pillow (from pgdrive==0.1.1)\n",
      "  Downloading Pillow-9.5.0-cp37-cp37m-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/rickbook/mambaforge/envs/pgdrive/lib/python3.7/site-packages (from gym->pgdrive==0.1.1) (1.7.3)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /home/rickbook/mambaforge/envs/pgdrive/lib/python3.7/site-packages (from gym->pgdrive==0.1.1) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /home/rickbook/mambaforge/envs/pgdrive/lib/python3.7/site-packages (from gym->pgdrive==0.1.1) (1.6.0)\n",
      "Collecting cycler>=0.10 (from matplotlib->pgdrive==0.1.1)\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->pgdrive==0.1.1)\n",
      "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1 (from matplotlib->pgdrive==0.1.1)\n",
      "  Downloading kiwisolver-1.4.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/rickbook/mambaforge/envs/pgdrive/lib/python3.7/site-packages (from matplotlib->pgdrive==0.1.1) (23.1)\n",
      "Collecting pyparsing>=2.2.1 (from matplotlib->pgdrive==0.1.1)\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/rickbook/mambaforge/envs/pgdrive/lib/python3.7/site-packages (from matplotlib->pgdrive==0.1.1) (2.8.2)\n",
      "INFO: pip is looking at multiple versions of panda3d-gltf to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting panda3d-gltf (from pgdrive==0.1.1)\n",
      "  Downloading panda3d_gltf-0.13-py3-none-any.whl (25 kB)\n",
      "  Downloading panda3d_gltf-0.12-py3-none-any.whl (24 kB)\n",
      "INFO: pip is looking at multiple versions of panda3d-simplepbr to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting panda3d-simplepbr (from pgdrive==0.1.1)\n",
      "  Downloading panda3d_simplepbr-0.9-py3-none-any.whl (12 kB)\n",
      "  Downloading panda3d_simplepbr-0.8-py3-none-any.whl (12 kB)\n",
      "  Downloading panda3d_simplepbr-0.7-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/rickbook/mambaforge/envs/pgdrive/lib/python3.7/site-packages (from pandas->pgdrive==0.1.1) (2023.3)\n",
      "Requirement already satisfied: typing_extensions in /home/rickbook/mambaforge/envs/pgdrive/lib/python3.7/site-packages (from seaborn->pgdrive==0.1.1) (4.6.2)\n",
      "Requirement already satisfied: future in /home/rickbook/mambaforge/envs/pgdrive/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym->pgdrive==0.1.1) (0.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/rickbook/mambaforge/envs/pgdrive/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->pgdrive==0.1.1) (1.16.0)\n",
      "Installing collected packages: yapf, pyparsing, pygame, pillow, panda3d-simplepbr, numpy, kiwisolver, fonttools, cycler, pandas, panda3d-gltf, matplotlib, seaborn, pgdrive\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.6\n",
      "    Uninstalling numpy-1.21.6:\n",
      "      Successfully uninstalled numpy-1.21.6\n",
      "Successfully installed cycler-0.11.0 fonttools-4.38.0 kiwisolver-1.4.4 matplotlib-3.5.3 numpy-1.19.3 panda3d-gltf-0.12 panda3d-simplepbr-0.7 pandas-1.1.5 pgdrive-0.1.1 pillow-9.5.0 pygame-2.4.0 pyparsing-3.0.9 seaborn-0.12.2 yapf-0.30.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install python==3.7\n",
    "!pip install panda3d==1.10.5\n",
    "!pip install cython==0.29.21\n",
    "!pip install gym==0.17.3\n",
    "!pip install pgdrive==0.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (887.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.14.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading torchaudio-0.13.1-cp37-cp37m-manylinux1_x86_64.whl (4.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/rickbook/mambaforge/envs/pgdrive/lib/python3.7/site-packages (from torch) (4.6.2)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch)\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch)\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Requirement already satisfied: setuptools in /home/rickbook/mambaforge/envs/pgdrive/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (67.7.2)\n",
      "Requirement already satisfied: wheel in /home/rickbook/mambaforge/envs/pgdrive/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\n",
      "Requirement already satisfied: numpy in /home/rickbook/mambaforge/envs/pgdrive/lib/python3.7/site-packages (from torchvision) (1.19.3)\n",
      "Requirement already satisfied: requests in /home/rickbook/mambaforge/envs/pgdrive/lib/python3.7/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/rickbook/mambaforge/envs/pgdrive/lib/python3.7/site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/rickbook/mambaforge/envs/pgdrive/lib/python3.7/site-packages (from requests->torchvision) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/rickbook/mambaforge/envs/pgdrive/lib/python3.7/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/rickbook/mambaforge/envs/pgdrive/lib/python3.7/site-packages (from requests->torchvision) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/rickbook/mambaforge/envs/pgdrive/lib/python3.7/site-packages (from requests->torchvision) (2023.5.7)\n",
      "Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n",
      "Successfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1 torchaudio-0.13.1 torchvision-0.14.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.0\n"
     ]
    }
   ],
   "source": [
    "from pgdrive import PGDriveEnv\n",
    "from pgdrive.examples import expert\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from pgdrive.envs import PGDriveEnv\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully registered the following environments: ['PGDrive-test-v0', 'PGDrive-validation-v0', 'PGDrive-v0', 'PGDrive-10envs-v0', 'PGDrive-1000envs-v0', 'PGDrive-training0-v0', 'PGDrive-training1-v0', 'PGDrive-training2-v0'].\n",
      "\n",
      "The action space: Box(-1.0, 1.0, (2,), float32)\n",
      "\n",
      "The observation space: Box(-0.0, 1.0, (275,), float32)\n",
      "\n",
      "Starting the environment ...\n",
      "\n",
      "DriveAgent(\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=275, out_features=16, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (5): GELU(approximate='none')\n",
      "    (6): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "    (7): Dropout(p=0.2, inplace=False)\n",
      "    (8): Linear(in_features=8, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":device(error): Error adding inotify watch on /dev/input: No such file or directory\n",
      ":device(error): Error opening directory /dev/input: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode reward:  1.76317587918967\n",
      "\n",
      "The returned loss: 309.1747741699219.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 24.04790840148926, 'steering': -0.4812576472759247, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  -5.0\n",
      "\n",
      "The returned loss: 306.09515380859375.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 3.531600022315979, 'steering': -0.5138373374938965, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  0.9092953627303899\n",
      "\n",
      "The returned loss: 309.2204284667969.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 24.277472877502444, 'steering': -0.16951484978199005, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  -5.0\n",
      "\n",
      "The returned loss: 306.1127624511719.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 3.531600022315979, 'steering': 0.3005501627922058, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  0.6040996624286388\n",
      "\n",
      "The returned loss: 307.8057861328125.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 23.11974563598633, 'steering': -0.3048853278160095, 'acceleration': 0.36729514598846436, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  -5.0\n",
      "\n",
      "The returned loss: 306.0205078125.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 3.531600022315979, 'steering': -0.1432204246520996, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "The returned loss: 2.676527500152588.\n",
      "\n",
      "The returned reward: 0.43453337081600846.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 24.58692169189453, 'steering': -0.22813285887241364, 'acceleration': 1.0, 'step_reward': 0.43453337081600846, 'crash': False, 'out_of_road': False, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  1.840464655815901\n",
      "\n",
      "The returned loss: 307.40460205078125.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 25.579867744445803, 'steering': -0.340875506401062, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  -5.0\n",
      "\n",
      "The returned loss: 306.0248718261719.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 3.531600022315979, 'steering': -0.044316962361335754, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  0.4638616040814405\n",
      "\n",
      "The returned loss: 309.0412902832031.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 23.556184387207033, 'steering': -0.173500657081604, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  -5.0\n",
      "\n",
      "The returned loss: 306.0544738769531.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 3.531600022315979, 'steering': -0.1943366825580597, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  1.0307212347396248\n",
      "\n",
      "The returned loss: 309.216064453125.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 24.23661231994629, 'steering': -0.3372870683670044, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  -5.0\n",
      "\n",
      "The returned loss: 306.06768798828125.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 3.531600022315979, 'steering': -0.369798868894577, 'acceleration': 0.8183507919311523, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  1.1213922411080128\n",
      "\n",
      "The returned loss: 309.2709045410156.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 23.964178848266602, 'steering': -0.33052754402160645, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  -5.0\n",
      "\n",
      "The returned loss: 306.0796203613281.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 3.531600022315979, 'steering': -0.46963322162628174, 'acceleration': 0.915497899055481, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  0.5849260398129612\n",
      "\n",
      "The returned loss: 307.0919494628906.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 25.51358585357666, 'steering': -0.3123818337917328, 'acceleration': 0.8906685709953308, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  -5.0\n",
      "\n",
      "The returned loss: 306.02008056640625.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 3.531600022315979, 'steering': -0.12038297951221466, 'acceleration': 0.9875125885009766, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  1.3697540273412976\n",
      "\n",
      "The returned loss: 309.0093994140625.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 24.642033576965332, 'steering': 0.00771254301071167, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  -5.0\n",
      "\n",
      "The returned loss: 306.2150573730469.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 3.531600022315979, 'steering': -0.22247770428657532, 'acceleration': 0.3797425627708435, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  -0.4158107831960596\n",
      "\n",
      "The returned loss: 307.02581787109375.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 22.144511604309084, 'steering': -0.3631933331489563, 'acceleration': 0.7310618758201599, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  -5.0\n",
      "\n",
      "The returned loss: 306.19091796875.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 3.531600022315979, 'steering': -0.6339163184165955, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  1.3804507956152872\n",
      "\n",
      "The returned loss: 307.94805908203125.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 25.034794807434082, 'steering': -0.03899534046649933, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  -5.0\n",
      "\n",
      "The returned loss: 306.048583984375.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 3.531600022315979, 'steering': 0.12118802964687347, 'acceleration': 0.9576879739761353, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  1.6655509977790874\n",
      "\n",
      "The returned loss: 308.1487731933594.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 22.981307601928712, 'steering': -0.5418258905410767, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  -5.0\n",
      "\n",
      "The returned loss: 306.0627136230469.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 3.531600022315979, 'steering': -0.16808587312698364, 'acceleration': 0.7090238332748413, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  0.8618797603289945\n",
      "\n",
      "The returned loss: 308.53515625.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 24.413772010803225, 'steering': -0.29136180877685547, 'acceleration': 0.8983824253082275, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  -5.0\n",
      "\n",
      "The returned loss: 306.0973815917969.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 3.531600022315979, 'steering': 0.07367859780788422, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  0.49106182503780893\n",
      "\n",
      "The returned loss: 307.9616394042969.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 22.09478645324707, 'steering': -0.5845737457275391, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  -5.0\n",
      "\n",
      "The returned loss: 306.1350402832031.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 3.531600022315979, 'steering': -0.34038156270980835, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  0.544935420416353\n",
      "\n",
      "The returned loss: 308.4143371582031.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 24.94354133605957, 'steering': 0.10269708931446075, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  -5.0\n",
      "\n",
      "The returned loss: 306.1239318847656.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 3.531600022315979, 'steering': -0.3646377921104431, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  0.35746624994805476\n",
      "\n",
      "The returned loss: 309.94610595703125.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 23.718797492980958, 'steering': -0.5872458815574646, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  -5.0\n",
      "\n",
      "The returned loss: 306.20050048828125.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 3.531600022315979, 'steering': -0.6242899298667908, 'acceleration': 0.6509387493133545, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  0.49423672316649814\n",
      "\n",
      "The returned loss: 306.61651611328125.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 23.595680236816406, 'steering': 0.09052412211894989, 'acceleration': 0.8489618301391602, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  -5.0\n",
      "\n",
      "The returned loss: 306.0328369140625.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 3.531600022315979, 'steering': 0.017989367246627808, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  0.14742407568465055\n",
      "\n",
      "The returned loss: 308.2638854980469.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 23.934793853759768, 'steering': -0.2120862603187561, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  -5.0\n",
      "\n",
      "The returned loss: 306.09332275390625.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 3.531600022315979, 'steering': -0.19400712847709656, 'acceleration': 0.6197395920753479, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  0.9786139140727581\n",
      "\n",
      "The returned loss: 307.654052734375.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 23.396362495422363, 'steering': 0.3430376648902893, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  -5.0\n",
      "\n",
      "The returned loss: 306.1322937011719.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 3.531600022315979, 'steering': -0.41062629222869873, 'acceleration': 0.6198691129684448, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  -0.7730819686890227\n",
      "\n",
      "The returned loss: 309.2684631347656.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 20.800804710388185, 'steering': 0.10831679403781891, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  -5.0\n",
      "\n",
      "The returned loss: 306.18914794921875.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 3.531600022315979, 'steering': -0.11924473941326141, 'acceleration': 0.418254554271698, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  0.7757984712196704\n",
      "\n",
      "The returned loss: 308.7332763671875.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 24.49368381500244, 'steering': 0.2365497201681137, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  -5.0\n",
      "\n",
      "The returned loss: 306.0751037597656.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 3.531600022315979, 'steering': -0.12254194915294647, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  -0.23625252483687653\n",
      "\n",
      "The returned loss: 308.6589050292969.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 20.41765823364258, 'steering': -0.3684844970703125, 'acceleration': 0.9326272010803223, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  -5.0\n",
      "\n",
      "The returned loss: 306.0553283691406.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 3.531600022315979, 'steering': -0.3558162450790405, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  0.9411029979524859\n",
      "\n",
      "The returned loss: 307.39556884765625.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 24.105239868164062, 'steering': -0.5223238468170166, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  -5.0\n",
      "\n",
      "The returned loss: 306.0586853027344.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 3.531600022315979, 'steering': -0.3914942741394043, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  -0.09634219405135713\n",
      "\n",
      "The returned loss: 307.90093994140625.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 23.09835147857666, 'steering': -0.2982921600341797, 'acceleration': 0.9167724847793579, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  -5.0\n",
      "\n",
      "The returned loss: 306.17681884765625.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 3.531600022315979, 'steering': -0.6211411952972412, 'acceleration': 0.7033354043960571, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n",
      "\n",
      "Episode reward:  1.0041313024688483\n",
      "\n",
      "The returned loss: 306.794921875.\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 24.498801040649415, 'steering': 0.03922185301780701, 'acceleration': 1.0, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n"
     ]
    }
   ],
   "source": [
    "class DriveAgent(nn.Module):\n",
    "    def __init__(self, obs_space, action_space):\n",
    "        super(DriveAgent, self).__init__()\n",
    "        # self.obs_space = obs_space\n",
    "        # self.action_space = action_space\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(275, 16),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(16),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(8),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(8, 2)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = self.mlp(obs)\n",
    "        return x\n",
    "\n",
    "\n",
    "# dont render the env\n",
    "env = gym.make(\"PGDrive-v0\", config=dict(use_render=False))  \n",
    "# render the env\n",
    "# env = gym.make(\"PGDrive-v0\", config=dict(use_render=True))  \n",
    "\n",
    "print(\"\\nThe action space: {}\".format(env.action_space))\n",
    "print(\"\\nThe observation space: {}\\n\".format(env.observation_space))\n",
    "print(\"Starting the environment ...\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def train_agent(num_episodes = 50, lr = 5e-2, gamma = 0.1):\n",
    "\n",
    "    agent = DriveAgent(env.observation_space, env.action_space)\n",
    "    optimizer = optim.AdamW(agent.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=gamma)\n",
    "\n",
    "    print(agent)\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        ep_reward = 0.0\n",
    "        obs = env.reset()\n",
    "\n",
    "        for i in range(num_episodes):\n",
    "            \n",
    "            logits = agent(torch.from_numpy(obs).float())\n",
    "\n",
    "            obs , reward, done, info = env.step(logits.detach().numpy())\n",
    "\n",
    "            target = torch.from_numpy(expert(obs)).float()\n",
    "\n",
    "            ep_reward += reward\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # loss 4\n",
    "            loss = torch.nn.functional.mse_loss(logits, target) / (1/(math.pi**(reward))) # * int(info['out_of_road'])  # 'arrive_dest': True\n",
    "            # print(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # render the environment\n",
    "            # env.render()\n",
    "            if int(info['out_of_road']) == 1:\n",
    "                env.reset()\n",
    "                print(\"\\nEpisode reward: \", ep_reward)\n",
    "                break\n",
    "            if int(info['arrive_dest']):\n",
    "                env.reset()\n",
    "                print(\"\\nEpisode reward: \", ep_reward)\n",
    "                break\n",
    "\n",
    "        print(\"\\nThe returned loss: {}.\".format(loss))\n",
    "        # obs.close()\n",
    "\n",
    "        print(\"\\nThe returned reward: {}.\".format(reward))\n",
    "        print(\"\\nThe returned information: {}\".format(info))\n",
    "\n",
    "        print(\"\\nPGDrive successfully run!\")\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "\n",
    "agent = train_agent(num_episodes=50, lr=5e-2, gamma=0.1)\n",
    "\n",
    "\n",
    "# save the agent\n",
    "\n",
    "torch.save(agent.state_dict(), \"drive_agent_layer_16-8_50_loss1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The action space: Box(-1.0, 1.0, (2,), float32)\n",
      "\n",
      "The observation space: Box(-0.0, 1.0, (275,), float32)\n",
      "\n",
      "Starting the environment ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Known pipe types:\n",
      "  glxGraphicsPipe\n",
      "(all display modules loaded.)\n",
      "Xlib:  extension \"XFree86-DGA\" missing on display \":0\".\n",
      ":task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode reward:  129.50468629473062\n",
      "\n",
      "The returned reward: -5.\n",
      "\n",
      "The returned information: {'cost': 0.0, 'velocity': 79.41844253540039, 'steering': 0.01143912598490715, 'acceleration': 0.5232788324356079, 'step_reward': 0.0, 'crash': False, 'out_of_road': True, 'arrive_dest': False}\n",
      "\n",
      "PGDrive successfully run!\n"
     ]
    }
   ],
   "source": [
    "from pgdrive import PGDriveEnv\n",
    "from pgdrive.examples import expert\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gym\n",
    "\n",
    "\n",
    "class DriveAgent(nn.Module):\n",
    "    def __init__(self, obs_space, action_space):\n",
    "        super(DriveAgent, self).__init__()\n",
    "        # self.obs_space = obs_space\n",
    "        # self.action_space = action_space\n",
    "        self.fc1 = nn.Linear(275, 8)\n",
    "        self.fc2 = nn.Linear(8, 2)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropuot = nn.Dropout(0.2)\n",
    "        self.layer_norm = nn.LayerNorm(8)\n",
    "\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = self.gelu(self.fc1(obs))\n",
    "        x = self.layer_norm(self.dropuot(x) + x) # Add & Norm\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "env = gym.make(\"PGDrive-10envs-v0\", config=dict(use_render=True))  \n",
    "\n",
    "# load the agent\n",
    "agent = DriveAgent(env.observation_space, env.action_space)\n",
    "agent.load_state_dict(torch.load(\"drive_agent_8_100_reverse_reward_double.pth\"))\n",
    "\n",
    "print(\"\\nThe action space: {}\".format(env.action_space))\n",
    "print(\"\\nThe observation space: {}\\n\".format(env.observation_space))\n",
    "print(\"Starting the environment ...\\n\")\n",
    "\n",
    "ep_reward = 0.0\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    logits = agent(torch.from_numpy(obs).float())\n",
    "\n",
    "    obs, reward, done, info = env.step(logits.detach().numpy())\n",
    "    ep_reward += reward\n",
    "\n",
    "    env.render()\n",
    "    \n",
    "    if info['out_of_road']:\n",
    "        print(\"\\nEpisode reward: \", ep_reward)\n",
    "        break\n",
    "\n",
    "print(\"\\nThe returned reward: {}.\".format(reward))\n",
    "print(\"\\nThe returned information: {}\".format(info))\n",
    "\n",
    "env.close()\n",
    "print(\"\\nPGDrive successfully run!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the agent\n",
    "from pgdrive import PGDriveEnv\n",
    "from pgdrive.examples import expert\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from pgdrive.envs import PGDriveEnv\n",
    "\n",
    "import math\n",
    "\n",
    "####################################################################\n",
    "### GLOBAL VARIABLES\n",
    "CROSSOVER_MAGNITUDE = 0.1\n",
    "MUTATION_FACTOR = 0.2\n",
    "####################################################################\n",
    "\n",
    "def attention_map_crossover(attention_map):\n",
    "    \"\"\" Apply the crossover over the attention maps of each head.\n",
    "    The crosseover consists in picking a random index in the maxtrix over\n",
    "    the columns and swapping the values in between the columns of the\n",
    "    attention map.\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # get the crossover magnitude\n",
    "    crossover_magnitude = CROSSOVER_MAGNITUDE\n",
    "    \n",
    "    # get the batch size\n",
    "    dim_batch = attention_map.shape[0]\n",
    "    \n",
    "    # get the number of heads\n",
    "    number_of_heads = attention_map.shape[1]\n",
    "    \n",
    "    for idx_batch in range(dim_batch):\n",
    "        for idx_head in range(number_of_heads):\n",
    "            \n",
    "            # get the crossover index\n",
    "            crossover_index = attention_map.shape[2] - int(attention_map.shape[2]*crossover_magnitude)\n",
    "            \n",
    "            # get two random indexes\n",
    "            random_index_1 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            random_index_2 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            \n",
    "            # swap the values in that position over the columns\n",
    "            for idx, (x_1, x_2) in enumerate(zip(attention_map[idx_batch][idx_head][random_index_1][crossover_index:].detach(), attention_map[idx_batch][idx_head][random_index_2][crossover_index:].detach())):\n",
    "                \n",
    "                # debug\n",
    "                # print(attention_map[idx_batch][idx_head].shape, random_index_1, random_index_2, crossover_index)         \n",
    "                # print(x_1, x_2,idx_batch, idx_head, idx)\n",
    "                \n",
    "                # swap the values in that position over the columns\n",
    "                attention_map[idx_batch][idx_head][random_index_1][crossover_index+idx] = x_2 # make crossover\n",
    "                attention_map[idx_batch][idx_head][random_index_2][crossover_index+idx] = x_1 # make crossover\n",
    "    \n",
    "    return attention_map\n",
    "\n",
    "################################################################\n",
    "\n",
    "def mutate_attention_map(attention_map):\n",
    "    \"\"\" Mutate the attention map by making an elementwise multiplication with \n",
    "    a random tensor with values between 1-mutation_factor and 1+mutation_factor\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, num_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:    \n",
    "        torch.Tensor: shape (batch_size, num_heads, activation_size, activation_size)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # get the mutation factor\n",
    "    mutation_factor = MUTATION_FACTOR\n",
    "    # return the mutated attention map\n",
    "    # multiplied elementwise with a \n",
    "    # random matrix with values between \n",
    "    # 1-mutation_factor and 1+mutation_factor\n",
    "    return torch.mul(attention_map, torch.randn(attention_map.shape).uniform_(1-mutation_factor,1+mutation_factor).to(attention_map.device))\n",
    "    \n",
    "    \n",
    "################################################################\n",
    "\n",
    "\n",
    "def head_batched_attention_mechanism(Q, K, V):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Q: (batch_size, num_heads, num_layer, activation_size)\n",
    "        K: (batch_size, num_heads, num_layer, activation_size)\n",
    "        V: (batch_size, num_heads, activation_size, 1) # activations in the current layer\n",
    "\n",
    "    Returns:\n",
    "        attention: (batch_size, num_heads, activation_size)\n",
    "\n",
    "        # attention mechanism\n",
    "        # # (batch_size, num_heads, activation_size, activation_size)\n",
    "        # attention = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        # attention = attention / torch.sqrt(torch.tensor(activation_size).float())\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = nn.Softmax(dim=-1)(attention)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = torch.matmul(attention, V)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size)\n",
    "        # attention = attention.squeeze(-1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # with probability p\n",
    "    p = torch.rand(1)\n",
    "    \n",
    "    # p <= 0.6 apply the mutation only\n",
    "    if p <= 0.6:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                    mutate_attention_map(torch.matmul( \n",
    "                                Q , \n",
    "                                K.transpose(-1,-2)\n",
    "                    )/torch.sqrt(torch.tensor(8)))    \n",
    "                ) @ V).squeeze(-1)\n",
    "        \n",
    "    # p > 0.6 apply the crossover only\n",
    "    else:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                attention_map_crossover(torch.matmul( \n",
    "                            Q , \n",
    "                            K.transpose(-1,-2)\n",
    "                )/torch.sqrt(torch.tensor(8)))    \n",
    "            ) @ V).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "class LinW_Attention_Module_C_M(nn.Module):\n",
    "    def __init__(self, dim_emb, n_head, out_dim = None) -> None:\n",
    "        super(LinW_Attention_Module_C_M, self).__init__()\n",
    "\n",
    "        assert dim_emb % n_head == 0, 'dim_emb must be divisible by n_head'\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.n_head = n_head\n",
    "\n",
    "        if out_dim is None:\n",
    "            self.W_O = nn.Linear(dim_emb*n_head, dim_emb)\n",
    "        else:\n",
    "            self.W_O = nn.Linear(dim_emb*n_head, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # get the input\n",
    "        Q, K, V = x.unsqueeze(0), x.unsqueeze(0), x.unsqueeze(0)\n",
    "        # get the shape of the input\n",
    "        batch_size, activation_size = Q.size()\n",
    "        \n",
    "        # # check the device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # move to device\n",
    "        Q.to(device)\n",
    "        K.to(device)\n",
    "        V.to(device)\n",
    "        \n",
    "        # reshape Q, K, V\n",
    "        # parallelize over the number of heads\n",
    "        # (batch_size, num_heads, num_layer, activation_size)\n",
    "        Q = torch.stack([Q for _ in range(self.n_head)], 1)\n",
    "        K = torch.stack([K for _ in range(self.n_head)], 1)\n",
    "        V = torch.stack([V for _ in range(self.n_head)], 1)\n",
    "\n",
    "        Q = Q.reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        K = K.reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        V = V.reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        \n",
    "        # apply attention mechanism\n",
    "        out_attention = head_batched_attention_mechanism(Q, K, V).reshape(batch_size, self.n_head*self.dim_emb)\n",
    "\n",
    "        # apply linear transformation\n",
    "        return self.W_O(out_attention).squeeze(0)\n",
    "    \n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "\n",
    "class DriveAgent(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(DriveAgent, self).__init__()\n",
    "        self.fc1 = nn.Linear(275, 8)\n",
    "        self.linW = LinW_Attention_Module_C_M(8, 2)\n",
    "        self.fc2 = nn.Linear(8, 2)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropuot = nn.Dropout(0.2)\n",
    "        self.layer_norm = nn.LayerNorm(8)\n",
    "\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = self.gelu(self.fc1(obs))\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.linW(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# dont render the env\n",
    "env = gym.make(\"PGDrive-v0\", config=dict(use_render=False))  \n",
    "\n",
    "# render the env\n",
    "# env = gym.make(\"PGDrive-v0\", config=dict(use_render=True))  \n",
    "\n",
    "print(\"\\nThe action space: {}\".format(env.action_space))\n",
    "print(\"\\nThe observation space: {}\\n\".format(env.observation_space))\n",
    "print(\"Starting the environment ...\\n\")\n",
    "\n",
    "\n",
    "def train_agent(num_episodes = 50, lr = 5e-2, gamma = 0.1):\n",
    "    # create the agent\n",
    "    agent = DriveAgent()\n",
    "    # create the optimizer\n",
    "    optimizer = optim.AdamW(agent.parameters(), lr=lr)\n",
    "    # create the scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2, eta_min=0.0001, last_epoch=-1)\n",
    "\n",
    "    print(agent)\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        ep_reward = 0.0\n",
    "        obs = env.reset()\n",
    "\n",
    "        for i in range(num_episodes):\n",
    "            \n",
    "            logits = agent(torch.from_numpy(obs).float())\n",
    "\n",
    "            obs , reward, done, info = env.step(logits.detach().numpy())\n",
    "\n",
    "            target = torch.from_numpy(expert(obs)).float()\n",
    "\n",
    "            ep_reward += reward\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # loss = torch.nn.functional.mse_loss(logits, target) / (1/(math.pi**(ep_reward))) \n",
    "            if reward <= 0:\n",
    "                loss = torch.nn.functional.mse_loss(logits, target) * abs(ep_reward)\n",
    "            else:\n",
    "                loss = 50/abs(reward) + torch.nn.functional.mse_loss(logits, target) * abs(ep_reward)\n",
    "            # print(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # render the environment\n",
    "            # env.render()\n",
    "            if i > 100 and int(info['out_of_road']) == 1:\n",
    "                env.reset()\n",
    "                print(\"\\nEpisode reward: \", ep_reward)\n",
    "                break\n",
    "            if int(info['arrive_dest']):\n",
    "                env.reset()\n",
    "                print(\"\\nEpisode reward: \", ep_reward)\n",
    "                break\n",
    "\n",
    "        print(\"\\nThe returned loss: {}.\".format(loss))\n",
    "        # obs.close()\n",
    "\n",
    "        print(\"\\nThe returned reward: {}.\".format(reward))\n",
    "        print(\"\\nThe returned information: {}\".format(info))\n",
    "\n",
    "        print(\"\\nPGDrive successfully run!\")\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "agent = train_agent(num_episodes=100, lr=5e-2, gamma=0.1)\n",
    "\n",
    "\n",
    "# save the agent\n",
    "torch.save(agent.state_dict(), \"pseudo_attention_agent.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render the animation of the experiment\n",
    "from pgdrive import PGDriveEnv\n",
    "from pgdrive.examples import expert\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from pgdrive.envs import PGDriveEnv\n",
    "\n",
    "import math\n",
    "\n",
    "####################################################################\n",
    "### GLOBAL VARIABLES\n",
    "CROSSOVER_MAGNITUDE = 0.1\n",
    "MUTATION_FACTOR = 0.2\n",
    "####################################################################\n",
    "\n",
    "def attention_map_crossover(attention_map):\n",
    "    \"\"\" Apply the crossover over the attention maps of each head.\n",
    "    The crosseover consists in picking a random index in the maxtrix over\n",
    "    the columns and swapping the values in between the columns of the\n",
    "    attention map.\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # get the crossover magnitude\n",
    "    crossover_magnitude = CROSSOVER_MAGNITUDE\n",
    "    \n",
    "    # get the batch size\n",
    "    dim_batch = attention_map.shape[0]\n",
    "    \n",
    "    # get the number of heads\n",
    "    number_of_heads = attention_map.shape[1]\n",
    "    \n",
    "    for idx_batch in range(dim_batch):\n",
    "        for idx_head in range(number_of_heads):\n",
    "            \n",
    "            # get the crossover index\n",
    "            crossover_index = attention_map.shape[2] - int(attention_map.shape[2]*crossover_magnitude)\n",
    "            \n",
    "            # get two random indexes\n",
    "            random_index_1 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            random_index_2 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            \n",
    "            # swap the values in that position over the columns\n",
    "            for idx, (x_1, x_2) in enumerate(zip(attention_map[idx_batch][idx_head][random_index_1][crossover_index:].detach(), attention_map[idx_batch][idx_head][random_index_2][crossover_index:].detach())):\n",
    "                \n",
    "                # debug\n",
    "                # print(attention_map[idx_batch][idx_head].shape, random_index_1, random_index_2, crossover_index)         \n",
    "                # print(x_1, x_2,idx_batch, idx_head, idx)\n",
    "                \n",
    "                # swap the values in that position over the columns\n",
    "                attention_map[idx_batch][idx_head][random_index_1][crossover_index+idx] = x_2 # make crossover\n",
    "                attention_map[idx_batch][idx_head][random_index_2][crossover_index+idx] = x_1 # make crossover\n",
    "    \n",
    "    return attention_map\n",
    "\n",
    "################################################################\n",
    "\n",
    "def mutate_attention_map(attention_map):\n",
    "    \"\"\" Mutate the attention map by making an elementwise multiplication with \n",
    "    a random tensor with values between 1-mutation_factor and 1+mutation_factor\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, num_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:    \n",
    "        torch.Tensor: shape (batch_size, num_heads, activation_size, activation_size)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # get the mutation factor\n",
    "    mutation_factor = MUTATION_FACTOR\n",
    "    # return the mutated attention map\n",
    "    # multiplied elementwise with a \n",
    "    # random matrix with values between \n",
    "    # 1-mutation_factor and 1+mutation_factor\n",
    "    return torch.mul(attention_map, torch.randn(attention_map.shape).uniform_(1-mutation_factor,1+mutation_factor).to(attention_map.device))\n",
    "    \n",
    "    \n",
    "################################################################\n",
    "\n",
    "\n",
    "def head_batched_attention_mechanism(Q, K, V):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Q: (batch_size, num_heads, num_layer, activation_size)\n",
    "        K: (batch_size, num_heads, num_layer, activation_size)\n",
    "        V: (batch_size, num_heads, activation_size, 1) # activations in the current layer\n",
    "\n",
    "    Returns:\n",
    "        attention: (batch_size, num_heads, activation_size)\n",
    "\n",
    "        # attention mechanism\n",
    "        # # (batch_size, num_heads, activation_size, activation_size)\n",
    "        # attention = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        # attention = attention / torch.sqrt(torch.tensor(activation_size).float())\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = nn.Softmax(dim=-1)(attention)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = torch.matmul(attention, V)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size)\n",
    "        # attention = attention.squeeze(-1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # with probability p\n",
    "    p = torch.rand(1)\n",
    "    \n",
    "    # p <= 0.6 apply the mutation only\n",
    "    if p <= 0.6:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                    mutate_attention_map(torch.matmul( \n",
    "                                Q , \n",
    "                                K.transpose(-1,-2)\n",
    "                    )/torch.sqrt(torch.tensor(8)))    \n",
    "                ) @ V).squeeze(-1)\n",
    "        \n",
    "    # p > 0.6 apply the crossover only\n",
    "    else:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                attention_map_crossover(torch.matmul( \n",
    "                            Q , \n",
    "                            K.transpose(-1,-2)\n",
    "                )/torch.sqrt(torch.tensor(8)))    \n",
    "            ) @ V).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "class LinW_Attention_Module_C_M(nn.Module):\n",
    "    def __init__(self, dim_emb, n_head, out_dim = None) -> None:\n",
    "        super(LinW_Attention_Module_C_M, self).__init__()\n",
    "\n",
    "        assert dim_emb % n_head == 0, 'dim_emb must be divisible by n_head'\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.n_head = n_head\n",
    "\n",
    "        if out_dim is None:\n",
    "            self.W_O = nn.Linear(dim_emb*n_head, dim_emb)\n",
    "        else:\n",
    "            self.W_O = nn.Linear(dim_emb*n_head, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # get the input\n",
    "        Q, K, V = x.unsqueeze(0), x.unsqueeze(0), x.unsqueeze(0)\n",
    "        # get the shape of the input\n",
    "        batch_size, activation_size = Q.size()\n",
    "        \n",
    "        # # check the device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # move to device\n",
    "        Q.to(device)\n",
    "        K.to(device)\n",
    "        V.to(device)\n",
    "        \n",
    "        # reshape Q, K, V\n",
    "        # parallelize over the number of heads\n",
    "        # (batch_size, num_heads, num_layer, activation_size)\n",
    "        Q = torch.stack([Q for _ in range(self.n_head)], 1)\n",
    "        K = torch.stack([K for _ in range(self.n_head)], 1)\n",
    "        V = torch.stack([V for _ in range(self.n_head)], 1)\n",
    "\n",
    "        Q = Q.reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        K = K.reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        V = V.reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        \n",
    "        # apply attention mechanism\n",
    "        out_attention = head_batched_attention_mechanism(Q, K, V).reshape(batch_size, self.n_head*self.dim_emb)\n",
    "\n",
    "        # apply linear transformation\n",
    "        return self.W_O(out_attention).squeeze(0)\n",
    "    \n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "\n",
    "class DriveAgent(nn.Module):\n",
    "    def __init__(self, obs_space, action_space):\n",
    "        super(DriveAgent, self).__init__()\n",
    "        # self.obs_space = obs_space\n",
    "        # self.action_space = action_space\n",
    "        self.fc1 = nn.Linear(275, 8)\n",
    "        self.linW = LinW_Attention_Module_C_M(8, 2)\n",
    "        self.fc2 = nn.Linear(8, 2)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropuot = nn.Dropout(0.2)\n",
    "        self.layer_norm = nn.LayerNorm(8)\n",
    "\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = self.gelu(self.fc1(obs))\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.linW(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    \n",
    "env = gym.make(\"PGDrive-10envs-v0\", config=dict(use_render=True))  \n",
    "\n",
    "# load the agent\n",
    "agent = DriveAgent(env.observation_space, env.action_space)\n",
    "agent.load_state_dict(torch.load(\"pseudo_attention_agent.pth\"))\n",
    "\n",
    "print(\"\\nThe action space: {}\".format(env.action_space))\n",
    "print(\"\\nThe observation space: {}\\n\".format(env.observation_space))\n",
    "print(\"Starting the environment ...\\n\")\n",
    "\n",
    "ep_reward = 0.0\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    logits = agent(torch.from_numpy(obs).float())\n",
    "\n",
    "    obs, reward, done, info = env.step(logits.detach().numpy())\n",
    "    ep_reward += reward\n",
    "\n",
    "    env.render()\n",
    "    \n",
    "    if info['out_of_road']:\n",
    "        print(\"\\nEpisode reward: \", ep_reward)\n",
    "        break\n",
    "\n",
    "print(\"\\nThe returned reward: {}.\".format(reward))\n",
    "print(\"\\nThe returned information: {}\".format(info))\n",
    "\n",
    "env.close()\n",
    "print(\"\\nPGDrive successfully run!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
