{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing a mechanism of mutations and crossover only on the attention maps \n",
    "\n",
    "Here we introduce a mechanism of mutations and crossover only on the attention maps. The idea is to propose a mechanism which evolve individually the attention map of each head obtained in the previous notebook by using crossover and mutations. The key idea is that we use those attention maps to compute the activations of the next layer. The operation is made by making a weighted average of the activatition in the previous layer where the weights are the attention maps (where we applied mutations and crossover). The objective is to evolve the attention maps in order to obtain a better representation of the input, exploiting faster weights that lead to speeding up the convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def head_batched_attention_mechanism(Q, K, V):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Q: (batch_size, num_heads, num_layer, activation_size)\n",
    "        K: (batch_size, num_heads, num_layer, activation_size)\n",
    "        V: (batch_size, num_heads, activation_size, 1) # activations in the current layer\n",
    "\n",
    "    Returns:\n",
    "        attention: (batch_size, num_heads, activation_size)\n",
    "\n",
    "        # attention mechanism\n",
    "        # # (batch_size, num_heads, activation_size, activation_size)\n",
    "        # attention = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        # attention = attention / torch.sqrt(torch.tensor(activation_size).float())\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = nn.Softmax(dim=-1)(attention)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = torch.matmul(attention, V)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size)\n",
    "        # attention = attention.squeeze(-1)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return (nn.Softmax(dim=-1)(\n",
    "                torch.matmul( \n",
    "                            Q.transpose(-1,-2) , \n",
    "                            K\n",
    "                )/torch.sqrt(torch.tensor(8))    \n",
    "            ) @ V).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_emb, n_head) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert dim_emb % n_head == 0, 'dim_emb must be divisible by n_head'\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.W_Q = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W_K = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W_V = nn.Linear(dim_emb, dim_emb)\n",
    "\n",
    "        self.W_O = nn.Linear(dim_emb*n_head, dim_emb)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # get the shape of the input\n",
    "        batch_size, activation_size, activation_size = Q.size()\n",
    "        \n",
    "        # check the device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # move to device\n",
    "        Q.to(device)\n",
    "        K.to(device)\n",
    "        V.to(device)\n",
    "        \n",
    "        # reshape Q, K, V\n",
    "        # parallelize over the number of heads\n",
    "        # (batch_size, num_heads, num_layer, activation_size)\n",
    "        Q = torch.stack([Q for _ in range(self.n_head)], 1)\n",
    "        K = torch.stack([K for _ in range(self.n_head)], 1)\n",
    "        V = torch.stack([V for _ in range(self.n_head)], 1)\n",
    "\n",
    "        # apply linear transformation\n",
    "        Q = self.W_Q(Q)\n",
    "        K = self.W_K(K)\n",
    "        V = self.W_V(V).reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        \n",
    "        # apply attention mechanism\n",
    "        out_attention = head_batched_attention_mechanism(Q, K, V).reshape(batch_size, self.n_head*activation_size)\n",
    "\n",
    "        # apply linear transformation\n",
    "        return self.W_O(out_attention)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "\n",
    "# # I have the batched activations for each layer for each sample\n",
    "# layers = torch.stack([torch.randn(batch_size, number_activations) for _ in range(10)])\n",
    "# print(layers.shape, '(num_layers, batch_size, number_activations)')\n",
    "\n",
    "# # I get the activations for each layer for each sample\n",
    "# obj_activations = torch.stack([layers[:,i,:] for i in range(layers.shape[1])])\n",
    "# print(obj_activations.shape, '(nr_object, num_layers, activation_for_each_layer)')\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "number_activations = 8\n",
    "\n",
    "a = torch.randn(batch_size, number_activations)\n",
    "\n",
    "\n",
    "attn = get_layer_activations([torch.randn(batch_size, number_activations) for _ in range(10)])\n",
    "\n",
    "\n",
    "MultiHeadAttention(8, 4)(attn, attn, a).shape\n",
    "\n",
    "# list(extract_activations_per_sample(\n",
    "#             extract_activations_layers(layers), \n",
    "#             mask=False\n",
    "#         )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 torch.Size([16, 8]) (num_layers, batch_size, number_activations)\n",
      "torch.Size([16, 8])\n",
      "torch.Size([16, 10, 8])\n",
      "torch.Size([8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modified attention mechanism\n",
    "\n",
    "### GLOBAL VARIABLES\n",
    "CROSSOVER_MAGNITUDE = 0.3\n",
    "MUTATION_FACTOR = 0.3\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "\n",
    "def attention_map_crossover(attention_map):\n",
    "    \"\"\" Apply the crossover over the attention maps of each head.\n",
    "    The crosseover consists in picking a random index in the maxtrix over\n",
    "    the columns and swapping the values in between the columns of the\n",
    "    attention map.\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # get the crossover magnitude\n",
    "    crossover_magnitude = CROSSOVER_MAGNITUDE\n",
    "    \n",
    "    # get the batch size\n",
    "    dim_batch = attention_map.shape[0]\n",
    "    \n",
    "    # get the number of heads\n",
    "    number_of_heads = attention_map.shape[1]\n",
    "    \n",
    "    for idx_batch in range(dim_batch):\n",
    "        for idx_head in range(number_of_heads):\n",
    "            \n",
    "            # get the crossover index\n",
    "            crossover_index = attention_map.shape[2] - int(attention_map.shape[2]*crossover_magnitude)\n",
    "            \n",
    "            # get two random indexes\n",
    "            random_index_1 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            random_index_2 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            \n",
    "            # swap the values in that position over the columns\n",
    "            for idx, (x_1, x_2) in enumerate(zip(attention_map[idx_batch][idx_head][random_index_1][crossover_index:].detach(), attention_map[idx_batch][idx_head][random_index_2][crossover_index:].detach())):\n",
    "                \n",
    "                # debug\n",
    "                # print(attention_map[idx_batch][idx_head].shape, random_index_1, random_index_2, crossover_index)         \n",
    "                # print(x_1, x_2,idx_batch, idx_head, idx)\n",
    "                \n",
    "                # swap the values in that position over the columns\n",
    "                attention_map[idx_batch][idx_head][random_index_1][crossover_index+idx] = x_2 # make crossover\n",
    "                attention_map[idx_batch][idx_head][random_index_2][crossover_index+idx] = x_1 # make crossover\n",
    "    \n",
    "    return attention_map\n",
    "\n",
    "################################################################\n",
    "\n",
    "def mutate_attention_map(attention_map):\n",
    "    \"\"\" Mutate the attention map by making an elementwise multiplication with \n",
    "    a random tensor with values between 1-mutation_factor and 1+mutation_factor\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, num_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:    \n",
    "        torch.Tensor: shape (batch_size, num_heads, activation_size, activation_size)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # get the mutation factor\n",
    "    mutation_factor = MUTATION_FACTOR\n",
    "    # return the mutated attention map\n",
    "    # multiplied elementwise with a \n",
    "    # random matrix with values between \n",
    "    # 1-mutation_factor and 1+mutation_factor\n",
    "    return torch.mul(attention_map, torch.randn(attention_map.shape).uniform_(1-mutation_factor,1+mutation_factor))\n",
    "    \n",
    "    \n",
    "################################################################\n",
    "\n",
    "\n",
    "def head_batched_attention_mechanism(Q, K, V):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Q: (batch_size, num_heads, num_layer, activation_size)\n",
    "        K: (batch_size, num_heads, num_layer, activation_size)\n",
    "        V: (batch_size, num_heads, activation_size, 1) # activations in the current layer\n",
    "\n",
    "    Returns:\n",
    "        attention: (batch_size, num_heads, activation_size)\n",
    "\n",
    "        # attention mechanism\n",
    "        # # (batch_size, num_heads, activation_size, activation_size)\n",
    "        # attention = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        # attention = attention / torch.sqrt(torch.tensor(activation_size).float())\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = nn.Softmax(dim=-1)(attention)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = torch.matmul(attention, V)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size)\n",
    "        # attention = attention.squeeze(-1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # with probability p\n",
    "    p = torch.rand(1)\n",
    "    \n",
    "    # p <= 0.6 apply the mutation only\n",
    "    if p <= 0.6:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                    mutate_attention_map(torch.matmul( \n",
    "                                Q.transpose(-1,-2) , \n",
    "                                K\n",
    "                    )/torch.sqrt(torch.tensor(8)))    \n",
    "                ) @ V).squeeze(-1)\n",
    "        \n",
    "    # p > 0.6 apply the crossover only\n",
    "    else:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                attention_map_crossover(torch.matmul( \n",
    "                            Q.transpose(-1,-2) , \n",
    "                            K\n",
    "                )/torch.sqrt(torch.tensor(8)))    \n",
    "            ) @ V).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_emb, n_head) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert dim_emb % n_head == 0, 'dim_emb must be divisible by n_head'\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.W_Q = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W_K = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W_V = nn.Linear(dim_emb, dim_emb)\n",
    "\n",
    "        self.W_O = nn.Linear(dim_emb*n_head, dim_emb)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # get the shape of the input\n",
    "        batch_size, activation_size, activation_size = Q.size()\n",
    "        \n",
    "        # check the device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # move to device\n",
    "        Q.to(device)\n",
    "        K.to(device)\n",
    "        V.to(device)\n",
    "        \n",
    "        # reshape Q, K, V\n",
    "        # parallelize over the number of heads\n",
    "        # (batch_size, num_heads, num_layer, activation_size)\n",
    "        Q = torch.stack([Q for _ in range(self.n_head)], 1)\n",
    "        K = torch.stack([K for _ in range(self.n_head)], 1)\n",
    "        V = torch.stack([V for _ in range(self.n_head)], 1)\n",
    "\n",
    "        # apply linear transformation\n",
    "        Q = self.W_Q(Q)\n",
    "        K = self.W_K(K)\n",
    "        V = self.W_V(V).reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        \n",
    "        # apply attention mechanism\n",
    "        out_attention = head_batched_attention_mechanism(Q, K, V).reshape(batch_size, self.n_head*activation_size)\n",
    "\n",
    "        # apply linear transformation\n",
    "        return self.W_O(out_attention)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "\n",
    "\n",
    "# EXAMPLE \n",
    "#######################################################################\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "\n",
    "# inputs\n",
    "batch_size = 16\n",
    "number_activations = 8\n",
    "\n",
    "#######################################################################\n",
    "number_layers = 10\n",
    "\n",
    "activations = [torch.randn(batch_size, number_activations) for _ in range(number_layers)]\n",
    "# get layered activations\n",
    "attn = get_layer_activations(activations)\n",
    "print(len(activations), activations[0].size(),'(num_layers, batch_size, number_activations)')\n",
    "\n",
    "print(activations[0].size()) # (batch_size, number_activations)\n",
    "\n",
    "print(attn.shape) # (batch_size, number_layers, number_activations)\n",
    "\n",
    "# get shape of the attention map\n",
    "# shape: (batch_size, num_heads, activation_size, activation_size)\n",
    "print(MultiHeadAttention(8, 4)(attn, attn, a)[1].shape)\n",
    "\n",
    "# extract attention map for each head\n",
    "out = MultiHeadAttention(8, 4)(attn, attn, activations[-1])\n",
    "\n",
    "out.shape # (batch_size, activation_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # EXAMPLE element-wise multiplication\n",
    "# # mat = torch.stack([torch.Tensor([i for i in range(3)]) for _ in range(3)], 1) \n",
    "# # print(mat)\n",
    "# # print(torch.mul(mat, mat)) # element-wise multiplication\n",
    "\n",
    "# # # multiply two torch matrix element-wise\n",
    "# # mutation_factor = 0.03\n",
    "\n",
    "# # # how to mutate the attention map\n",
    "# # mutated_attention_map = torch.mul(attention_map, torch.randn(attention_map.shape).uniform_(1-mutation_factor,1+mutation_factor))\n",
    "# # # get the difference between the two matrices mutated_attention_map and attention_map\n",
    "# # print('mutation magnitude:',abs(torch.sum(mutated_attention_map-attention_map).detach().numpy()))\n",
    "\n",
    "\n",
    "# # indexing over the columns of the attention map\n",
    "\n",
    "# # def crossover_attention_map(attention_map, crossover_magnitude):\n",
    "\n",
    "\n",
    "# import multiprocessing\n",
    "\n",
    "# print(multiprocessing.cpu_count())\n",
    "\n",
    "# def attention_map_crossover(attention_map):\n",
    "    \n",
    "#     crossover_magnitude = 0.3\n",
    "    \n",
    "#     dim_batch = attention_map.shape[0]\n",
    "#     number_of_heads = attention_map.shape[1]\n",
    "    \n",
    "#     for idx_batch in range(dim_batch):\n",
    "#         for idx_head in range(number_of_heads):\n",
    "            \n",
    "#             print(idx_head)\n",
    "            \n",
    "#             crossover_index = attention_map.shape[2] - int(attention_map.shape[2]*crossover_magnitude)\n",
    "                \n",
    "#             random_index_1 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "#             random_index_2 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            \n",
    "#             for idx, (x_1, x_2) in enumerate(zip(attention_map[idx_batch][idx_head][random_index_1][crossover_index:].detach(), attention_map[idx_batch][idx_head][random_index_2][crossover_index:].detach())):\n",
    "                \n",
    "#                 print(attention_map[idx_batch][idx_head].shape, random_index_1, random_index_2, crossover_index)\n",
    "                            \n",
    "#                 print(x_1, x_2,idx_batch, idx_head, idx)\n",
    "                \n",
    "#                 attention_map[idx_batch][idx_head][random_index_1][crossover_index+idx] = x_2 # make crossover\n",
    "#                 attention_map[idx_batch][idx_head][random_index_2][crossover_index+idx] = x_1 # make crossover\n",
    "    \n",
    "#     return attention_map\n",
    "    \n",
    "\n",
    "# import time\n",
    "\n",
    "# start = time.time()\n",
    "# attention_map_crossover(attention_map).shape\n",
    "# end = time.time()\n",
    "\n",
    "# print('time required to perform crossover:',end-start)\n",
    "\n",
    "# torch.rand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Evaluate the layer with few neurons on the MNIST dataset. The results are good even with few neurons. The evidence shows that introducing mutations and crossover on the attention map used to weight the activation of the next layer makes the netwoek converge. It would be intresting to see the evaluation with more neurons and on more complex dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinW layers:\n",
      "\n",
      "Depth 0: LinW(\n",
      "  in_features=8, out_features=8, bias=True\n",
      "  (mha): MultiHeadAttention(\n",
      "    (W_Q): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (W_K): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (W_V): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (W_O): Linear(in_features=32, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "Depth 1: LinW(\n",
      "  in_features=8, out_features=8, bias=True\n",
      "  (mha): MultiHeadAttention(\n",
      "    (W_Q): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (W_K): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (W_V): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (W_O): Linear(in_features=32, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1/10, Training Loss: 0.9341, Training Accuracy: 68.37%, Test accuracy: 83.95%\n",
      "Epoch 2/10, Training Loss: 0.4378, Training Accuracy: 87.18%, Test accuracy: 89.38%\n",
      "Epoch 3/10, Training Loss: 0.3512, Training Accuracy: 89.89%, Test accuracy: 89.93%\n",
      "Epoch 4/10, Training Loss: 0.3149, Training Accuracy: 90.98%, Test accuracy: 91.60%\n",
      "Epoch 5/10, Training Loss: 0.2906, Training Accuracy: 91.63%, Test accuracy: 92.18%\n",
      "Epoch 6/10, Training Loss: 0.2740, Training Accuracy: 92.15%, Test accuracy: 92.10%\n",
      "Epoch 7/10, Training Loss: 0.2648, Training Accuracy: 92.50%, Test accuracy: 92.06%\n",
      "Epoch 8/10, Training Loss: 0.2585, Training Accuracy: 92.60%, Test accuracy: 92.44%\n",
      "Epoch 9/10, Training Loss: 0.2508, Training Accuracy: 92.83%, Test accuracy: 92.65%\n",
      "Epoch 10/10, Training Loss: 0.2460, Training Accuracy: 92.96%, Test accuracy: 92.60%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def evaluate(device, model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / total\n",
    "    return train_loss, accuracy\n",
    "\n",
    "### GLOBAL VARIABLES\n",
    "CROSSOVER_MAGNITUDE = 0.3\n",
    "MUTATION_FACTOR = 0.3\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "\n",
    "def attention_map_crossover(attention_map):\n",
    "    \"\"\" Apply the crossover over the attention maps of each head.\n",
    "    The crosseover consists in picking a random index in the maxtrix over\n",
    "    the columns and swapping the values in between the columns of the\n",
    "    attention map.\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # get the crossover magnitude\n",
    "    crossover_magnitude = CROSSOVER_MAGNITUDE\n",
    "    \n",
    "    # get the batch size\n",
    "    dim_batch = attention_map.shape[0]\n",
    "    \n",
    "    # get the number of heads\n",
    "    number_of_heads = attention_map.shape[1]\n",
    "    \n",
    "    for idx_batch in range(dim_batch):\n",
    "        for idx_head in range(number_of_heads):\n",
    "            \n",
    "            # get the crossover index\n",
    "            crossover_index = attention_map.shape[2] - int(attention_map.shape[2]*crossover_magnitude)\n",
    "            \n",
    "            # get two random indexes\n",
    "            random_index_1 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            random_index_2 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            \n",
    "            # swap the values in that position over the columns\n",
    "            for idx, (x_1, x_2) in enumerate(zip(attention_map[idx_batch][idx_head][random_index_1][crossover_index:].detach(), attention_map[idx_batch][idx_head][random_index_2][crossover_index:].detach())):\n",
    "                \n",
    "                # debug\n",
    "                # print(attention_map[idx_batch][idx_head].shape, random_index_1, random_index_2, crossover_index)         \n",
    "                # print(x_1, x_2,idx_batch, idx_head, idx)\n",
    "                \n",
    "                # swap the values in that position over the columns\n",
    "                attention_map[idx_batch][idx_head][random_index_1][crossover_index+idx] = x_2 # make crossover\n",
    "                attention_map[idx_batch][idx_head][random_index_2][crossover_index+idx] = x_1 # make crossover\n",
    "    \n",
    "    return attention_map\n",
    "\n",
    "################################################################\n",
    "\n",
    "def mutate_attention_map(attention_map):\n",
    "    \"\"\" Mutate the attention map by making an elementwise multiplication with \n",
    "    a random tensor with values between 1-mutation_factor and 1+mutation_factor\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, num_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:    \n",
    "        torch.Tensor: shape (batch_size, num_heads, activation_size, activation_size)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # get the mutation factor\n",
    "    mutation_factor = MUTATION_FACTOR\n",
    "    # return the mutated attention map\n",
    "    # multiplied elementwise with a \n",
    "    # random matrix with values between \n",
    "    # 1-mutation_factor and 1+mutation_factor\n",
    "    return torch.mul(attention_map, torch.randn(attention_map.shape).uniform_(1-mutation_factor,1+mutation_factor))\n",
    "    \n",
    "    \n",
    "################################################################\n",
    "\n",
    "\n",
    "def head_batched_attention_mechanism(Q, K, V):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Q: (batch_size, num_heads, num_layer, activation_size)\n",
    "        K: (batch_size, num_heads, num_layer, activation_size)\n",
    "        V: (batch_size, num_heads, activation_size, 1) # activations in the current layer\n",
    "\n",
    "    Returns:\n",
    "        attention: (batch_size, num_heads, activation_size)\n",
    "\n",
    "        # attention mechanism\n",
    "        # # (batch_size, num_heads, activation_size, activation_size)\n",
    "        # attention = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        # attention = attention / torch.sqrt(torch.tensor(activation_size).float())\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = nn.Softmax(dim=-1)(attention)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = torch.matmul(attention, V)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size)\n",
    "        # attention = attention.squeeze(-1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # with probability p\n",
    "    p = torch.rand(1)\n",
    "    \n",
    "    # p <= 0.6 apply the mutation only\n",
    "    if p <= 0.6:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                    mutate_attention_map(torch.matmul( \n",
    "                                Q.transpose(-1,-2) , \n",
    "                                K\n",
    "                    )/torch.sqrt(torch.tensor(8)))    \n",
    "                ) @ V).squeeze(-1)\n",
    "        \n",
    "    # p > 0.6 apply the crossover only\n",
    "    else:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                attention_map_crossover(torch.matmul( \n",
    "                            Q.transpose(-1,-2) , \n",
    "                            K\n",
    "                )/torch.sqrt(torch.tensor(8)))    \n",
    "            ) @ V).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_emb, n_head) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert dim_emb % n_head == 0, 'dim_emb must be divisible by n_head'\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.W_Q = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W_K = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W_V = nn.Linear(dim_emb, dim_emb)\n",
    "\n",
    "        self.W_O = nn.Linear(dim_emb*n_head, dim_emb)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # get the shape of the input\n",
    "        batch_size, activation_size, activation_size = Q.size()\n",
    "        \n",
    "        # check the device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # move to device\n",
    "        Q.to(device)\n",
    "        K.to(device)\n",
    "        V.to(device)\n",
    "        \n",
    "        # reshape Q, K, V\n",
    "        # parallelize over the number of heads\n",
    "        # (batch_size, num_heads, num_layer, activation_size)\n",
    "        Q = torch.stack([Q for _ in range(self.n_head)], 1)\n",
    "        K = torch.stack([K for _ in range(self.n_head)], 1)\n",
    "        V = torch.stack([V for _ in range(self.n_head)], 1)\n",
    "\n",
    "        # apply linear transformation\n",
    "        Q = self.W_Q(Q)\n",
    "        K = self.W_K(K)\n",
    "        V = self.W_V(V).reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        \n",
    "        # apply attention mechanism\n",
    "        out_attention = head_batched_attention_mechanism(Q, K, V).reshape(batch_size, self.n_head*activation_size)\n",
    "\n",
    "        # apply linear transformation\n",
    "        return self.W_O(out_attention)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "####################################################################\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(784, 8)\n",
    "        self.l2 = LinW(in_features=8, out_features=8, depth=0)\n",
    "        self.l3 = LinW(in_features=8, out_features=8, depth=1, layers=[self.l2])\n",
    "        self.l4 = nn.Linear(8, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.l1(x)\n",
    "        repr.append(x)\n",
    "        x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        # x = self.gelu(self.l2(x, repr))\n",
    "        x = self.l2(x, repr)\n",
    "        repr.append(x)\n",
    "        x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, depth, layers=[]):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.depth = depth\n",
    "        self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "        self.mha = MultiHeadAttention(in_features, 4)\n",
    "\n",
    "    def forward(self, input, activations=[]):\n",
    "        activations = get_layer_activations(activations)\n",
    "        return F.linear(self.mha(activations, activations, input), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "LinW layers:\n",
      "\n",
      "Depth 0: LinW(\n",
      "  in_features=32, out_features=32, bias=True\n",
      "  (mha): MultiHeadAttention(\n",
      "    (W_Q): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (W_K): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (W_V): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (W_O): Linear(in_features=128, out_features=32, bias=True)\n",
      "  )\n",
      ")\n",
      "Depth 1: LinW(\n",
      "  in_features=32, out_features=32, bias=True\n",
      "  (mha): MultiHeadAttention(\n",
      "    (W_Q): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (W_K): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (W_V): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (W_O): Linear(in_features=128, out_features=32, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1/10, Training Loss: 1.9556, Training Accuracy: 27.93%, Test accuracy: 34.38%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[270], line 348\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLinW layers:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDepth \u001b[39m\u001b[39m{\u001b[39;00mmodel[i]\u001b[39m.\u001b[39mdepth\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mmodel[i]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(model))]), sep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    347\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m--> 348\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(model, train_loader, criterion, optimizer, device)\n\u001b[1;32m    349\u001b[0m     test_accuracy \u001b[39m=\u001b[39m evaluate(device, model, test_loader)\n\u001b[1;32m    350\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mEPOCHS\u001b[39m}\u001b[39;00m\u001b[39m, Training Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Training Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%, Test accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtest_accuracy\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[270], line 36\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m images, labels \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     35\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 36\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     37\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     38\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/mambaforge/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[270], line 307\u001b[0m, in \u001b[0;36mMLPWD.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    305\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgelu(x)\n\u001b[1;32m    306\u001b[0m \u001b[39m# repr.append(x.detach().cpu().numpy())\u001b[39;00m\n\u001b[0;32m--> 307\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml3(x, \u001b[39mrepr\u001b[39;49m))\n\u001b[1;32m    308\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml4(x)\n\u001b[1;32m    309\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/mambaforge/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[270], line 327\u001b[0m, in \u001b[0;36mLinW.forward\u001b[0;34m(self, input, activations)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, activations\u001b[39m=\u001b[39m[]):\n\u001b[1;32m    326\u001b[0m     activations \u001b[39m=\u001b[39m get_layer_activations(activations)\n\u001b[0;32m--> 327\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmha(activations, activations, \u001b[39minput\u001b[39;49m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/mambaforge/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[270], line 218\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, Q, K, V)\u001b[0m\n\u001b[1;32m    215\u001b[0m V \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_V(V)\u001b[39m.\u001b[39mreshape(batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_head, activation_size, \u001b[39m1\u001b[39m)\n\u001b[1;32m    217\u001b[0m \u001b[39m# apply attention mechanism\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m out_attention \u001b[39m=\u001b[39m head_batched_attention_mechanism(Q, K, V)\u001b[39m.\u001b[39mreshape(batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_head\u001b[39m*\u001b[39mactivation_size)\n\u001b[1;32m    220\u001b[0m \u001b[39m# apply linear transformation\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_O(out_attention)\n",
      "Cell \u001b[0;32mIn[270], line 170\u001b[0m, in \u001b[0;36mhead_batched_attention_mechanism\u001b[0;34m(Q, K, V)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[39mreturn\u001b[39;00m (nn\u001b[39m.\u001b[39mSoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)(\n\u001b[1;32m    161\u001b[0m                 mutate_attention_map(torch\u001b[39m.\u001b[39mmatmul( \n\u001b[1;32m    162\u001b[0m                             Q\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m) , \n\u001b[1;32m    163\u001b[0m                             K\n\u001b[1;32m    164\u001b[0m                 )\u001b[39m/\u001b[39mtorch\u001b[39m.\u001b[39msqrt(torch\u001b[39m.\u001b[39mtensor(\u001b[39m8\u001b[39m)))    \n\u001b[1;32m    165\u001b[0m             ) \u001b[39m@\u001b[39m V)\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    167\u001b[0m \u001b[39m# p > 0.6 apply the crossover only\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m (nn\u001b[39m.\u001b[39mSoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)(\n\u001b[0;32m--> 170\u001b[0m             attention_map_crossover(torch\u001b[39m.\u001b[39;49mmatmul( \n\u001b[1;32m    171\u001b[0m                         Q\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m) , \n\u001b[1;32m    172\u001b[0m                         K\n\u001b[1;32m    173\u001b[0m             )\u001b[39m/\u001b[39;49mtorch\u001b[39m.\u001b[39;49msqrt(torch\u001b[39m.\u001b[39;49mtensor(\u001b[39m8\u001b[39;49m)))    \n\u001b[1;32m    174\u001b[0m         ) \u001b[39m@\u001b[39m V)\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[270], line 91\u001b[0m, in \u001b[0;36mattention_map_crossover\u001b[0;34m(attention_map)\u001b[0m\n\u001b[1;32m     88\u001b[0m random_index_2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, attention_map\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m],(\u001b[39m1\u001b[39m,))[\u001b[39m0\u001b[39m]\n\u001b[1;32m     90\u001b[0m \u001b[39m# swap the values in that position over the columns\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m \u001b[39mfor\u001b[39;00m idx, (x_1, x_2) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39;49m(attention_map[idx_batch][idx_head][random_index_1][crossover_index:]\u001b[39m.\u001b[39;49mdetach(), attention_map[idx_batch][idx_head][random_index_2][crossover_index:]\u001b[39m.\u001b[39;49mdetach())):\n\u001b[1;32m     92\u001b[0m     \n\u001b[1;32m     93\u001b[0m     \u001b[39m# debug\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     \u001b[39m# print(attention_map[idx_batch][idx_head].shape, random_index_1, random_index_2, crossover_index)         \u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[39m# print(x_1, x_2,idx_batch, idx_head, idx)\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \n\u001b[1;32m     97\u001b[0m     \u001b[39m# swap the values in that position over the columns\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     attention_map[idx_batch][idx_head][random_index_1][crossover_index\u001b[39m+\u001b[39midx] \u001b[39m=\u001b[39m x_2 \u001b[39m# make crossover\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     attention_map[idx_batch][idx_head][random_index_2][crossover_index\u001b[39m+\u001b[39midx] \u001b[39m=\u001b[39m x_1 \u001b[39m# make crossover\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/ml/lib/python3.10/site-packages/torch/_tensor.py:926\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_get_tracing_state():\n\u001b[1;32m    918\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    919\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    920\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPassing a tensor of different shape won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt change the number of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    924\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m    925\u001b[0m     )\n\u001b[0;32m--> 926\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39miter\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munbind(\u001b[39m0\u001b[39;49m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def evaluate(device, model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / total\n",
    "    return train_loss, accuracy\n",
    "\n",
    "### GLOBAL VARIABLES\n",
    "CROSSOVER_MAGNITUDE = 0.3\n",
    "MUTATION_FACTOR = 0.3\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "\n",
    "def attention_map_crossover(attention_map):\n",
    "    \"\"\" Apply the crossover over the attention maps of each head.\n",
    "    The crosseover consists in picking a random index in the maxtrix over\n",
    "    the columns and swapping the values in between the columns of the\n",
    "    attention map.\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # get the crossover magnitude\n",
    "    crossover_magnitude = CROSSOVER_MAGNITUDE\n",
    "    \n",
    "    # get the batch size\n",
    "    dim_batch = attention_map.shape[0]\n",
    "    \n",
    "    # get the number of heads\n",
    "    number_of_heads = attention_map.shape[1]\n",
    "    \n",
    "    for idx_batch in range(dim_batch):\n",
    "        for idx_head in range(number_of_heads):\n",
    "            \n",
    "            # get the crossover index\n",
    "            crossover_index = attention_map.shape[2] - int(attention_map.shape[2]*crossover_magnitude)\n",
    "            \n",
    "            # get two random indexes\n",
    "            random_index_1 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            random_index_2 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            \n",
    "            # swap the values in that position over the columns\n",
    "            for idx, (x_1, x_2) in enumerate(zip(attention_map[idx_batch][idx_head][random_index_1][crossover_index:].detach(), attention_map[idx_batch][idx_head][random_index_2][crossover_index:].detach())):\n",
    "                \n",
    "                # debug\n",
    "                # print(attention_map[idx_batch][idx_head].shape, random_index_1, random_index_2, crossover_index)         \n",
    "                # print(x_1, x_2,idx_batch, idx_head, idx)\n",
    "                \n",
    "                # swap the values in that position over the columns\n",
    "                attention_map[idx_batch][idx_head][random_index_1][crossover_index+idx] = x_2 # make crossover\n",
    "                attention_map[idx_batch][idx_head][random_index_2][crossover_index+idx] = x_1 # make crossover\n",
    "    \n",
    "    return attention_map\n",
    "\n",
    "################################################################\n",
    "\n",
    "def mutate_attention_map(attention_map):\n",
    "    \"\"\" Mutate the attention map by making an elementwise multiplication with \n",
    "    a random tensor with values between 1-mutation_factor and 1+mutation_factor\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, num_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:    \n",
    "        torch.Tensor: shape (batch_size, num_heads, activation_size, activation_size)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # get the mutation factor\n",
    "    mutation_factor = MUTATION_FACTOR\n",
    "    # return the mutated attention map\n",
    "    # multiplied elementwise with a \n",
    "    # random matrix with values between \n",
    "    # 1-mutation_factor and 1+mutation_factor\n",
    "    return torch.mul(attention_map, torch.randn(attention_map.shape).uniform_(1-mutation_factor,1+mutation_factor))\n",
    "    \n",
    "    \n",
    "################################################################\n",
    "\n",
    "\n",
    "def head_batched_attention_mechanism(Q, K, V):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Q: (batch_size, num_heads, num_layer, activation_size)\n",
    "        K: (batch_size, num_heads, num_layer, activation_size)\n",
    "        V: (batch_size, num_heads, activation_size, 1) # activations in the current layer\n",
    "\n",
    "    Returns:\n",
    "        attention: (batch_size, num_heads, activation_size)\n",
    "\n",
    "        # attention mechanism\n",
    "        # # (batch_size, num_heads, activation_size, activation_size)\n",
    "        # attention = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        # attention = attention / torch.sqrt(torch.tensor(activation_size).float())\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = nn.Softmax(dim=-1)(attention)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = torch.matmul(attention, V)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size)\n",
    "        # attention = attention.squeeze(-1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # with probability p\n",
    "    p = torch.rand(1)\n",
    "    \n",
    "    # p <= 0.6 apply the mutation only\n",
    "    if p <= 0.6:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                    mutate_attention_map(torch.matmul( \n",
    "                                Q.transpose(-1,-2) , \n",
    "                                K\n",
    "                    )/torch.sqrt(torch.tensor(8)))    \n",
    "                ) @ V).squeeze(-1)\n",
    "        \n",
    "    # p > 0.6 apply the crossover only\n",
    "    else:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                attention_map_crossover(torch.matmul( \n",
    "                            Q.transpose(-1,-2) , \n",
    "                            K\n",
    "                )/torch.sqrt(torch.tensor(8)))    \n",
    "            ) @ V).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_emb, n_head) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert dim_emb % n_head == 0, 'dim_emb must be divisible by n_head'\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.W_Q = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W_K = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W_V = nn.Linear(dim_emb, dim_emb)\n",
    "\n",
    "        self.W_O = nn.Linear(dim_emb*n_head, dim_emb)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # get the shape of the input\n",
    "        batch_size, activation_size, activation_size = Q.size()\n",
    "        \n",
    "        # check the device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # move to device\n",
    "        Q.to(device)\n",
    "        K.to(device)\n",
    "        V.to(device)\n",
    "        \n",
    "        # reshape Q, K, V\n",
    "        # parallelize over the number of heads\n",
    "        # (batch_size, num_heads, num_layer, activation_size)\n",
    "        Q = torch.stack([Q for _ in range(self.n_head)], 1)\n",
    "        K = torch.stack([K for _ in range(self.n_head)], 1)\n",
    "        V = torch.stack([V for _ in range(self.n_head)], 1)\n",
    "\n",
    "        # apply linear transformation\n",
    "        Q = self.W_Q(Q)\n",
    "        K = self.W_K(K)\n",
    "        V = self.W_V(V).reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        \n",
    "        # apply attention mechanism\n",
    "        out_attention = head_batched_attention_mechanism(Q, K, V).reshape(batch_size, self.n_head*activation_size)\n",
    "\n",
    "        # apply linear transformation\n",
    "        return self.W_O(out_attention)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, 32)\n",
    "        self.l2 = LinW(in_features=32, out_features=32, depth=0)\n",
    "        self.l3 = LinW(in_features=32, out_features=32, depth=1, layers=[self.l2])\n",
    "        self.l4 = nn.Linear(32, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.l1(x)\n",
    "        repr.append(x)\n",
    "        x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        # x = self.gelu(self.l2(x, repr))\n",
    "        x = self.l2(x, repr)\n",
    "        repr.append(x)\n",
    "        x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, depth, layers=[]):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.depth = depth\n",
    "        self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "        self.mha = MultiHeadAttention(in_features, 4)\n",
    "\n",
    "    def forward(self, input, activations=[]):\n",
    "        activations = get_layer_activations(activations)\n",
    "        return F.linear(self.mha(activations, activations, input), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 120\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I would like to test if adding ADD & NORM when we perform the crossover improve the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "LinW layers:\n",
      "\n",
      "Depth 0: LinW(\n",
      "  in_features=32, out_features=32, bias=True\n",
      "  (mha): MultiHeadAttention(\n",
      "    (W_Q): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (W_K): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (W_V): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (W_O): Linear(in_features=128, out_features=32, bias=True)\n",
      "  )\n",
      ")\n",
      "Depth 1: LinW(\n",
      "  in_features=32, out_features=32, bias=True\n",
      "  (mha): MultiHeadAttention(\n",
      "    (W_Q): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (W_K): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (W_V): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (W_O): Linear(in_features=128, out_features=32, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[273], line 354\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLinW layers:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDepth \u001b[39m\u001b[39m{\u001b[39;00mmodel[i]\u001b[39m.\u001b[39mdepth\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mmodel[i]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(model))]), sep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    353\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m--> 354\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(model, train_loader, criterion, optimizer, device)\n\u001b[1;32m    355\u001b[0m     test_accuracy \u001b[39m=\u001b[39m evaluate(device, model, test_loader)\n\u001b[1;32m    356\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mEPOCHS\u001b[39m}\u001b[39;00m\u001b[39m, Training Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Training Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%, Test accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtest_accuracy\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[273], line 38\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     36\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     37\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 38\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     39\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     40\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m images\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/ml/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/ml/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def evaluate(device, model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / total\n",
    "    return train_loss, accuracy\n",
    "\n",
    "### GLOBAL VARIABLES\n",
    "CROSSOVER_MAGNITUDE = 0.3\n",
    "MUTATION_FACTOR = 0.3\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "# attention_map_crossover + ADD & NORM\n",
    "\n",
    "def attention_map_crossover(attention_map):\n",
    "    \"\"\" Apply the crossover over the attention maps of each head.\n",
    "    The crosseover consists in picking a random index in the maxtrix over\n",
    "    the columns and swapping the values in between the columns of the\n",
    "    attention map.\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: shape (batch_size, number_of_heads, activation_size, activation_size)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # get the crossover magnitude\n",
    "    crossover_magnitude = CROSSOVER_MAGNITUDE\n",
    "    \n",
    "    # get the batch size\n",
    "    dim_batch = attention_map.shape[0]\n",
    "    \n",
    "    # get the number of heads\n",
    "    number_of_heads = attention_map.shape[1]\n",
    "    \n",
    "    for idx_batch in range(dim_batch):\n",
    "        for idx_head in range(number_of_heads):\n",
    "            \n",
    "            # get the crossover index\n",
    "            crossover_index = attention_map.shape[2] - int(attention_map.shape[2]*crossover_magnitude)\n",
    "            \n",
    "            # get two random indexes\n",
    "            random_index_1 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            random_index_2 = torch.randint(0, attention_map.shape[2],(1,))[0]\n",
    "            \n",
    "            # swap the values in that position over the columns\n",
    "            for idx, (x_1, x_2) in enumerate(zip(attention_map[idx_batch][idx_head][random_index_1][crossover_index:].detach(), attention_map[idx_batch][idx_head][random_index_2][crossover_index:].detach())):\n",
    "                \n",
    "                # debug\n",
    "                # print(attention_map[idx_batch][idx_head].shape, random_index_1, random_index_2, crossover_index)         \n",
    "                # print(x_1, x_2,idx_batch, idx_head, idx)\n",
    "                \n",
    "                temp = attention_map[idx_batch][idx_head]\n",
    "                \n",
    "                # swap the values in that position over the columns\n",
    "                attention_map[idx_batch][idx_head][random_index_1][crossover_index+idx] = x_2 # make crossover\n",
    "                attention_map[idx_batch][idx_head][random_index_2][crossover_index+idx] = x_1 # make crossover\n",
    "                \n",
    "                # ADD & NORM\n",
    "                attention_map[idx_batch][idx_head] = torch.nn.LayerNorm(attention_map[idx_batch][idx_head].shape)(attention_map[idx_batch][idx_head] + temp)\n",
    "    \n",
    "    return attention_map\n",
    "\n",
    "################################################################\n",
    "\n",
    "def mutate_attention_map(attention_map):\n",
    "    \"\"\" Mutate the attention map by making an elementwise multiplication with \n",
    "    a random tensor with values between 1-mutation_factor and 1+mutation_factor\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): shape (batch_size, num_heads, activation_size, activation_size)\n",
    "        \n",
    "    Returns:    \n",
    "        torch.Tensor: shape (batch_size, num_heads, activation_size, activation_size)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # get the mutation factor\n",
    "    mutation_factor = MUTATION_FACTOR\n",
    "    # return the mutated attention map\n",
    "    # multiplied elementwise with a \n",
    "    # random matrix with values between \n",
    "    # 1-mutation_factor and 1+mutation_factor\n",
    "    return torch.mul(attention_map, torch.randn(attention_map.shape).uniform_(1-mutation_factor,1+mutation_factor))\n",
    "    \n",
    "    \n",
    "################################################################\n",
    "\n",
    "\n",
    "def head_batched_attention_mechanism(Q, K, V):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Q: (batch_size, num_heads, num_layer, activation_size)\n",
    "        K: (batch_size, num_heads, num_layer, activation_size)\n",
    "        V: (batch_size, num_heads, activation_size, 1) # activations in the current layer\n",
    "\n",
    "    Returns:\n",
    "        attention: (batch_size, num_heads, activation_size)\n",
    "\n",
    "        # attention mechanism\n",
    "        # # (batch_size, num_heads, activation_size, activation_size)\n",
    "        # attention = torch.matmul(Q, K.transpose(-1,-2))\n",
    "        # attention = attention / torch.sqrt(torch.tensor(activation_size).float())\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = nn.Softmax(dim=-1)(attention)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size, 1)\n",
    "        # attention = torch.matmul(attention, V)\n",
    "\n",
    "        # # (batch_size, num_heads, activation_size)\n",
    "        # attention = attention.squeeze(-1)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # with probability p\n",
    "    p = torch.rand(1)\n",
    "    \n",
    "    # p <= 0.6 apply the mutation only\n",
    "    if p <= 0.6:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                    mutate_attention_map(torch.matmul( \n",
    "                                Q.transpose(-1,-2) , \n",
    "                                K\n",
    "                    )/torch.sqrt(torch.tensor(8)))    \n",
    "                ) @ V).squeeze(-1)\n",
    "        \n",
    "    # p > 0.6 apply the crossover only\n",
    "    else:\n",
    "        return (nn.Softmax(dim=-1)(\n",
    "                attention_map_crossover(torch.matmul( \n",
    "                            Q.transpose(-1,-2) , \n",
    "                            K\n",
    "                )/torch.sqrt(torch.tensor(8)))    \n",
    "            ) @ V).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_emb, n_head) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert dim_emb % n_head == 0, 'dim_emb must be divisible by n_head'\n",
    "\n",
    "        self.dim_emb = dim_emb\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.W_Q = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W_K = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W_V = nn.Linear(dim_emb, dim_emb)\n",
    "\n",
    "        self.W_O = nn.Linear(dim_emb*n_head, dim_emb)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # get the shape of the input\n",
    "        batch_size, activation_size, activation_size = Q.size()\n",
    "        \n",
    "        # check the device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # move to device\n",
    "        Q.to(device)\n",
    "        K.to(device)\n",
    "        V.to(device)\n",
    "        \n",
    "        # reshape Q, K, V\n",
    "        # parallelize over the number of heads\n",
    "        # (batch_size, num_heads, num_layer, activation_size)\n",
    "        Q = torch.stack([Q for _ in range(self.n_head)], 1)\n",
    "        K = torch.stack([K for _ in range(self.n_head)], 1)\n",
    "        V = torch.stack([V for _ in range(self.n_head)], 1)\n",
    "\n",
    "        # apply linear transformation\n",
    "        Q = self.W_Q(Q)\n",
    "        K = self.W_K(K)\n",
    "        V = self.W_V(V).reshape(batch_size, self.n_head, activation_size, 1)\n",
    "        \n",
    "        # apply attention mechanism\n",
    "        out_attention = head_batched_attention_mechanism(Q, K, V).reshape(batch_size, self.n_head*activation_size)\n",
    "\n",
    "        # apply linear transformation\n",
    "        return self.W_O(out_attention)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "def get_activations_per_object(activations):\n",
    "    \"\"\" Get the activations for each object per layer\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (num_layers, batch_size, number_activations)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.stack([activations[:,i,:] for i in range(activations.shape[1])])\n",
    "\n",
    "def get_layer_activations(activations):\n",
    "    \"\"\" Get the activations for each layer for each sample\n",
    "\n",
    "    Args:\n",
    "        activations (torch.Tensor): shape (batch_size, number_activations)\n",
    "        batch_size (int): batch size\n",
    "        number_activations (int): number of activations\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape (nr_object, num_layers, activation_for_each_layer)\n",
    "\n",
    "    \"\"\"\n",
    "    return get_activations_per_object(torch.stack(activations))\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, 32)\n",
    "        self.l2 = LinW(in_features=32, out_features=32, depth=0)\n",
    "        self.l3 = LinW(in_features=32, out_features=32, depth=1, layers=[self.l2])\n",
    "        self.l4 = nn.Linear(32, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.l1(x)\n",
    "        repr.append(x)\n",
    "        x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        # x = self.gelu(self.l2(x, repr))\n",
    "        x = self.l2(x, repr)\n",
    "        repr.append(x)\n",
    "        x = self.gelu(x)\n",
    "        # repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, depth, layers=[]):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.depth = depth\n",
    "        self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "        self.mha = MultiHeadAttention(in_features, 4)\n",
    "\n",
    "    def forward(self, input, activations=[]):\n",
    "        activations = get_layer_activations(activations)\n",
    "        return F.linear(self.mha(activations, activations, input), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3163cfb8aa3549ad3f5400bc3427ee7a4002d2a0d6d7ead52f641c6a7636395"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
