{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / total\n",
    "    return train_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def evaluate(device, model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinW layers:\n",
      "\n",
      "Depth 0: LinW(in_features=64, out_features=64, bias=True)\n",
      "Depth 1: LinW(in_features=64, out_features=64, bias=True)\n",
      "Epoch 1/10, Training Loss: 1.6687, Training Accuracy: 39.01%, Test accuracy: 49.06%\n",
      "Epoch 2/10, Training Loss: 1.3445, Training Accuracy: 51.48%, Test accuracy: 53.64%\n",
      "Epoch 3/10, Training Loss: 1.2649, Training Accuracy: 54.51%, Test accuracy: 56.10%\n",
      "Epoch 4/10, Training Loss: 1.2258, Training Accuracy: 55.48%, Test accuracy: 57.01%\n",
      "Epoch 5/10, Training Loss: 1.1836, Training Accuracy: 56.70%, Test accuracy: 56.09%\n",
      "Epoch 6/10, Training Loss: 1.1564, Training Accuracy: 57.52%, Test accuracy: 57.70%\n",
      "Epoch 7/10, Training Loss: 1.1321, Training Accuracy: 58.13%, Test accuracy: 57.77%\n",
      "Epoch 8/10, Training Loss: 1.1127, Training Accuracy: 59.04%, Test accuracy: 59.86%\n",
      "Epoch 9/10, Training Loss: 1.0984, Training Accuracy: 59.62%, Test accuracy: 59.93%\n",
      "Epoch 10/10, Training Loss: 1.0898, Training Accuracy: 59.99%, Test accuracy: 61.62%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "\n",
    "def extract_activations_layers(layers):\n",
    "    \"\"\" Extract for each layer the activations\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (layer_activation, batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array([np.array([np.array(h) for h in l]) for l in layers])\n",
    "\n",
    "def extract_activations_per_sample(layers, mask = False):\n",
    "    \"\"\" Extract for each sample the activations \n",
    "    for each layer and store them in a list.\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    if mask == True:\n",
    "        # mask the activations to remove zeros\n",
    "        mask = layers != 0\n",
    "        layers = [[np.array(h[m]) for h, m in zip(l,sm)] \n",
    "                for l, sm in zip(layers, mask)]\n",
    "        \n",
    "    return np.array([layers[:,i,:].flatten().reshape(-1, 1) for i in range(layers.shape[1])])\n",
    "\n",
    "\n",
    "def get_sampled_activations(activations, bandwidth = 0.2):\n",
    "    \"\"\" Sample the activations using KDE\n",
    "\n",
    "    Args:\n",
    "        activations (np.array): shape (batch_size, number_of_activations)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.from_numpy(np.array([KernelDensity(kernel=\"gaussian\", bandwidth=bandwidth).fit(a).sample([64]) for a in activations], dtype=\"float32\")).squeeze(2)\n",
    "\n",
    "def wd(layers: list()):\n",
    "    \"\"\" Compute the weight decay for each layer\n",
    "\n",
    "    Args:\n",
    "        layers (list): list of layers\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: weight decay\n",
    "\n",
    "    \"\"\"\n",
    "    return get_sampled_activations(\n",
    "                list(\n",
    "                    extract_activations_per_sample(\n",
    "                            extract_activations_layers(layers), \n",
    "                            mask=False\n",
    "                        )\n",
    "                ), \n",
    "                bandwidth=0.2\n",
    "            )\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(784, 64)\n",
    "        self.l2 = LinW(in_features=64, out_features=64, depth=0)\n",
    "        self.l3 = LinW(in_features=64, out_features=64, depth=1, layers=[self.l2])\n",
    "        self.l4 = nn.Linear(64, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l2(x, repr))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, depth, layers=[]):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.depth = depth\n",
    "        self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "\n",
    "    def forward(self, input, prev=[]):\n",
    "        # weight_decay = wd(prev)\n",
    "        # weight = self.weight * weight_decay.to('cuda:0')\n",
    "        # return F.linear(input, weight, self.bias)\n",
    "        # print(wd(prev).shape)\n",
    "        return F.linear(input* wd(prev).to('cuda:0'), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinW layers:\n",
      "\n",
      "Depth 0: LinW(in_features=64, out_features=64, bias=True)\n",
      "Depth 1: LinW(in_features=64, out_features=64, bias=True)\n",
      "Epoch 1/10, Training Loss: 2.3016, Training Accuracy: 11.37%, Test accuracy: 11.24%\n",
      "Epoch 2/10, Training Loss: 2.2997, Training Accuracy: 11.81%, Test accuracy: 11.93%\n",
      "Epoch 3/10, Training Loss: 2.2986, Training Accuracy: 12.07%, Test accuracy: 12.17%\n",
      "Epoch 4/10, Training Loss: 2.2990, Training Accuracy: 11.91%, Test accuracy: 11.67%\n",
      "Epoch 5/10, Training Loss: 2.2993, Training Accuracy: 11.83%, Test accuracy: 12.25%\n",
      "Epoch 6/10, Training Loss: 2.2988, Training Accuracy: 12.08%, Test accuracy: 12.16%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 142\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLinW layers:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDepth \u001b[39m\u001b[39m{\u001b[39;00mmodel[i]\u001b[39m.\u001b[39mdepth\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mmodel[i]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(model))]), sep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    141\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m--> 142\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(model, train_loader, criterion, optimizer, device)\n\u001b[1;32m    143\u001b[0m     test_accuracy \u001b[39m=\u001b[39m evaluate(device, model, test_loader)\n\u001b[1;32m    144\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mEPOCHS\u001b[39m}\u001b[39;00m\u001b[39m, Training Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Training Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%, Test accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtest_accuracy\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     11\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 12\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     13\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m images\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m     14\u001b[0m _, predicted \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mmax(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/jax/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     68\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/jax/lib/python3.11/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/jax/lib/python3.11/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/mambaforge/envs/jax/lib/python3.11/site-packages/torch/optim/adamw.py:171\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    158\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    160\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    161\u001b[0m         group,\n\u001b[1;32m    162\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    168\u001b[0m         state_steps,\n\u001b[1;32m    169\u001b[0m     )\n\u001b[0;32m--> 171\u001b[0m     adamw(\n\u001b[1;32m    172\u001b[0m         params_with_grad,\n\u001b[1;32m    173\u001b[0m         grads,\n\u001b[1;32m    174\u001b[0m         exp_avgs,\n\u001b[1;32m    175\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    176\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    177\u001b[0m         state_steps,\n\u001b[1;32m    178\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    179\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    180\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    181\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    182\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    183\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    184\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    185\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    186\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    187\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    188\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    189\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    190\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    191\u001b[0m     )\n\u001b[1;32m    193\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/mambaforge/envs/jax/lib/python3.11/site-packages/torch/optim/adamw.py:321\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 321\u001b[0m func(\n\u001b[1;32m    322\u001b[0m     params,\n\u001b[1;32m    323\u001b[0m     grads,\n\u001b[1;32m    324\u001b[0m     exp_avgs,\n\u001b[1;32m    325\u001b[0m     exp_avg_sqs,\n\u001b[1;32m    326\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m    327\u001b[0m     state_steps,\n\u001b[1;32m    328\u001b[0m     amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    329\u001b[0m     beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    330\u001b[0m     beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    331\u001b[0m     lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    332\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    333\u001b[0m     eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    334\u001b[0m     maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    335\u001b[0m     capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    336\u001b[0m     differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    337\u001b[0m     grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    338\u001b[0m     found_inf\u001b[39m=\u001b[39;49mfound_inf,\n\u001b[1;32m    339\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/jax/lib/python3.11/site-packages/torch/optim/adamw.py:493\u001b[0m, in \u001b[0;36m_multi_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    490\u001b[0m device_params \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mview_as_real(x) \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mis_complex(x) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m device_params]\n\u001b[1;32m    492\u001b[0m \u001b[39m# update steps\u001b[39;00m\n\u001b[0;32m--> 493\u001b[0m torch\u001b[39m.\u001b[39;49m_foreach_add_(device_state_steps, \u001b[39m1\u001b[39;49m)\n\u001b[1;32m    495\u001b[0m \u001b[39m# Perform stepweight decay\u001b[39;00m\n\u001b[1;32m    496\u001b[0m torch\u001b[39m.\u001b[39m_foreach_mul_(device_params, \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m lr \u001b[39m*\u001b[39m weight_decay)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "\n",
    "def extract_activations_layers(layers):\n",
    "    \"\"\" Extract for each layer the activations\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (layer_activation, batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array([np.array([np.array(h) for h in l]) for l in layers])\n",
    "\n",
    "def extract_activations_per_sample(layers, mask = False):\n",
    "    \"\"\" Extract for each sample the activations \n",
    "    for each layer and store them in a list.\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    if mask == True:\n",
    "        # mask the activations to remove zeros\n",
    "        mask = layers != 0\n",
    "        layers = [[np.array(h[m]) for h, m in zip(l,sm)] \n",
    "                for l, sm in zip(layers, mask)]\n",
    "        \n",
    "    return np.array([layers[:,i,:].flatten().reshape(-1, 1) for i in range(layers.shape[1])])\n",
    "\n",
    "\n",
    "def get_sampled_activations(activations, bandwidth = 0.2):\n",
    "    \"\"\" Sample the activations using KDE\n",
    "\n",
    "    Args:\n",
    "        activations (np.array): shape (batch_size, number_of_activations)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.from_numpy(np.array([KernelDensity(kernel=\"gaussian\", bandwidth=bandwidth).fit(a).sample([64]) for a in activations], dtype=\"float32\")).squeeze(2)\n",
    "\n",
    "def wd(layers: list()):\n",
    "    \"\"\" Compute the weight decay for each layer\n",
    "\n",
    "    Args:\n",
    "        layers (list): list of layers\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: weight decay\n",
    "\n",
    "    \"\"\"\n",
    "    return get_sampled_activations(\n",
    "                list(\n",
    "                    extract_activations_per_sample(\n",
    "                            extract_activations_layers(layers), \n",
    "                            mask=False\n",
    "                        )\n",
    "                ), \n",
    "                bandwidth=0.2\n",
    "            )\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(784, 64)\n",
    "        self.l2 = LinW(in_features=64, out_features=64, depth=0)\n",
    "        self.l3 = LinW(in_features=64, out_features=64, depth=1, layers=[self.l2])\n",
    "        self.l4 = nn.Linear(64, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l2(x, repr))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, depth, layers=[]):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.depth = depth\n",
    "        self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "\n",
    "    def forward(self, input, prev=[]):\n",
    "        # weight_decay = wd(prev)\n",
    "        # weight = self.weight * weight_decay.to('cuda:0')\n",
    "        # return F.linear(input, weight, self.bias)\n",
    "        # print(wd(prev).shape)\n",
    "        return F.linear(wd(prev).to('cuda:0'), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinW layers:\n",
      "\n",
      "Depth 0: LinW(in_features=64, out_features=64, bias=True)\n",
      "Depth 1: LinW(in_features=64, out_features=64, bias=True)\n",
      "Epoch 1/10, Training Loss: 0.3766, Training Accuracy: 88.75%, Test accuracy: 93.53%\n",
      "Epoch 2/10, Training Loss: 0.1644, Training Accuracy: 95.06%, Test accuracy: 95.93%\n",
      "Epoch 3/10, Training Loss: 0.1139, Training Accuracy: 96.59%, Test accuracy: 96.23%\n",
      "Epoch 4/10, Training Loss: 0.0916, Training Accuracy: 97.22%, Test accuracy: 96.74%\n",
      "Epoch 5/10, Training Loss: 0.0743, Training Accuracy: 97.71%, Test accuracy: 97.07%\n",
      "Epoch 6/10, Training Loss: 0.0606, Training Accuracy: 98.08%, Test accuracy: 96.70%\n",
      "Epoch 7/10, Training Loss: 0.0522, Training Accuracy: 98.39%, Test accuracy: 96.80%\n",
      "Epoch 8/10, Training Loss: 0.0446, Training Accuracy: 98.58%, Test accuracy: 97.38%\n",
      "Epoch 9/10, Training Loss: 0.0388, Training Accuracy: 98.82%, Test accuracy: 97.44%\n",
      "Epoch 10/10, Training Loss: 0.0337, Training Accuracy: 98.93%, Test accuracy: 97.44%\n"
     ]
    }
   ],
   "source": [
    "\"\"\" In this experiment we introduce a mutation magnitude over the \n",
    "activations in the intermediate layers. Since the activations mutate\n",
    "we still introduce an additional dynamic weight that play an important\n",
    "role in the training process to update the weights of the network.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def extract_activations_layers(layers):\n",
    "    \"\"\" Extract for each layer the activations\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (layer_activation, batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array([np.array([np.array(h) for h in l]) for l in layers])\n",
    "\n",
    "def extract_activations_per_sample(layers, mask = False):\n",
    "    \"\"\" Extract for each sample the activations \n",
    "    for each layer and store them in a list.\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    if mask == True:\n",
    "        # mask the activations to remove zeros\n",
    "        mask = layers != 0\n",
    "        layers = [[np.array(h[m]) for h, m in zip(l,sm)] \n",
    "                for l, sm in zip(layers, mask)]\n",
    "        \n",
    "    return np.array([layers[:,i,:].flatten().reshape(-1, 1) for i in range(layers.shape[1])])\n",
    "\n",
    "\n",
    "def get_sampled_activations(activations, bandwidth = 0.2):\n",
    "    \"\"\" Sample the activations using KDE\n",
    "\n",
    "    Args:\n",
    "        activations (np.array): shape (batch_size, number_of_activations)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.from_numpy(np.array([KernelDensity(kernel=\"gaussian\", bandwidth=bandwidth).fit(a).sample([64]) for a in activations], dtype=\"float32\")).squeeze(2)\n",
    "\n",
    "def wd(layers: list()):\n",
    "    \"\"\" Compute the weight decay for each layer\n",
    "\n",
    "    Args:\n",
    "        layers (list): list of layers\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: weight decay\n",
    "\n",
    "    \"\"\"\n",
    "    return get_sampled_activations(\n",
    "                list(\n",
    "                    extract_activations_per_sample(\n",
    "                            extract_activations_layers(layers), \n",
    "                            mask=False\n",
    "                        )\n",
    "                ), \n",
    "                bandwidth=0.2\n",
    "            )\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(784, 64)\n",
    "        self.l2 = LinW(in_features=64, out_features=64, depth=0)\n",
    "        self.l3 = LinW(in_features=64, out_features=64, depth=1, layers=[self.l2])\n",
    "        self.l4 = nn.Linear(64, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l2(x, repr))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, depth, layers=[]):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.depth = depth\n",
    "        self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "\n",
    "    def forward(self, input, prev=[]):\n",
    "        # weight_decay = wd(prev)\n",
    "        # weight = self.weight * weight_decay.to('cuda:0')\n",
    "        # return F.linear(input, weight, self.bias)\n",
    "        # print(wd(prev).shape)\n",
    "        return F.linear(input*random.uniform(0.99,1.09), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinW layers:\n",
      "\n",
      "Depth 0: LinW(in_features=64, out_features=64, bias=True)\n",
      "Depth 1: LinW(in_features=64, out_features=64, bias=True)\n",
      "Epoch 1/10, Training Loss: 0.7291, Training Accuracy: 75.94%, Test accuracy: 82.01%\n",
      "Epoch 2/10, Training Loss: 0.5565, Training Accuracy: 82.27%, Test accuracy: 83.00%\n",
      "Epoch 3/10, Training Loss: 0.5347, Training Accuracy: 82.81%, Test accuracy: 83.62%\n",
      "Epoch 4/10, Training Loss: 0.5326, Training Accuracy: 83.13%, Test accuracy: 83.76%\n",
      "Epoch 5/10, Training Loss: 0.5273, Training Accuracy: 83.41%, Test accuracy: 82.78%\n",
      "Epoch 6/10, Training Loss: 0.5334, Training Accuracy: 83.11%, Test accuracy: 82.91%\n",
      "Epoch 7/10, Training Loss: 0.5336, Training Accuracy: 83.21%, Test accuracy: 84.01%\n",
      "Epoch 8/10, Training Loss: 0.5358, Training Accuracy: 83.08%, Test accuracy: 83.01%\n",
      "Epoch 9/10, Training Loss: 0.5353, Training Accuracy: 83.09%, Test accuracy: 82.71%\n",
      "Epoch 10/10, Training Loss: 0.5376, Training Accuracy: 83.33%, Test accuracy: 83.13%\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Here insted of multiply I sum the sampled activations\n",
    "of the previous layers on the inputs.\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "\n",
    "def extract_activations_layers(layers):\n",
    "    \"\"\" Extract for each layer the activations\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (layer_activation, batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array([np.array([np.array(h) for h in l]) for l in layers])\n",
    "\n",
    "def extract_activations_per_sample(layers, mask = False):\n",
    "    \"\"\" Extract for each sample the activations \n",
    "    for each layer and store them in a list.\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    if mask == True:\n",
    "        # mask the activations to remove zeros\n",
    "        mask = layers != 0\n",
    "        layers = [[np.array(h[m]) for h, m in zip(l,sm)] \n",
    "                for l, sm in zip(layers, mask)]\n",
    "        \n",
    "    return np.array([layers[:,i,:].flatten().reshape(-1, 1) for i in range(layers.shape[1])])\n",
    "\n",
    "\n",
    "def get_sampled_activations(activations, bandwidth = 0.2):\n",
    "    \"\"\" Sample the activations using KDE\n",
    "\n",
    "    Args:\n",
    "        activations (np.array): shape (batch_size, number_of_activations)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.from_numpy(np.array([KernelDensity(kernel=\"gaussian\", bandwidth=bandwidth).fit(a).sample([64]) for a in activations], dtype=\"float32\")).squeeze(2)\n",
    "\n",
    "def wd(layers: list()):\n",
    "    \"\"\" Compute the weight decay for each layer\n",
    "\n",
    "    Args:\n",
    "        layers (list): list of layers\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: weight decay\n",
    "\n",
    "    \"\"\"\n",
    "    return get_sampled_activations(\n",
    "                list(\n",
    "                    extract_activations_per_sample(\n",
    "                            extract_activations_layers(layers), \n",
    "                            mask=False\n",
    "                        )\n",
    "                ), \n",
    "                bandwidth=0.2\n",
    "            )\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(784, 64)\n",
    "        self.l2 = LinW(in_features=64, out_features=64, depth=0)\n",
    "        self.l3 = LinW(in_features=64, out_features=64, depth=1, layers=[self.l2])\n",
    "        self.l4 = nn.Linear(64, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l2(x, repr))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, depth, layers=[]):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.depth = depth\n",
    "        self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "\n",
    "    def forward(self, input, prev=[]):\n",
    "        # weight_decay = wd(prev)\n",
    "        # weight = self.weight * weight_decay.to('cuda:0')\n",
    "        # return F.linear(input, weight, self.bias)\n",
    "        return F.linear(input+wd(prev).to('cuda:0'), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "\n",
    "def extract_activations_layers(layers):\n",
    "    \"\"\" Extract for each layer the activations\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (layer_activation, batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array([np.array([np.array(h) for h in l]) for l in layers])\n",
    "\n",
    "def extract_activations_per_sample(layers, mask = False):\n",
    "    \"\"\" Extract for each sample the activations \n",
    "    for each layer and store them in a list.\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    if mask == True:\n",
    "        # mask the activations to remove zeros\n",
    "        mask = layers != 0\n",
    "        layers = [[np.array(h[m]) for h, m in zip(l,sm)] \n",
    "                for l, sm in zip(layers, mask)]\n",
    "        \n",
    "    return np.array([layers[:,i,:].flatten().reshape(-1, 1) for i in range(layers.shape[1])])\n",
    "\n",
    "\n",
    "def get_sampled_activations(activations, bandwidth = 0.2):\n",
    "    \"\"\" Sample the activations using KDE\n",
    "\n",
    "    Args:\n",
    "        activations (np.array): shape (batch_size, number_of_activations)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.from_numpy(np.array([KernelDensity(kernel=\"gaussian\", bandwidth=bandwidth).fit(a).sample([64]) for a in activations], dtype=\"float32\")).squeeze(2)\n",
    "\n",
    "def wd(layers: list()):\n",
    "    \"\"\" Compute the weight decay for each layer\n",
    "\n",
    "    Args:\n",
    "        layers (list): list of layers\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: weight decay\n",
    "\n",
    "    \"\"\"\n",
    "    return get_sampled_activations(\n",
    "                list(\n",
    "                    extract_activations_per_sample(\n",
    "                            extract_activations_layers(layers), \n",
    "                            mask=False\n",
    "                        )\n",
    "                ), \n",
    "                bandwidth=0.2\n",
    "            )\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, 64)\n",
    "        self.l2 = LinW(in_features=64, out_features=64, depth=0)\n",
    "        self.l3 = LinW(in_features=64, out_features=64, depth=1, layers=[self.l2])\n",
    "        self.l4 = nn.Linear(64, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l2(x, repr))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, depth, layers=[]):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.depth = depth\n",
    "        self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "\n",
    "    def forward(self, input, prev=[]):\n",
    "        # weight_decay = wd(prev)\n",
    "        # weight = self.weight * weight_decay.to('cuda:0')\n",
    "        # return F.linear(input, weight, self.bias)\n",
    "        print(wd(prev).shape)\n",
    "        return F.linear(input, self.weight* wd(prev).to('cuda:0'), self.bias)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5981148 ],\n",
       "       [0.9634854 ],\n",
       "       [1.1180352 ],\n",
       "       [0.25245157],\n",
       "       [2.484147  ],\n",
       "       [1.161348  ],\n",
       "       [0.87380683],\n",
       "       [0.44336656],\n",
       "       [0.9320248 ],\n",
       "       [1.1274574 ],\n",
       "       [0.12749925],\n",
       "       [1.7562745 ],\n",
       "       [0.7457105 ],\n",
       "       [0.08640575],\n",
       "       [1.2357011 ],\n",
       "       [1.8209292 ],\n",
       "       [1.2359884 ],\n",
       "       [0.30162323],\n",
       "       [0.12449323],\n",
       "       [0.05077058],\n",
       "       [0.42775792],\n",
       "       [0.5930919 ],\n",
       "       [0.6066395 ],\n",
       "       [0.26709583],\n",
       "       [1.0321921 ],\n",
       "       [0.07010241],\n",
       "       [0.12023831],\n",
       "       [0.1930366 ],\n",
       "       [1.2029365 ],\n",
       "       [0.21384852],\n",
       "       [0.667683  ],\n",
       "       [0.22509019],\n",
       "       [0.97664076],\n",
       "       [0.33046725],\n",
       "       [1.3761361 ],\n",
       "       [0.6719827 ],\n",
       "       [0.3749767 ],\n",
       "       [0.51912063],\n",
       "       [2.6440165 ],\n",
       "       [0.3438414 ],\n",
       "       [0.44473463],\n",
       "       [0.35270986],\n",
       "       [0.8643006 ],\n",
       "       [0.46024805],\n",
       "       [0.7238416 ],\n",
       "       [1.5828304 ],\n",
       "       [1.4176172 ],\n",
       "       [0.28053018],\n",
       "       [0.47170863],\n",
       "       [1.2234609 ],\n",
       "       [0.50784916],\n",
       "       [0.2045496 ],\n",
       "       [0.7511464 ],\n",
       "       [2.2519631 ],\n",
       "       [0.53807163],\n",
       "       [0.18482922],\n",
       "       [1.0371845 ],\n",
       "       [0.7920953 ],\n",
       "       [1.0215054 ],\n",
       "       [0.9137935 ],\n",
       "       [0.03120091],\n",
       "       [2.0445652 ],\n",
       "       [0.41222066],\n",
       "       [0.99777234],\n",
       "       [0.19569786],\n",
       "       [1.4346701 ],\n",
       "       [0.5884427 ],\n",
       "       [0.8527775 ],\n",
       "       [1.1589556 ],\n",
       "       [0.06320994],\n",
       "       [0.23740946],\n",
       "       [1.695517  ],\n",
       "       [0.73437613],\n",
       "       [1.1560967 ],\n",
       "       [0.54575336],\n",
       "       [0.4884792 ],\n",
       "       [0.14459895],\n",
       "       [0.3690647 ],\n",
       "       [0.8651468 ],\n",
       "       [0.778015  ],\n",
       "       [0.93204296],\n",
       "       [1.1930641 ],\n",
       "       [0.53393674],\n",
       "       [0.24493909],\n",
       "       [1.074556  ],\n",
       "       [2.1423216 ],\n",
       "       [0.8089287 ],\n",
       "       [1.55228   ],\n",
       "       [0.16377902],\n",
       "       [0.7034127 ],\n",
       "       [0.09425773],\n",
       "       [1.121094  ],\n",
       "       [0.3372933 ],\n",
       "       [0.53050864],\n",
       "       [0.5872485 ],\n",
       "       [0.6916148 ],\n",
       "       [0.47560677],\n",
       "       [0.7894868 ],\n",
       "       [1.4552455 ],\n",
       "       [1.3207943 ],\n",
       "       [1.576813  ],\n",
       "       [0.6690262 ],\n",
       "       [2.5116918 ],\n",
       "       [0.6993155 ],\n",
       "       [0.07243504],\n",
       "       [0.4027928 ],\n",
       "       [0.03596292],\n",
       "       [1.0209202 ],\n",
       "       [0.42574608],\n",
       "       [0.34751523],\n",
       "       [1.287222  ],\n",
       "       [0.9133664 ],\n",
       "       [0.228193  ],\n",
       "       [0.22320612],\n",
       "       [1.4524608 ],\n",
       "       [0.57388884],\n",
       "       [1.3554044 ],\n",
       "       [1.1602938 ],\n",
       "       [1.9208887 ],\n",
       "       [1.4232796 ],\n",
       "       [0.6460886 ],\n",
       "       [0.4586695 ],\n",
       "       [0.7327422 ],\n",
       "       [1.042147  ],\n",
       "       [1.0741682 ],\n",
       "       [0.0640603 ],\n",
       "       [1.0612597 ],\n",
       "       [0.1775695 ],\n",
       "       [2.018416  ],\n",
       "       [0.60332924],\n",
       "       [1.1085734 ],\n",
       "       [1.0403162 ],\n",
       "       [1.4402299 ],\n",
       "       [0.26818538],\n",
       "       [0.07688841],\n",
       "       [0.56014514],\n",
       "       [0.8123944 ],\n",
       "       [0.01805807],\n",
       "       [1.1996312 ],\n",
       "       [1.207614  ],\n",
       "       [0.59501743],\n",
       "       [0.39193186],\n",
       "       [0.560128  ],\n",
       "       [0.52633315],\n",
       "       [1.4703368 ],\n",
       "       [0.01261687],\n",
       "       [0.6666628 ],\n",
       "       [0.7889252 ],\n",
       "       [0.1775695 ],\n",
       "       [0.930572  ],\n",
       "       [1.1527754 ],\n",
       "       [0.9446422 ],\n",
       "       [0.57017314],\n",
       "       [0.5061317 ],\n",
       "       [1.5567933 ],\n",
       "       [0.3890073 ],\n",
       "       [1.1896926 ],\n",
       "       [0.22254352],\n",
       "       [0.3779905 ],\n",
       "       [1.392532  ],\n",
       "       [0.10936603],\n",
       "       [0.28346473],\n",
       "       [0.27952614],\n",
       "       [0.30967483],\n",
       "       [0.8877983 ],\n",
       "       [1.0146838 ],\n",
       "       [0.56167877],\n",
       "       [0.12194374],\n",
       "       [0.33555678],\n",
       "       [0.48301175],\n",
       "       [0.17589402],\n",
       "       [0.9133664 ],\n",
       "       [3.136038  ],\n",
       "       [0.9940765 ],\n",
       "       [0.13363504],\n",
       "       [2.2219634 ],\n",
       "       [1.97149   ],\n",
       "       [0.05064965],\n",
       "       [0.6725232 ],\n",
       "       [1.1178315 ],\n",
       "       [0.37686482],\n",
       "       [1.5633483 ],\n",
       "       [0.5267799 ],\n",
       "       [0.4258197 ],\n",
       "       [1.034227  ],\n",
       "       [0.55753845],\n",
       "       [1.5645511 ],\n",
       "       [0.9010264 ],\n",
       "       [0.55299866],\n",
       "       [0.9640119 ],\n",
       "       [1.5734146 ],\n",
       "       [1.7805357 ],\n",
       "       [1.4475883 ],\n",
       "       [0.21543406],\n",
       "       [0.34803212],\n",
       "       [0.8963098 ],\n",
       "       [0.20717287],\n",
       "       [1.0394332 ],\n",
       "       [0.1775695 ],\n",
       "       [1.7986577 ],\n",
       "       [1.9142076 ],\n",
       "       [1.0908751 ],\n",
       "       [1.1357872 ],\n",
       "       [0.44622457],\n",
       "       [0.8096691 ],\n",
       "       [1.2675534 ],\n",
       "       [0.59504235],\n",
       "       [1.0145636 ],\n",
       "       [1.4249876 ],\n",
       "       [1.514438  ],\n",
       "       [0.13732429],\n",
       "       [1.3617759 ],\n",
       "       [0.3521906 ],\n",
       "       [0.57723355],\n",
       "       [0.08985323],\n",
       "       [3.0772026 ],\n",
       "       [0.63465965],\n",
       "       [0.30708033],\n",
       "       [0.3627165 ],\n",
       "       [1.97149   ],\n",
       "       [0.64237463],\n",
       "       [1.2600849 ],\n",
       "       [1.0624195 ],\n",
       "       [1.7796872 ],\n",
       "       [0.768236  ],\n",
       "       [0.12132883],\n",
       "       [1.1736423 ],\n",
       "       [0.24950922],\n",
       "       [0.8920729 ],\n",
       "       [1.7750767 ],\n",
       "       [0.43655518],\n",
       "       [1.1984861 ],\n",
       "       [0.50459594],\n",
       "       [0.96194965],\n",
       "       [0.57226354],\n",
       "       [1.6398058 ],\n",
       "       [1.643845  ],\n",
       "       [1.4081545 ],\n",
       "       [0.29326728],\n",
       "       [1.7232093 ],\n",
       "       [0.6098047 ],\n",
       "       [0.32941854],\n",
       "       [1.953321  ],\n",
       "       [0.29289347],\n",
       "       [1.1071558 ],\n",
       "       [0.6211758 ],\n",
       "       [1.2004658 ],\n",
       "       [0.09378791],\n",
       "       [0.32033768],\n",
       "       [0.03670176],\n",
       "       [3.7366617 ],\n",
       "       [0.08468546],\n",
       "       [0.4667275 ],\n",
       "       [0.7361483 ],\n",
       "       [0.07269607],\n",
       "       [0.67573196]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = [torch.randn(16,256) for i in range(5)]\n",
    "\n",
    "layers = np.array([np.array(i.flatten()) for i in layers]).flatten()\n",
    "mask = layers > 0\n",
    "layers = layers[mask]\n",
    "\n",
    "res = layers[np.random.randint(0, high = len(layers), size = 256)].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 74\u001b[0m\n\u001b[1;32m     70\u001b[0m layers \u001b[39m=\u001b[39m extract_activations_per_sample(extract_activations_layers(layers))\n\u001b[1;32m     71\u001b[0m layers \u001b[39m=\u001b[39m [l[l \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m layers]\n\u001b[0;32m---> 74\u001b[0m [random\u001b[39m.\u001b[39;49mchoice(l) \u001b[39mfor\u001b[39;49;00m l \u001b[39min\u001b[39;49;00m layers]\n\u001b[1;32m     76\u001b[0m \u001b[39m# res = layers[np.random.randint(0, high = layers.shape[0]-1, size = 256)].reshape(-1, 1)\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[39m# return torch.from_numpy(np.array(res, dtype=\"float32\"))\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \n\u001b[1;32m     79\u001b[0m \n\u001b[1;32m     80\u001b[0m \u001b[39m# wd(layers).shape\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 74\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     70\u001b[0m layers \u001b[39m=\u001b[39m extract_activations_per_sample(extract_activations_layers(layers))\n\u001b[1;32m     71\u001b[0m layers \u001b[39m=\u001b[39m [l[l \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m layers]\n\u001b[0;32m---> 74\u001b[0m [random\u001b[39m.\u001b[39;49mchoice(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m layers]\n\u001b[1;32m     76\u001b[0m \u001b[39m# res = layers[np.random.randint(0, high = layers.shape[0]-1, size = 256)].reshape(-1, 1)\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[39m# return torch.from_numpy(np.array(res, dtype=\"float32\"))\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \n\u001b[1;32m     79\u001b[0m \n\u001b[1;32m     80\u001b[0m \u001b[39m# wd(layers).shape\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/jax/lib/python3.11/random.py:369\u001b[0m, in \u001b[0;36mRandom.choice\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mchoice\u001b[39m(\u001b[39mself\u001b[39m, seq):\n\u001b[1;32m    368\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Choose a random element from a non-empty sequence.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m seq:\n\u001b[1;32m    370\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mCannot choose from an empty sequence\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    371\u001b[0m     \u001b[39mreturn\u001b[39;00m seq[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_randbelow(\u001b[39mlen\u001b[39m(seq))]\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_activations_layers(layers):\n",
    "    \"\"\" Extract for each layer the activations\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (layer_activation, batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array([np.array([np.array(h) for h in l]) for l in layers])\n",
    "\n",
    "def extract_activations_per_sample(layers, mask = False):\n",
    "    \"\"\" Extract for each sample the activations \n",
    "    for each layer and store them in a list.\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    if mask == True:\n",
    "        # mask the activations to remove zeros\n",
    "        mask = layers != 0\n",
    "        layers = [[np.array(h[m]) for h, m in zip(l,sm)] \n",
    "                for l, sm in zip(layers, mask)]\n",
    "        \n",
    "    return np.array([layers[:,i,:].flatten().reshape(-1, 1) for i in range(layers.shape[1])])\n",
    "\n",
    "\n",
    "def get_sampled_activations(activations, bandwidth = 0.2):\n",
    "    \"\"\" Sample the activations using KDE\n",
    "\n",
    "    Args:\n",
    "        activations (np.array): shape (batch_size, number_of_activations)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.from_numpy(np.array([KernelDensity(kernel=\"gaussian\", bandwidth=bandwidth).fit(a).sample([64]) for a in activations], dtype=\"float32\")).squeeze(2)\n",
    "\n",
    "def wd(layers: list()):\n",
    "    \"\"\" Compute the weight decay for each layer\n",
    "\n",
    "    Args:\n",
    "        layers (list): list of layers\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: weight decay\n",
    "\n",
    "    \"\"\"\n",
    "    return get_sampled_activations(\n",
    "                list(\n",
    "                    extract_activations_per_sample(\n",
    "                            extract_activations_layers(layers), \n",
    "                            mask=False\n",
    "                        )\n",
    "                ), \n",
    "                bandwidth=0.2\n",
    "            )\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "layers = [torch.randn(64,5) for i in range(5)]\n",
    "\n",
    "layers = extract_activations_per_sample(extract_activations_layers(layers))\n",
    "layers = [l[l < 0] for l in layers]\n",
    "\n",
    "\n",
    "[random.choice(l) for l in layers]\n",
    "\n",
    "# res = layers[np.random.randint(0, high = layers.shape[0]-1, size = 256)].reshape(-1, 1)\n",
    "# return torch.from_numpy(np.array(res, dtype=\"float32\"))\n",
    "\n",
    "\n",
    "# wd(layers).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/10, Training Loss: 1.8164, Training Accuracy: 33.93%, Test accuracy: 39.72%\n",
      "Epoch 2/10, Training Loss: 1.6439, Training Accuracy: 40.74%, Test accuracy: 41.99%\n",
      "Epoch 3/10, Training Loss: 1.5454, Training Accuracy: 44.20%, Test accuracy: 42.73%\n",
      "Epoch 4/10, Training Loss: 1.4934, Training Accuracy: 45.90%, Test accuracy: 44.51%\n",
      "Epoch 5/10, Training Loss: 1.4409, Training Accuracy: 48.04%, Test accuracy: 49.23%\n",
      "Epoch 6/10, Training Loss: 1.4050, Training Accuracy: 49.34%, Test accuracy: 47.79%\n",
      "Epoch 7/10, Training Loss: 1.3646, Training Accuracy: 50.74%, Test accuracy: 48.76%\n",
      "Epoch 8/10, Training Loss: 1.3404, Training Accuracy: 51.69%, Test accuracy: 48.74%\n",
      "Epoch 9/10, Training Loss: 1.3010, Training Accuracy: 53.03%, Test accuracy: 49.75%\n",
      "Epoch 10/10, Training Loss: 1.2674, Training Accuracy: 54.31%, Test accuracy: 49.79%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, 256)\n",
    "        self.l4 = nn.Linear(256, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        x = self.gelu(self.l2(x))\n",
    "        x = self.gelu(self.l3(x))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLP().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "LinW layers:\n",
      "\n",
      "Depth 0: LinW(in_features=64, out_features=64, bias=True)\n",
      "Depth 1: LinW(in_features=64, out_features=64, bias=True)\n",
      "Epoch 1/10, Training Loss: 2.1590, Training Accuracy: 22.46%, Test accuracy: 24.77%\n",
      "Epoch 2/10, Training Loss: 2.0821, Training Accuracy: 25.27%, Test accuracy: 25.72%\n",
      "Epoch 3/10, Training Loss: 2.0594, Training Accuracy: 25.38%, Test accuracy: 26.05%\n",
      "Epoch 4/10, Training Loss: 2.0429, Training Accuracy: 26.14%, Test accuracy: 27.14%\n",
      "Epoch 5/10, Training Loss: 2.0360, Training Accuracy: 26.53%, Test accuracy: 26.75%\n",
      "Epoch 6/10, Training Loss: 2.0265, Training Accuracy: 26.52%, Test accuracy: 26.78%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 151\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLinW layers:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDepth \u001b[39m\u001b[39m{\u001b[39;00mmodel[i]\u001b[39m.\u001b[39mdepth\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mmodel[i]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(model))]), sep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    150\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m--> 151\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(model, train_loader, criterion, optimizer, device)\n\u001b[1;32m    152\u001b[0m     test_accuracy \u001b[39m=\u001b[39m evaluate(device, model, test_loader)\n\u001b[1;32m    153\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mEPOCHS\u001b[39m}\u001b[39;00m\u001b[39m, Training Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Training Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%, Test accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtest_accuracy\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[31], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      5\u001b[0m total \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfor\u001b[39;00m images, labels \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m      7\u001b[0m     images, labels \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/mambaforge/envs/ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/mambaforge/envs/ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/mambaforge/envs/ml/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/mambaforge/envs/ml/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/mambaforge/envs/ml/lib/python3.10/site-packages/torchvision/datasets/mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/mambaforge/envs/ml/lib/python3.10/site-packages/torchvision/transforms/transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m    128\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[0;32m~/mambaforge/envs/ml/lib/python3.10/site-packages/torchvision/transforms/functional.py:163\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39m# handle PIL Image\u001b[39;00m\n\u001b[1;32m    162\u001b[0m mode_to_nptype \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mI\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mint32, \u001b[39m\"\u001b[39m\u001b[39mI;16\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mint16, \u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mfloat32}\n\u001b[0;32m--> 163\u001b[0m img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39;49marray(pic, mode_to_nptype\u001b[39m.\u001b[39;49mget(pic\u001b[39m.\u001b[39;49mmode, np\u001b[39m.\u001b[39;49muint8), copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    166\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n",
      "File \u001b[0;32m~/mambaforge/envs/ml/lib/python3.10/site-packages/PIL/Image.py:513\u001b[0m, in \u001b[0;36mImage.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpyaccess \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    511\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exif \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 513\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[1;32m    514\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcategory\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    515\u001b[0m         deprecate(\u001b[39m\"\u001b[39m\u001b[39mImage categories\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mis_animated\u001b[39m\u001b[39m\"\u001b[39m, plural\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "\n",
    "def extract_activations_layers(layers):\n",
    "    \"\"\" Extract for each layer the activations\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (layer_activation, batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array([np.array([np.array(h) for h in l]) for l in layers])\n",
    "\n",
    "def extract_activations_per_sample(layers, mask = False):\n",
    "    \"\"\" Extract for each sample the activations \n",
    "    for each layer and store them in a list.\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    if mask == True:\n",
    "        # mask the activations to remove zeros\n",
    "        mask = layers != 0\n",
    "        layers = [[np.array(h[m]) for h, m in zip(l,sm)] \n",
    "                for l, sm in zip(layers, mask)]\n",
    "        \n",
    "    return np.array([layers[:,i,:].flatten().reshape(-1, 1) for i in range(layers.shape[1])])\n",
    "\n",
    "\n",
    "def get_sampled_activations(activations, bandwidth = 0.2):\n",
    "    \"\"\" Sample the activations using KDE\n",
    "\n",
    "    Args:\n",
    "        activations (np.array): shape (batch_size, number_of_activations)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.stack([\n",
    "                    torch.tensor(KernelDensity(kernel=\"gaussian\", bandwidth=bandwidth).fit(a).sample([1]), dtype=torch.float32).squeeze(0) \n",
    "                    for a in activations], dim=0)\n",
    "\n",
    "def wd(layers: list()):\n",
    "    \"\"\" Compute the weight decay for each layer\n",
    "\n",
    "    Args:\n",
    "        layers (list): list of layers\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: weight decay\n",
    "\n",
    "    \"\"\"\n",
    "    return get_sampled_activations(\n",
    "                list(torch.from_numpy(extract_activations_layers(layers)).permute(1,0,2).numpy()), \n",
    "                bandwidth=0.2\n",
    "            )\n",
    "    \n",
    "    \n",
    "# N_layer = 5\n",
    "\n",
    "# prev = [torch.randn(12,64) for i in range(N_layer)]\n",
    "\n",
    "# wd(prev).shape\n",
    "\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(784, 64)\n",
    "        self.l2 = LinW(in_features=64, out_features=64, depth=0)\n",
    "        self.l3 = LinW(in_features=64, out_features=64, depth=1, layers=[self.l2])\n",
    "        self.l4 = nn.Linear(64, 10)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l2(x, repr))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, depth, layers=[]):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.depth = depth\n",
    "        self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "\n",
    "    def forward(self, input, prev=[]):\n",
    "        # weight_decay = wd(prev)\n",
    "        # weight = self.weight * weight_decay.to('cuda:0')\n",
    "        # return F.linear(input, weight, self.bias)\n",
    "        # print(wd(prev).shape)\n",
    "        return F.linear(wd(prev).to(device), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# check if mps are available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "LinW layers:\n",
      "\n",
      "Depth 0: LinW(in_features=8, out_features=8, bias=True)\n",
      "Depth 1: LinW(in_features=8, out_features=8, bias=True)\n",
      "Epoch 1/8, Training Loss: 1.9131, Training Accuracy: 29.59%, Test accuracy: 33.05%\n",
      "Epoch 2/8, Training Loss: 1.8118, Training Accuracy: 34.05%, Test accuracy: 33.90%\n",
      "Epoch 3/8, Training Loss: 1.8016, Training Accuracy: 34.52%, Test accuracy: 34.20%\n",
      "Epoch 4/8, Training Loss: 1.8011, Training Accuracy: 34.51%, Test accuracy: 35.09%\n",
      "Epoch 5/8, Training Loss: 1.7988, Training Accuracy: 34.48%, Test accuracy: 34.99%\n",
      "Epoch 6/8, Training Loss: 1.7970, Training Accuracy: 34.71%, Test accuracy: 34.54%\n",
      "Epoch 7/8, Training Loss: 1.7936, Training Accuracy: 34.67%, Test accuracy: 34.90%\n",
      "Epoch 8/8, Training Loss: 1.7837, Training Accuracy: 35.40%, Test accuracy: 35.95%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "\n",
    "def extract_activations_layers(layers):\n",
    "    \"\"\" Extract for each layer the activations\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (layer_activation, batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array([np.array([np.array(h) for h in l]) for l in layers])\n",
    "\n",
    "def extract_activations_per_sample(layers, mask = False):\n",
    "    \"\"\" Extract for each sample the activations \n",
    "    for each layer and store them in a list.\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    if mask == True:\n",
    "        # mask the activations to remove zeros\n",
    "        mask = layers != 0\n",
    "        layers = [[np.array(h[m]) for h, m in zip(l,sm)] \n",
    "                for l, sm in zip(layers, mask)]\n",
    "        \n",
    "    return np.array([layers[:,i,:].flatten().reshape(-1, 1) for i in range(layers.shape[1])])\n",
    "\n",
    "\n",
    "def get_sampled_activations(activations, bandwidth = 0.2):\n",
    "    \"\"\" Sample the activations using KDE\n",
    "\n",
    "    Args:\n",
    "        activations (np.array): shape (batch_size, number_of_activations)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.stack([\n",
    "                    torch.tensor(KernelDensity(kernel=\"gaussian\", bandwidth=bandwidth).fit(a).sample([1]), dtype=torch.float32).squeeze(0) \n",
    "                    for a in activations], dim=0)\n",
    "\n",
    "def wd(layers: list()):\n",
    "    \"\"\" Compute the weight decay for each layer\n",
    "\n",
    "    Args:\n",
    "        layers (list): list of layers\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: weight decay\n",
    "\n",
    "    \"\"\"\n",
    "    return get_sampled_activations(\n",
    "                list(torch.from_numpy(extract_activations_layers(layers)).permute(1,0,2).numpy()), \n",
    "                bandwidth=0.2\n",
    "            )\n",
    "    \n",
    "    \n",
    "# N_layer = 5\n",
    "\n",
    "# prev = [torch.randn(12,64) for i in range(N_layer)]\n",
    "\n",
    "# wd(prev).shape\n",
    "\n",
    "n_neurons = 8\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(784, n_neurons)\n",
    "        self.l2 = LinW(in_features=n_neurons, out_features=n_neurons, depth=0)\n",
    "        self.l3 = LinW(in_features=n_neurons, out_features=n_neurons, depth=1, layers=[self.l2])\n",
    "        self.l4 = nn.Linear(n_neurons, 10)\n",
    "        self.layer_norm = nn.LayerNorm(n_neurons)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l2(x, repr))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, depth, layers=[]):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.depth = depth\n",
    "        self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "\n",
    "    def forward(self, input, prev=[]):\n",
    "        # weight_decay = wd(prev)\n",
    "        # weight = self.weight * weight_decay.to('cuda:0')\n",
    "        # return F.linear(input, weight, self.bias)\n",
    "        # print(wd(prev).shape)\n",
    "        return F.linear(wd(prev).to(device), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 8\n",
    "BATCH_SIZE = 120\n",
    "\n",
    "# check if mps are available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "LinW layers:\n",
      "\n",
      "Depth 0: LinW(in_features=64, out_features=64, bias=True)\n",
      "Depth 1: LinW(in_features=64, out_features=64, bias=True)\n",
      "Epoch 1/8, Training Loss: 0.7540, Training Accuracy: 75.49%, Test accuracy: 82.97%\n",
      "Epoch 2/8, Training Loss: 0.5118, Training Accuracy: 83.48%, Test accuracy: 85.69%\n",
      "Epoch 3/8, Training Loss: 0.4610, Training Accuracy: 85.07%, Test accuracy: 85.93%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 156\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLinW layers:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDepth \u001b[39m\u001b[39m{\u001b[39;00mmodel[i]\u001b[39m.\u001b[39mdepth\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mmodel[i]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(model))]), sep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    155\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m--> 156\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(model, train_loader, criterion, optimizer, device)\n\u001b[1;32m    157\u001b[0m     test_accuracy \u001b[39m=\u001b[39m evaluate(device, model, test_loader)\n\u001b[1;32m    158\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mEPOCHS\u001b[39m}\u001b[39;00m\u001b[39m, Training Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Training Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%, Test accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtest_accuracy\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[31], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m images, labels \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> 9\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     10\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     11\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/mambaforge/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[42], line 105\u001b[0m, in \u001b[0;36mMLPWD.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    103\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(x)\n\u001b[1;32m    104\u001b[0m \u001b[39mrepr\u001b[39m\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m--> 105\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml2(x, \u001b[39mrepr\u001b[39;49m))\n\u001b[1;32m    106\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(x)\n\u001b[1;32m    107\u001b[0m \u001b[39mrepr\u001b[39m\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/mambaforge/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[42], line 130\u001b[0m, in \u001b[0;36mLinW.forward\u001b[0;34m(self, input, prev)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, prev\u001b[39m=\u001b[39m[]):\n\u001b[1;32m    126\u001b[0m     \u001b[39m# weight_decay = wd(prev)\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     \u001b[39m# weight = self.weight * weight_decay.to('cuda:0')\u001b[39;00m\n\u001b[1;32m    128\u001b[0m     \u001b[39m# return F.linear(input, weight, self.bias)\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     \u001b[39m# print(wd(prev).shape)\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(wd(prev)\u001b[39m.\u001b[39mto(device), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "Cell \u001b[0;32mIn[42], line 73\u001b[0m, in \u001b[0;36mwd\u001b[0;34m(layers)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwd\u001b[39m(layers: \u001b[39mlist\u001b[39m()):\n\u001b[1;32m     64\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Compute the weight decay for each layer\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \n\u001b[1;32m     66\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m get_sampled_activations(\n\u001b[1;32m     74\u001b[0m                 \u001b[39mlist\u001b[39;49m(torch\u001b[39m.\u001b[39;49mfrom_numpy(extract_activations_layers(layers))\u001b[39m.\u001b[39;49mpermute(\u001b[39m1\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39;49mnumpy()), \n\u001b[1;32m     75\u001b[0m                 bandwidth\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m\n\u001b[1;32m     76\u001b[0m             )\n",
      "Cell \u001b[0;32mIn[42], line 59\u001b[0m, in \u001b[0;36mget_sampled_activations\u001b[0;34m(activations, bandwidth)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_sampled_activations\u001b[39m(activations, bandwidth \u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39m):\n\u001b[1;32m     50\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Sample the activations using KDE\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[1;32m     52\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39m        np.array: shape (batch_size, number_of_activations)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack([\n\u001b[1;32m     60\u001b[0m                     torch\u001b[39m.\u001b[39mtensor(KernelDensity(kernel\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgaussian\u001b[39m\u001b[39m\"\u001b[39m, bandwidth\u001b[39m=\u001b[39mbandwidth)\u001b[39m.\u001b[39mfit(a)\u001b[39m.\u001b[39msample([\u001b[39m1\u001b[39m]), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m) \n\u001b[1;32m     61\u001b[0m                     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m activations], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[42], line 60\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_sampled_activations\u001b[39m(activations, bandwidth \u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39m):\n\u001b[1;32m     50\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Sample the activations using KDE\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[1;32m     52\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39m        np.array: shape (batch_size, number_of_activations)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack([\n\u001b[0;32m---> 60\u001b[0m                     torch\u001b[39m.\u001b[39mtensor(KernelDensity(kernel\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgaussian\u001b[39;49m\u001b[39m\"\u001b[39;49m, bandwidth\u001b[39m=\u001b[39;49mbandwidth)\u001b[39m.\u001b[39;49mfit(a)\u001b[39m.\u001b[39msample([\u001b[39m1\u001b[39m]), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m) \n\u001b[1;32m     61\u001b[0m                     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m activations], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/ml/lib/python3.10/site-packages/sklearn/neighbors/_kde.py:235\u001b[0m, in \u001b[0;36mKernelDensity.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m     kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 235\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtree_ \u001b[39m=\u001b[39m TREE_DICT[algorithm](\n\u001b[1;32m    236\u001b[0m     X,\n\u001b[1;32m    237\u001b[0m     metric\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetric,\n\u001b[1;32m    238\u001b[0m     leaf_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mleaf_size,\n\u001b[1;32m    239\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    240\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    241\u001b[0m )\n\u001b[1;32m    242\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "\n",
    "def extract_activations_layers(layers):\n",
    "    \"\"\" Extract for each layer the activations\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (layer_activation, batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array([np.array([np.array(h) for h in l]) for l in layers])\n",
    "\n",
    "def extract_activations_per_sample(layers, mask = False):\n",
    "    \"\"\" Extract for each sample the activations \n",
    "    for each layer and store them in a list.\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    if mask == True:\n",
    "        # mask the activations to remove zeros\n",
    "        mask = layers != 0\n",
    "        layers = [[np.array(h[m]) for h, m in zip(l,sm)] \n",
    "                for l, sm in zip(layers, mask)]\n",
    "        \n",
    "    return np.array([layers[:,i,:].flatten().reshape(-1, 1) for i in range(layers.shape[1])])\n",
    "\n",
    "\n",
    "def get_sampled_activations(activations, bandwidth = 0.2):\n",
    "    \"\"\" Sample the activations using KDE\n",
    "\n",
    "    Args:\n",
    "        activations (np.array): shape (batch_size, number_of_activations)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.stack([\n",
    "                    torch.tensor(KernelDensity(kernel=\"gaussian\", bandwidth=bandwidth).fit(a).sample([1]), dtype=torch.float32).squeeze(0) \n",
    "                    for a in activations], dim=0)\n",
    "\n",
    "def wd(layers: list()):\n",
    "    \"\"\" Compute the weight decay for each layer\n",
    "\n",
    "    Args:\n",
    "        layers (list): list of layers\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: weight decay\n",
    "\n",
    "    \"\"\"\n",
    "    return get_sampled_activations(\n",
    "                list(torch.from_numpy(extract_activations_layers(layers)).permute(1,0,2).numpy()), \n",
    "                bandwidth=0.2\n",
    "            )\n",
    "    \n",
    "    \n",
    "# N_layer = 5\n",
    "\n",
    "# prev = [torch.randn(12,64) for i in range(N_layer)]\n",
    "\n",
    "# wd(prev).shape\n",
    "\n",
    "n_neurons = 64\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(784, n_neurons)\n",
    "        self.l2 = LinW(in_features=n_neurons, out_features=n_neurons, depth=0)\n",
    "        self.l3 = LinW(in_features=n_neurons, out_features=n_neurons, depth=1, layers=[self.l2])\n",
    "        self.l4 = nn.Linear(n_neurons, 10)\n",
    "        self.layer_norm = nn.LayerNorm(n_neurons)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layers = [self.l2, self.l3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l2(x, repr))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, depth, layers=[]):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "        self.depth = depth\n",
    "        self.layers = layers[:self.depth] if len(layers)>0 else layers\n",
    "\n",
    "    def forward(self, input, prev=[]):\n",
    "        # weight_decay = wd(prev)\n",
    "        # weight = self.weight * weight_decay.to('cuda:0')\n",
    "        # return F.linear(input, weight, self.bias)\n",
    "        # print(wd(prev).shape)\n",
    "        return F.linear(wd(prev).to(device), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 8\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# check if mps are available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = 8, T_mult=1, eta_min=5e-3)\n",
    "# StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "MLPWD(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (l1): Linear(in_features=784, out_features=120, bias=True)\n",
      "  (l2): Linear(in_features=120, out_features=120, bias=True)\n",
      "  (l3): LinW(in_features=120, out_features=120, bias=True)\n",
      "  (l4): Linear(in_features=120, out_features=10, bias=True)\n",
      "  (layer_norm): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "  (gelu): GELU(approximate='none')\n",
      ")\n",
      "Epoch 1/8, Training Loss: 0.4732, Training Accuracy: 85.15%, Test accuracy: 90.38%\n",
      "Epoch 2/8, Training Loss: 0.2742, Training Accuracy: 91.38%, Test accuracy: 92.41%\n",
      "Epoch 3/8, Training Loss: 0.2385, Training Accuracy: 92.53%, Test accuracy: 92.82%\n",
      "Epoch 4/8, Training Loss: 0.2137, Training Accuracy: 93.28%, Test accuracy: 93.04%\n",
      "Epoch 5/8, Training Loss: 0.1997, Training Accuracy: 93.62%, Test accuracy: 93.70%\n",
      "Epoch 6/8, Training Loss: 0.1803, Training Accuracy: 94.31%, Test accuracy: 93.83%\n",
      "Epoch 7/8, Training Loss: 0.1697, Training Accuracy: 94.67%, Test accuracy: 93.88%\n",
      "Epoch 8/8, Training Loss: 0.1614, Training Accuracy: 94.98%, Test accuracy: 94.16%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "\n",
    "def extract_activations_layers(layers):\n",
    "    \"\"\" Extract for each layer the activations\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (layer_activation, batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array([np.array([np.array(h) for h in l]) for l in layers])\n",
    "\n",
    "def extract_activations_per_sample(layers, mask = False):\n",
    "    \"\"\" Extract for each sample the activations \n",
    "    for each layer and store them in a list.\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    if mask == True:\n",
    "        # mask the activations to remove zeros\n",
    "        mask = layers != 0\n",
    "        layers = [[np.array(h[m]) for h, m in zip(l,sm)] \n",
    "                for l, sm in zip(layers, mask)]\n",
    "        \n",
    "    return np.array([layers[:,i,:].flatten().reshape(-1, 1) for i in range(layers.shape[1])])\n",
    "\n",
    "\n",
    "def get_sampled_activations(activations, bandwidth = 0.2):\n",
    "    \"\"\" Sample the activations using KDE\n",
    "\n",
    "    Args:\n",
    "        activations (np.array): shape (batch_size, number_of_activations)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.stack([\n",
    "                    torch.tensor(KernelDensity(kernel=\"gaussian\", bandwidth=bandwidth).fit(a).sample([1]), dtype=torch.float32).squeeze(0) \n",
    "                    for a in activations], dim=0)\n",
    "\n",
    "def wd(layers: list()):\n",
    "    \"\"\" Compute the weight decay for each layer\n",
    "\n",
    "    Args:\n",
    "        layers (list): list of layers\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: weight decay\n",
    "\n",
    "    \"\"\"\n",
    "    return get_sampled_activations(\n",
    "                list(torch.from_numpy(extract_activations_layers(layers)).permute(1,0,2).numpy()), \n",
    "                bandwidth=0.2\n",
    "            )\n",
    "    \n",
    "    \n",
    "# N_layer = 5\n",
    "\n",
    "# prev = [torch.randn(12,64) for i in range(N_layer)]\n",
    "\n",
    "# wd(prev).shape\n",
    "\n",
    "n_neurons = 120\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(784, n_neurons)\n",
    "        self.l2 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.l3 = LinW(in_features=n_neurons, out_features=n_neurons)\n",
    "        self.l4 = nn.Linear(n_neurons, 10)\n",
    "        self.layer_norm = nn.LayerNorm(n_neurons)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l2(x))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "\n",
    "    def forward(self, input, prev=[]):\n",
    "        # weight_decay = wd(prev)\n",
    "        # weight = self.weight * weight_decay.to('cuda:0')\n",
    "        # return F.linear(input, weight, self.bias)\n",
    "        # print(wd(prev).shape)\n",
    "        return F.linear(wd(prev).to(device), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 8\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# check if mps are available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = 8, T_mult=1, eta_min=5e-3)\n",
    "# StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(model)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "MLPWD(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (l1): Linear(in_features=3072, out_features=256, bias=True)\n",
      "  (l2): LinW(in_features=256, out_features=256, bias=True)\n",
      "  (l3): LinW(in_features=256, out_features=256, bias=True)\n",
      "  (l4): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (gelu): GELU(approximate='none')\n",
      ")\n",
      "Epoch 1/40, Training Loss: 1.9619, Training Accuracy: 27.99%, Test accuracy: 34.12%\n",
      "Epoch 2/40, Training Loss: 1.7523, Training Accuracy: 36.41%, Test accuracy: 39.03%\n",
      "Epoch 3/40, Training Loss: 1.6934, Training Accuracy: 39.15%, Test accuracy: 40.33%\n",
      "Epoch 4/40, Training Loss: 1.6638, Training Accuracy: 40.22%, Test accuracy: 40.92%\n",
      "Epoch 5/40, Training Loss: 1.6374, Training Accuracy: 40.95%, Test accuracy: 41.85%\n",
      "Epoch 6/40, Training Loss: 1.6202, Training Accuracy: 41.79%, Test accuracy: 41.31%\n",
      "Epoch 7/40, Training Loss: 1.6007, Training Accuracy: 42.45%, Test accuracy: 43.04%\n",
      "Epoch 8/40, Training Loss: 1.5775, Training Accuracy: 43.20%, Test accuracy: 44.07%\n",
      "Epoch 9/40, Training Loss: 1.4590, Training Accuracy: 47.57%, Test accuracy: 47.15%\n",
      "Epoch 10/40, Training Loss: 1.4325, Training Accuracy: 48.53%, Test accuracy: 47.46%\n",
      "Epoch 11/40, Training Loss: 1.4411, Training Accuracy: 48.07%, Test accuracy: 47.38%\n",
      "Epoch 12/40, Training Loss: 1.4517, Training Accuracy: 47.62%, Test accuracy: 47.23%\n",
      "Epoch 13/40, Training Loss: 1.4738, Training Accuracy: 46.75%, Test accuracy: 45.69%\n",
      "Epoch 14/40, Training Loss: 1.4911, Training Accuracy: 46.31%, Test accuracy: 46.27%\n",
      "Epoch 15/40, Training Loss: 1.4848, Training Accuracy: 46.49%, Test accuracy: 46.51%\n",
      "Epoch 16/40, Training Loss: 1.4737, Training Accuracy: 47.16%, Test accuracy: 45.12%\n",
      "Epoch 17/40, Training Loss: 1.3752, Training Accuracy: 50.42%, Test accuracy: 48.64%\n",
      "Epoch 18/40, Training Loss: 1.3453, Training Accuracy: 51.39%, Test accuracy: 49.39%\n",
      "Epoch 19/40, Training Loss: 1.3450, Training Accuracy: 51.42%, Test accuracy: 48.38%\n",
      "Epoch 20/40, Training Loss: 1.3753, Training Accuracy: 50.50%, Test accuracy: 46.69%\n",
      "Epoch 21/40, Training Loss: 1.3834, Training Accuracy: 50.18%, Test accuracy: 46.36%\n",
      "Epoch 22/40, Training Loss: 1.4127, Training Accuracy: 49.07%, Test accuracy: 46.56%\n",
      "Epoch 23/40, Training Loss: 1.4221, Training Accuracy: 48.74%, Test accuracy: 46.63%\n",
      "Epoch 24/40, Training Loss: 1.4180, Training Accuracy: 49.05%, Test accuracy: 44.97%\n",
      "Epoch 25/40, Training Loss: 1.3028, Training Accuracy: 52.98%, Test accuracy: 49.90%\n",
      "Epoch 26/40, Training Loss: 1.2767, Training Accuracy: 54.36%, Test accuracy: 50.37%\n",
      "Epoch 27/40, Training Loss: 1.2837, Training Accuracy: 53.81%, Test accuracy: 50.17%\n",
      "Epoch 28/40, Training Loss: 1.3052, Training Accuracy: 53.12%, Test accuracy: 48.61%\n",
      "Epoch 29/40, Training Loss: 1.3330, Training Accuracy: 51.68%, Test accuracy: 47.72%\n",
      "Epoch 30/40, Training Loss: 1.3423, Training Accuracy: 51.66%, Test accuracy: 48.99%\n",
      "Epoch 31/40, Training Loss: 1.3690, Training Accuracy: 50.75%, Test accuracy: 48.25%\n",
      "Epoch 32/40, Training Loss: 1.3642, Training Accuracy: 50.68%, Test accuracy: 45.37%\n",
      "Epoch 33/40, Training Loss: 1.2532, Training Accuracy: 54.66%, Test accuracy: 50.61%\n",
      "Epoch 34/40, Training Loss: 1.2250, Training Accuracy: 55.65%, Test accuracy: 50.14%\n",
      "Epoch 35/40, Training Loss: 1.2378, Training Accuracy: 55.35%, Test accuracy: 50.25%\n",
      "Epoch 36/40, Training Loss: 1.2550, Training Accuracy: 54.95%, Test accuracy: 50.03%\n",
      "Epoch 37/40, Training Loss: 1.2814, Training Accuracy: 53.83%, Test accuracy: 46.11%\n",
      "Epoch 38/40, Training Loss: 1.3209, Training Accuracy: 52.53%, Test accuracy: 47.77%\n",
      "Epoch 39/40, Training Loss: 1.3253, Training Accuracy: 52.07%, Test accuracy: 48.17%\n",
      "Epoch 40/40, Training Loss: 1.3272, Training Accuracy: 52.23%, Test accuracy: 49.08%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "\n",
    "def extract_activations_layers(layers):\n",
    "    \"\"\" Extract for each layer the activations\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (layer_activation, batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array([np.array([np.array(h) for h in l]) for l in layers])\n",
    "\n",
    "def extract_activations_per_sample(layers, mask = False):\n",
    "    \"\"\" Extract for each sample the activations \n",
    "    for each layer and store them in a list.\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    if mask == True:\n",
    "        # mask the activations to remove zeros\n",
    "        mask = layers != 0\n",
    "        layers = [[np.array(h[m]) for h, m in zip(l,sm)] \n",
    "                for l, sm in zip(layers, mask)]\n",
    "        \n",
    "    return np.array([layers[:,i,:].flatten().reshape(-1, 1) for i in range(layers.shape[1])])\n",
    "\n",
    "\n",
    "def get_sampled_activations(activations, bandwidth = 0.2):\n",
    "    \"\"\" Sample the activations using KDE\n",
    "\n",
    "    Args:\n",
    "        activations (np.array): shape (batch_size, number_of_activations)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.stack([\n",
    "                    torch.tensor(KernelDensity(kernel=\"gaussian\", bandwidth=bandwidth).fit(a).sample([1]), dtype=torch.float32).squeeze(0) \n",
    "                    for a in activations], dim=0)\n",
    "\n",
    "def wd(layers: list()):\n",
    "    \"\"\" Compute the weight decay for each layer\n",
    "\n",
    "    Args:\n",
    "        layers (list): list of layers\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: weight decay\n",
    "\n",
    "    \"\"\"\n",
    "    return get_sampled_activations(\n",
    "                list(torch.from_numpy(extract_activations_layers(layers)).permute(1,0,2).numpy()), \n",
    "                bandwidth=0.2\n",
    "            )\n",
    "    \n",
    "    \n",
    "# N_layer = 5\n",
    "\n",
    "# prev = [torch.randn(12,64) for i in range(N_layer)]\n",
    "\n",
    "# wd(prev).shape\n",
    "\n",
    "n_neurons = 256\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, n_neurons)\n",
    "        self.l2 = LinW(in_features=n_neurons, out_features=n_neurons)\n",
    "        self.l3 = LinW(in_features=n_neurons, out_features=n_neurons)\n",
    "        self.l4 = nn.Linear(n_neurons, 10)\n",
    "        self.layer_norm = nn.LayerNorm(n_neurons)\n",
    "        self.gelu = nn.GELU()\n",
    "        # self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l2(x, repr))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.l4(x)\n",
    "        # x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "\n",
    "    def forward(self, input, prev=[]):\n",
    "        # weight_decay = wd(prev)\n",
    "        # weight = self.weight * weight_decay.to('cuda:0')\n",
    "        # return F.linear(input, weight, self.bias)\n",
    "        # print(wd(prev).shape)\n",
    "        return F.linear(input * wd(prev).to(device), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# check if mps are available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = 8, T_mult=1, eta_min=5e-3)\n",
    "# StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(model)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "MLPWD(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (l1): Linear(in_features=3072, out_features=120, bias=True)\n",
      "  (l2): Linear(in_features=120, out_features=120, bias=True)\n",
      "  (l3): LinW(in_features=120, out_features=120, bias=True)\n",
      "  (l4): Linear(in_features=120, out_features=120, bias=True)\n",
      "  (l5): LinW(in_features=120, out_features=120, bias=True)\n",
      "  (l6): Linear(in_features=120, out_features=10, bias=True)\n",
      "  (layer_norm): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "  (gelu): GELU(approximate='none')\n",
      ")\n",
      "Epoch 1/50, Training Loss: 2.1340, Training Accuracy: 21.17%, Test accuracy: 25.31%\n",
      "Epoch 2/50, Training Loss: 2.0068, Training Accuracy: 27.19%, Test accuracy: 28.33%\n",
      "Epoch 3/50, Training Loss: 1.9659, Training Accuracy: 28.54%, Test accuracy: 29.18%\n",
      "Epoch 4/50, Training Loss: 1.9438, Training Accuracy: 29.51%, Test accuracy: 29.77%\n",
      "Epoch 5/50, Training Loss: 1.9298, Training Accuracy: 30.06%, Test accuracy: 29.66%\n",
      "Epoch 6/50, Training Loss: 1.9259, Training Accuracy: 29.96%, Test accuracy: 29.06%\n",
      "Epoch 7/50, Training Loss: 1.9176, Training Accuracy: 30.16%, Test accuracy: 30.49%\n",
      "Epoch 8/50, Training Loss: 1.9132, Training Accuracy: 30.75%, Test accuracy: 30.19%\n",
      "Epoch 9/50, Training Loss: 1.8742, Training Accuracy: 32.19%, Test accuracy: 32.36%\n",
      "Epoch 10/50, Training Loss: 1.8644, Training Accuracy: 32.64%, Test accuracy: 33.33%\n",
      "Epoch 11/50, Training Loss: 1.8653, Training Accuracy: 32.61%, Test accuracy: 33.26%\n",
      "Epoch 12/50, Training Loss: 1.8693, Training Accuracy: 32.34%, Test accuracy: 32.00%\n",
      "Epoch 13/50, Training Loss: 1.8834, Training Accuracy: 31.70%, Test accuracy: 31.62%\n",
      "Epoch 14/50, Training Loss: 1.8883, Training Accuracy: 31.41%, Test accuracy: 31.67%\n",
      "Epoch 15/50, Training Loss: 1.8970, Training Accuracy: 31.47%, Test accuracy: 32.14%\n",
      "Epoch 16/50, Training Loss: 1.8994, Training Accuracy: 31.15%, Test accuracy: 31.20%\n",
      "Epoch 17/50, Training Loss: 1.8709, Training Accuracy: 32.25%, Test accuracy: 32.47%\n",
      "Epoch 18/50, Training Loss: 1.8639, Training Accuracy: 32.38%, Test accuracy: 32.53%\n",
      "Epoch 19/50, Training Loss: 1.8667, Training Accuracy: 32.61%, Test accuracy: 32.41%\n",
      "Epoch 20/50, Training Loss: 1.8678, Training Accuracy: 32.52%, Test accuracy: 31.60%\n",
      "Epoch 21/50, Training Loss: 1.8825, Training Accuracy: 31.95%, Test accuracy: 32.16%\n",
      "Epoch 22/50, Training Loss: 1.8898, Training Accuracy: 31.70%, Test accuracy: 31.23%\n",
      "Epoch 23/50, Training Loss: 1.8957, Training Accuracy: 31.46%, Test accuracy: 30.78%\n",
      "Epoch 24/50, Training Loss: 1.9096, Training Accuracy: 30.66%, Test accuracy: 30.30%\n",
      "Epoch 25/50, Training Loss: 1.8812, Training Accuracy: 31.69%, Test accuracy: 32.25%\n",
      "Epoch 26/50, Training Loss: 1.8741, Training Accuracy: 32.23%, Test accuracy: 31.92%\n",
      "Epoch 27/50, Training Loss: 1.8741, Training Accuracy: 32.34%, Test accuracy: 31.91%\n",
      "Epoch 28/50, Training Loss: 1.8837, Training Accuracy: 31.61%, Test accuracy: 31.10%\n",
      "Epoch 29/50, Training Loss: 1.8908, Training Accuracy: 31.70%, Test accuracy: 31.32%\n",
      "Epoch 30/50, Training Loss: 1.8997, Training Accuracy: 31.30%, Test accuracy: 30.31%\n",
      "Epoch 31/50, Training Loss: 1.9163, Training Accuracy: 30.48%, Test accuracy: 30.43%\n",
      "Epoch 32/50, Training Loss: 1.9178, Training Accuracy: 30.21%, Test accuracy: 30.38%\n",
      "Epoch 33/50, Training Loss: 1.8949, Training Accuracy: 31.72%, Test accuracy: 31.38%\n",
      "Epoch 34/50, Training Loss: 1.8848, Training Accuracy: 31.94%, Test accuracy: 31.96%\n",
      "Epoch 35/50, Training Loss: 1.8845, Training Accuracy: 31.82%, Test accuracy: 31.85%\n",
      "Epoch 36/50, Training Loss: 1.8921, Training Accuracy: 31.11%, Test accuracy: 31.53%\n",
      "Epoch 37/50, Training Loss: 1.9051, Training Accuracy: 30.88%, Test accuracy: 31.19%\n",
      "Epoch 38/50, Training Loss: 1.9130, Training Accuracy: 30.66%, Test accuracy: 30.09%\n",
      "Epoch 39/50, Training Loss: 1.9201, Training Accuracy: 30.36%, Test accuracy: 30.38%\n",
      "Epoch 40/50, Training Loss: 1.9264, Training Accuracy: 30.21%, Test accuracy: 30.20%\n",
      "Epoch 41/50, Training Loss: 1.9055, Training Accuracy: 31.03%, Test accuracy: 31.11%\n",
      "Epoch 42/50, Training Loss: 1.9034, Training Accuracy: 31.15%, Test accuracy: 31.09%\n",
      "Epoch 43/50, Training Loss: 1.9011, Training Accuracy: 30.76%, Test accuracy: 30.37%\n",
      "Epoch 44/50, Training Loss: 1.9091, Training Accuracy: 30.87%, Test accuracy: 30.22%\n",
      "Epoch 45/50, Training Loss: 1.9151, Training Accuracy: 30.52%, Test accuracy: 30.00%\n",
      "Epoch 46/50, Training Loss: 1.9250, Training Accuracy: 30.20%, Test accuracy: 30.74%\n",
      "Epoch 47/50, Training Loss: 1.9393, Training Accuracy: 29.70%, Test accuracy: 29.18%\n",
      "Epoch 48/50, Training Loss: 1.9422, Training Accuracy: 29.57%, Test accuracy: 29.85%\n",
      "Epoch 49/50, Training Loss: 1.9235, Training Accuracy: 30.01%, Test accuracy: 30.38%\n",
      "Epoch 50/50, Training Loss: 1.9160, Training Accuracy: 30.43%, Test accuracy: 30.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "\n",
    "def extract_activations_layers(layers):\n",
    "    \"\"\" Extract for each layer the activations\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (layer_activation, batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array([np.array([np.array(h) for h in l]) for l in layers])\n",
    "\n",
    "def extract_activations_per_sample(layers, mask = False):\n",
    "    \"\"\" Extract for each sample the activations \n",
    "    for each layer and store them in a list.\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    if mask == True:\n",
    "        # mask the activations to remove zeros\n",
    "        mask = layers != 0\n",
    "        layers = [[np.array(h[m]) for h, m in zip(l,sm)] \n",
    "                for l, sm in zip(layers, mask)]\n",
    "        \n",
    "    return np.array([layers[:,i,:].flatten().reshape(-1, 1) for i in range(layers.shape[1])])\n",
    "\n",
    "\n",
    "def get_sampled_activations(activations, bandwidth = 0.2):\n",
    "    \"\"\" Sample the activations using KDE\n",
    "\n",
    "    Args:\n",
    "        activations (np.array): shape (batch_size, number_of_activations)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.stack([\n",
    "                    torch.tensor(KernelDensity(kernel=\"gaussian\", bandwidth=bandwidth).fit(a).sample([1]), dtype=torch.float32).squeeze(0) \n",
    "                    for a in activations], dim=0)\n",
    "\n",
    "def wd(layers: list()):\n",
    "    \"\"\" Compute the weight decay for each layer\n",
    "\n",
    "    Args:\n",
    "        layers (list): list of layers\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: weight decay\n",
    "\n",
    "    \"\"\"\n",
    "    return get_sampled_activations(\n",
    "                list(torch.from_numpy(extract_activations_layers(layers)).permute(1,0,2).numpy()), \n",
    "                bandwidth=0.2\n",
    "            )\n",
    "    \n",
    "    \n",
    "# N_layer = 5\n",
    "# prev = [torch.randn(12,64) for i in range(N_layer)]\n",
    "# wd(prev).shape\n",
    "\n",
    "n_neurons = 120\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, n_neurons)\n",
    "        self.l2 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.l3 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.l4 = nn.Linear(n_neurons, n_neurons)\n",
    "        # self.l2 = LinW(in_features=n_neurons, out_features=n_neurons)\n",
    "        self.l3 = LinW(in_features=n_neurons, out_features=n_neurons)\n",
    "        self.l5 = LinW(in_features=n_neurons, out_features=n_neurons)\n",
    "        self.l6 = nn.Linear(n_neurons, 10)\n",
    "        self.layer_norm = nn.LayerNorm(n_neurons)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l2(x))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l4(x))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l5(x, repr))\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.l6(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "\n",
    "    def forward(self, input, prev=[]):\n",
    "        # weight_decay = wd(prev)\n",
    "        # weight = self.weight * weight_decay.to('cuda:0')\n",
    "        # return F.linear(input, weight, self.bias)\n",
    "        # print(wd(prev).shape)\n",
    "        return F.linear(wd(prev).to(device), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# check if mps are available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = 8, T_mult=1, eta_min=5e-3)\n",
    "# StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(model)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "MLPWD(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (l1): Linear(in_features=3072, out_features=120, bias=True)\n",
      "  (l2): Linear(in_features=120, out_features=120, bias=True)\n",
      "  (l3): LinW(in_features=120, out_features=120, bias=True)\n",
      "  (l4): Linear(in_features=120, out_features=120, bias=True)\n",
      "  (l5): LinW(in_features=120, out_features=120, bias=True)\n",
      "  (l6): Linear(in_features=120, out_features=10, bias=True)\n",
      "  (layer_norm): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "  (gelu): GELU(approximate='none')\n",
      ")\n",
      "Epoch 1/50, Training Loss: 2.0278, Training Accuracy: 24.53%, Test accuracy: 33.27%\n",
      "Epoch 2/50, Training Loss: 1.8030, Training Accuracy: 34.68%, Test accuracy: 36.84%\n",
      "Epoch 3/50, Training Loss: 1.7368, Training Accuracy: 37.09%, Test accuracy: 39.38%\n",
      "Epoch 4/50, Training Loss: 1.6867, Training Accuracy: 39.54%, Test accuracy: 38.17%\n",
      "Epoch 5/50, Training Loss: 1.6643, Training Accuracy: 40.02%, Test accuracy: 43.60%\n",
      "Epoch 6/50, Training Loss: 1.6201, Training Accuracy: 41.79%, Test accuracy: 43.09%\n",
      "Epoch 7/50, Training Loss: 1.6071, Training Accuracy: 42.40%, Test accuracy: 42.78%\n",
      "Epoch 8/50, Training Loss: 1.5716, Training Accuracy: 43.65%, Test accuracy: 42.71%\n",
      "Epoch 9/50, Training Loss: 1.4549, Training Accuracy: 47.89%, Test accuracy: 46.97%\n",
      "Epoch 10/50, Training Loss: 1.4288, Training Accuracy: 48.75%, Test accuracy: 48.25%\n",
      "Epoch 11/50, Training Loss: 1.4260, Training Accuracy: 48.94%, Test accuracy: 46.96%\n",
      "Epoch 12/50, Training Loss: 1.4393, Training Accuracy: 48.24%, Test accuracy: 46.74%\n",
      "Epoch 13/50, Training Loss: 1.4562, Training Accuracy: 47.65%, Test accuracy: 45.66%\n",
      "Epoch 14/50, Training Loss: 1.4618, Training Accuracy: 47.85%, Test accuracy: 46.83%\n",
      "Epoch 15/50, Training Loss: 1.4663, Training Accuracy: 47.44%, Test accuracy: 45.69%\n",
      "Epoch 16/50, Training Loss: 1.4645, Training Accuracy: 47.62%, Test accuracy: 46.42%\n",
      "Epoch 17/50, Training Loss: 1.3324, Training Accuracy: 52.38%, Test accuracy: 49.57%\n",
      "Epoch 18/50, Training Loss: 1.3080, Training Accuracy: 53.17%, Test accuracy: 48.51%\n",
      "Epoch 19/50, Training Loss: 1.3149, Training Accuracy: 52.89%, Test accuracy: 48.43%\n",
      "Epoch 20/50, Training Loss: 1.3394, Training Accuracy: 52.14%, Test accuracy: 47.34%\n",
      "Epoch 21/50, Training Loss: 1.3430, Training Accuracy: 51.81%, Test accuracy: 48.64%\n",
      "Epoch 22/50, Training Loss: 1.3704, Training Accuracy: 50.86%, Test accuracy: 47.89%\n",
      "Epoch 23/50, Training Loss: 1.3809, Training Accuracy: 50.40%, Test accuracy: 47.79%\n",
      "Epoch 24/50, Training Loss: 1.3903, Training Accuracy: 50.22%, Test accuracy: 47.97%\n",
      "Epoch 25/50, Training Loss: 1.2381, Training Accuracy: 55.94%, Test accuracy: 50.15%\n",
      "Epoch 26/50, Training Loss: 1.2165, Training Accuracy: 56.52%, Test accuracy: 50.49%\n",
      "Epoch 27/50, Training Loss: 1.2189, Training Accuracy: 56.30%, Test accuracy: 50.25%\n",
      "Epoch 28/50, Training Loss: 1.2380, Training Accuracy: 55.74%, Test accuracy: 50.21%\n",
      "Epoch 29/50, Training Loss: 1.2549, Training Accuracy: 55.09%, Test accuracy: 48.86%\n",
      "Epoch 30/50, Training Loss: 1.2927, Training Accuracy: 53.85%, Test accuracy: 47.20%\n",
      "Epoch 31/50, Training Loss: 1.3149, Training Accuracy: 52.97%, Test accuracy: 48.82%\n",
      "Epoch 32/50, Training Loss: 1.3064, Training Accuracy: 53.24%, Test accuracy: 49.57%\n",
      "Epoch 33/50, Training Loss: 1.1578, Training Accuracy: 58.69%, Test accuracy: 51.90%\n",
      "Epoch 34/50, Training Loss: 1.1355, Training Accuracy: 59.39%, Test accuracy: 51.52%\n",
      "Epoch 35/50, Training Loss: 1.1422, Training Accuracy: 59.02%, Test accuracy: 49.96%\n",
      "Epoch 36/50, Training Loss: 1.1669, Training Accuracy: 58.22%, Test accuracy: 50.07%\n",
      "Epoch 37/50, Training Loss: 1.2051, Training Accuracy: 56.83%, Test accuracy: 50.09%\n",
      "Epoch 38/50, Training Loss: 1.2217, Training Accuracy: 56.18%, Test accuracy: 49.00%\n",
      "Epoch 39/50, Training Loss: 1.2345, Training Accuracy: 55.72%, Test accuracy: 49.70%\n",
      "Epoch 40/50, Training Loss: 1.2489, Training Accuracy: 55.39%, Test accuracy: 46.63%\n",
      "Epoch 41/50, Training Loss: 1.0972, Training Accuracy: 60.83%, Test accuracy: 52.18%\n",
      "Epoch 42/50, Training Loss: 1.0641, Training Accuracy: 61.83%, Test accuracy: 51.40%\n",
      "Epoch 43/50, Training Loss: 1.0771, Training Accuracy: 61.51%, Test accuracy: 50.91%\n",
      "Epoch 44/50, Training Loss: 1.1052, Training Accuracy: 60.40%, Test accuracy: 50.62%\n",
      "Epoch 45/50, Training Loss: 1.1306, Training Accuracy: 59.54%, Test accuracy: 46.63%\n",
      "Epoch 46/50, Training Loss: 1.1733, Training Accuracy: 57.87%, Test accuracy: 50.52%\n",
      "Epoch 47/50, Training Loss: 1.1838, Training Accuracy: 57.78%, Test accuracy: 48.30%\n",
      "Epoch 48/50, Training Loss: 1.2004, Training Accuracy: 56.97%, Test accuracy: 49.51%\n",
      "Epoch 49/50, Training Loss: 1.0389, Training Accuracy: 62.61%, Test accuracy: 51.22%\n",
      "Epoch 50/50, Training Loss: 1.0074, Training Accuracy: 63.98%, Test accuracy: 51.70%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "\n",
    "def extract_activations_layers(layers):\n",
    "    \"\"\" Extract for each layer the activations\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (layer_activation, batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array([np.array([np.array(h) for h in l]) for l in layers])\n",
    "\n",
    "def extract_activations_per_sample(layers, mask = False):\n",
    "    \"\"\" Extract for each sample the activations \n",
    "    for each layer and store them in a list.\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    if mask == True:\n",
    "        # mask the activations to remove zeros\n",
    "        mask = layers != 0\n",
    "        layers = [[np.array(h[m]) for h, m in zip(l,sm)] \n",
    "                for l, sm in zip(layers, mask)]\n",
    "        \n",
    "    return np.array([layers[:,i,:].flatten().reshape(-1, 1) for i in range(layers.shape[1])])\n",
    "\n",
    "\n",
    "def get_sampled_activations(activations, bandwidth = 0.2):\n",
    "    \"\"\" Sample the activations using KDE\n",
    "\n",
    "    Args:\n",
    "        activations (np.array): shape (batch_size, number_of_activations)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.stack([\n",
    "                    torch.tensor(KernelDensity(kernel=\"gaussian\", bandwidth=bandwidth).fit(a).sample([1]), dtype=torch.float32).squeeze(0) \n",
    "                    for a in activations], dim=0)\n",
    "\n",
    "def wd(layers: list()):\n",
    "    \"\"\" Compute the weight decay for each layer\n",
    "\n",
    "    Args:\n",
    "        layers (list): list of layers\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: weight decay\n",
    "\n",
    "    \"\"\"\n",
    "    return get_sampled_activations(\n",
    "                list(torch.from_numpy(extract_activations_layers(layers)).permute(1,0,2).numpy()), \n",
    "                bandwidth=0.2\n",
    "            )\n",
    "    \n",
    "    \n",
    "# N_layer = 5\n",
    "# prev = [torch.randn(12,64) for i in range(N_layer)]\n",
    "# wd(prev).shape\n",
    "\n",
    "n_neurons = 120\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, n_neurons)\n",
    "        self.l2 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.l3 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.l4 = nn.Linear(n_neurons, n_neurons)\n",
    "        # self.l2 = LinW(in_features=n_neurons, out_features=n_neurons)\n",
    "        self.l3 = LinW(in_features=n_neurons, out_features=n_neurons)\n",
    "        self.l5 = LinW(in_features=n_neurons, out_features=n_neurons)\n",
    "        self.l6 = nn.Linear(n_neurons, 10)\n",
    "        self.layer_norm = nn.LayerNorm(n_neurons)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l2(x))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l4(x))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l5(x, repr))\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.l6(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "\n",
    "    def forward(self, input, prev=[]):\n",
    "        # weight_decay = wd(prev)\n",
    "        # weight = self.weight * weight_decay.to('cuda:0')\n",
    "        # return F.linear(input, weight, self.bias)\n",
    "        # print(wd(prev).shape)\n",
    "        return F.linear(input + wd(prev).to(device), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# check if mps are available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = 8, T_mult=1, eta_min=5e-3)\n",
    "# StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(model)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "MLPWD(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (l1): Linear(in_features=3072, out_features=120, bias=True)\n",
      "  (l2): Linear(in_features=120, out_features=120, bias=True)\n",
      "  (l3): LinW(in_features=120, out_features=120, bias=True)\n",
      "  (l4): Linear(in_features=120, out_features=120, bias=True)\n",
      "  (l5): LinW(in_features=120, out_features=120, bias=True)\n",
      "  (l6): Linear(in_features=120, out_features=10, bias=True)\n",
      "  (layer_norm): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "  (gelu): GELU(approximate='none')\n",
      ")\n",
      "Epoch 1/50, Training Loss: 2.1603, Training Accuracy: 17.04%, Test accuracy: 24.12%\n",
      "Epoch 2/50, Training Loss: 1.9425, Training Accuracy: 28.18%, Test accuracy: 29.56%\n",
      "Epoch 3/50, Training Loss: 1.8612, Training Accuracy: 32.12%, Test accuracy: 33.63%\n",
      "Epoch 4/50, Training Loss: 1.8200, Training Accuracy: 34.00%, Test accuracy: 35.00%\n",
      "Epoch 5/50, Training Loss: 1.7953, Training Accuracy: 34.99%, Test accuracy: 35.46%\n",
      "Epoch 6/50, Training Loss: 1.7740, Training Accuracy: 35.69%, Test accuracy: 35.75%\n",
      "Epoch 7/50, Training Loss: 1.7523, Training Accuracy: 36.79%, Test accuracy: 37.97%\n",
      "Epoch 8/50, Training Loss: 1.7340, Training Accuracy: 37.54%, Test accuracy: 35.54%\n",
      "Epoch 9/50, Training Loss: 1.6297, Training Accuracy: 41.28%, Test accuracy: 41.34%\n",
      "Epoch 10/50, Training Loss: 1.6003, Training Accuracy: 42.20%, Test accuracy: 41.99%\n",
      "Epoch 11/50, Training Loss: 1.6041, Training Accuracy: 42.06%, Test accuracy: 42.69%\n",
      "Epoch 12/50, Training Loss: 1.6165, Training Accuracy: 41.67%, Test accuracy: 42.44%\n",
      "Epoch 13/50, Training Loss: 1.6324, Training Accuracy: 41.05%, Test accuracy: 40.89%\n",
      "Epoch 14/50, Training Loss: 1.6354, Training Accuracy: 41.25%, Test accuracy: 39.19%\n",
      "Epoch 15/50, Training Loss: 1.6335, Training Accuracy: 40.99%, Test accuracy: 40.84%\n",
      "Epoch 16/50, Training Loss: 1.6263, Training Accuracy: 41.01%, Test accuracy: 41.51%\n",
      "Epoch 17/50, Training Loss: 1.5252, Training Accuracy: 45.22%, Test accuracy: 44.68%\n",
      "Epoch 18/50, Training Loss: 1.5060, Training Accuracy: 45.97%, Test accuracy: 45.20%\n",
      "Epoch 19/50, Training Loss: 1.5105, Training Accuracy: 45.81%, Test accuracy: 45.36%\n",
      "Epoch 20/50, Training Loss: 1.5246, Training Accuracy: 45.16%, Test accuracy: 44.24%\n",
      "Epoch 21/50, Training Loss: 1.5581, Training Accuracy: 43.99%, Test accuracy: 42.19%\n",
      "Epoch 22/50, Training Loss: 1.5629, Training Accuracy: 43.50%, Test accuracy: 42.72%\n",
      "Epoch 23/50, Training Loss: 1.5841, Training Accuracy: 43.10%, Test accuracy: 41.84%\n",
      "Epoch 24/50, Training Loss: 1.5752, Training Accuracy: 43.50%, Test accuracy: 39.03%\n",
      "Epoch 25/50, Training Loss: 1.4832, Training Accuracy: 46.77%, Test accuracy: 46.31%\n",
      "Epoch 26/50, Training Loss: 1.4538, Training Accuracy: 47.83%, Test accuracy: 46.25%\n",
      "Epoch 27/50, Training Loss: 1.4547, Training Accuracy: 47.76%, Test accuracy: 45.41%\n",
      "Epoch 28/50, Training Loss: 1.4707, Training Accuracy: 46.93%, Test accuracy: 45.51%\n",
      "Epoch 29/50, Training Loss: 1.4919, Training Accuracy: 46.42%, Test accuracy: 45.01%\n",
      "Epoch 30/50, Training Loss: 1.5090, Training Accuracy: 45.90%, Test accuracy: 43.62%\n",
      "Epoch 31/50, Training Loss: 1.5490, Training Accuracy: 44.05%, Test accuracy: 44.08%\n",
      "Epoch 32/50, Training Loss: 1.5343, Training Accuracy: 44.78%, Test accuracy: 45.35%\n",
      "Epoch 33/50, Training Loss: 1.4295, Training Accuracy: 48.99%, Test accuracy: 47.27%\n",
      "Epoch 34/50, Training Loss: 1.4147, Training Accuracy: 49.17%, Test accuracy: 47.49%\n",
      "Epoch 35/50, Training Loss: 1.4202, Training Accuracy: 49.06%, Test accuracy: 46.77%\n",
      "Epoch 36/50, Training Loss: 1.4441, Training Accuracy: 48.23%, Test accuracy: 46.27%\n",
      "Epoch 37/50, Training Loss: 1.4628, Training Accuracy: 47.47%, Test accuracy: 44.96%\n",
      "Epoch 38/50, Training Loss: 1.4819, Training Accuracy: 46.84%, Test accuracy: 44.20%\n",
      "Epoch 39/50, Training Loss: 1.4877, Training Accuracy: 46.62%, Test accuracy: 42.94%\n",
      "Epoch 40/50, Training Loss: 1.5137, Training Accuracy: 45.95%, Test accuracy: 45.07%\n",
      "Epoch 41/50, Training Loss: 1.3986, Training Accuracy: 49.93%, Test accuracy: 47.80%\n",
      "Epoch 42/50, Training Loss: 1.3863, Training Accuracy: 50.30%, Test accuracy: 47.61%\n",
      "Epoch 43/50, Training Loss: 1.3936, Training Accuracy: 50.10%, Test accuracy: 47.75%\n",
      "Epoch 44/50, Training Loss: 1.4094, Training Accuracy: 49.50%, Test accuracy: 46.75%\n",
      "Epoch 45/50, Training Loss: 1.4381, Training Accuracy: 48.36%, Test accuracy: 45.83%\n",
      "Epoch 46/50, Training Loss: 1.4477, Training Accuracy: 48.16%, Test accuracy: 45.92%\n",
      "Epoch 47/50, Training Loss: 1.4606, Training Accuracy: 47.75%, Test accuracy: 46.10%\n",
      "Epoch 48/50, Training Loss: 1.4720, Training Accuracy: 47.38%, Test accuracy: 45.84%\n",
      "Epoch 49/50, Training Loss: 1.3717, Training Accuracy: 50.79%, Test accuracy: 48.00%\n",
      "Epoch 50/50, Training Loss: 1.3580, Training Accuracy: 51.54%, Test accuracy: 48.37%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "\n",
    "def extract_activations_layers(layers):\n",
    "    \"\"\" Extract for each layer the activations\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (layer_activation, batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array([np.array([np.array(h) for h in l]) for l in layers])\n",
    "\n",
    "def extract_activations_per_sample(layers, mask = False):\n",
    "    \"\"\" Extract for each sample the activations \n",
    "    for each layer and store them in a list.\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    if mask == True:\n",
    "        # mask the activations to remove zeros\n",
    "        mask = layers != 0\n",
    "        layers = [[np.array(h[m]) for h, m in zip(l,sm)] \n",
    "                for l, sm in zip(layers, mask)]\n",
    "        \n",
    "    return np.array([layers[:,i,:].flatten().reshape(-1, 1) for i in range(layers.shape[1])])\n",
    "\n",
    "\n",
    "def get_sampled_activations(activations, bandwidth = 0.2):\n",
    "    \"\"\" Sample the activations using KDE\n",
    "\n",
    "    Args:\n",
    "        activations (np.array): shape (batch_size, number_of_activations)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.stack([\n",
    "                    torch.tensor(KernelDensity(kernel=\"gaussian\", bandwidth=bandwidth).fit(a).sample([1]), dtype=torch.float32).squeeze(0) \n",
    "                    for a in activations], dim=0)\n",
    "\n",
    "def wd(layers: list()):\n",
    "    \"\"\" Compute the weight decay for each layer\n",
    "\n",
    "    Args:\n",
    "        layers (list): list of layers\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: weight decay\n",
    "\n",
    "    \"\"\"\n",
    "    return get_sampled_activations(\n",
    "                list(torch.from_numpy(extract_activations_layers(layers)).permute(1,0,2).numpy()), \n",
    "                bandwidth=0.2\n",
    "            )\n",
    "    \n",
    "    \n",
    "# N_layer = 5\n",
    "# prev = [torch.randn(12,64) for i in range(N_layer)]\n",
    "# wd(prev).shape\n",
    "\n",
    "n_neurons = 120\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, n_neurons)\n",
    "        self.l2 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.l3 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.l4 = nn.Linear(n_neurons, n_neurons)\n",
    "        # self.l2 = LinW(in_features=n_neurons, out_features=n_neurons)\n",
    "        self.l3 = LinW(in_features=n_neurons, out_features=n_neurons)\n",
    "        self.l5 = LinW(in_features=n_neurons, out_features=n_neurons)\n",
    "        self.l6 = nn.Linear(n_neurons, 10)\n",
    "        self.layer_norm = nn.LayerNorm(n_neurons)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l2(x))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l4(x))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l5(x, repr))\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.l6(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "\n",
    "    def forward(self, input, prev=[]):\n",
    "        # weight_decay = wd(prev)\n",
    "        # weight = self.weight * weight_decay.to('cuda:0')\n",
    "        # return F.linear(input, weight, self.bias)\n",
    "        # print(wd(prev).shape)\n",
    "        return F.linear(input * wd(prev).to(device), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# check if mps are available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = 8, T_mult=1, eta_min=5e-3)\n",
    "# StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(model)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "MLPWD(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (l1): Linear(in_features=3072, out_features=120, bias=True)\n",
      "  (l2): Linear(in_features=120, out_features=120, bias=True)\n",
      "  (l3): Linear(in_features=120, out_features=120, bias=True)\n",
      "  (l4): Linear(in_features=120, out_features=120, bias=True)\n",
      "  (l6): Linear(in_features=120, out_features=10, bias=True)\n",
      "  (layer_norm): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "  (gelu): GELU(approximate='none')\n",
      ")\n",
      "Epoch 1/50, Training Loss: 2.0233, Training Accuracy: 25.62%, Test accuracy: 33.15%\n",
      "Epoch 2/50, Training Loss: 1.7594, Training Accuracy: 36.43%, Test accuracy: 38.83%\n",
      "Epoch 3/50, Training Loss: 1.6658, Training Accuracy: 40.35%, Test accuracy: 43.26%\n",
      "Epoch 4/50, Training Loss: 1.6232, Training Accuracy: 41.66%, Test accuracy: 39.48%\n",
      "Epoch 5/50, Training Loss: 1.5897, Training Accuracy: 42.67%, Test accuracy: 42.88%\n",
      "Epoch 6/50, Training Loss: 1.5701, Training Accuracy: 43.68%, Test accuracy: 42.50%\n",
      "Epoch 7/50, Training Loss: 1.5353, Training Accuracy: 44.80%, Test accuracy: 44.96%\n",
      "Epoch 8/50, Training Loss: 1.5033, Training Accuracy: 46.26%, Test accuracy: 44.40%\n",
      "Epoch 9/50, Training Loss: 1.3676, Training Accuracy: 51.06%, Test accuracy: 49.52%\n",
      "Epoch 10/50, Training Loss: 1.3432, Training Accuracy: 51.88%, Test accuracy: 49.91%\n",
      "Epoch 11/50, Training Loss: 1.3445, Training Accuracy: 51.69%, Test accuracy: 48.73%\n",
      "Epoch 12/50, Training Loss: 1.3652, Training Accuracy: 51.05%, Test accuracy: 48.15%\n",
      "Epoch 13/50, Training Loss: 1.3747, Training Accuracy: 50.65%, Test accuracy: 48.61%\n",
      "Epoch 14/50, Training Loss: 1.3828, Training Accuracy: 50.35%, Test accuracy: 46.89%\n",
      "Epoch 15/50, Training Loss: 1.3765, Training Accuracy: 50.47%, Test accuracy: 47.72%\n",
      "Epoch 16/50, Training Loss: 1.3747, Training Accuracy: 50.62%, Test accuracy: 45.57%\n",
      "Epoch 17/50, Training Loss: 1.2167, Training Accuracy: 56.60%, Test accuracy: 51.71%\n",
      "Epoch 18/50, Training Loss: 1.1909, Training Accuracy: 57.23%, Test accuracy: 51.94%\n",
      "Epoch 19/50, Training Loss: 1.1926, Training Accuracy: 57.27%, Test accuracy: 51.33%\n",
      "Epoch 20/50, Training Loss: 1.2152, Training Accuracy: 56.61%, Test accuracy: 50.87%\n",
      "Epoch 21/50, Training Loss: 1.2419, Training Accuracy: 55.50%, Test accuracy: 50.55%\n",
      "Epoch 22/50, Training Loss: 1.2681, Training Accuracy: 54.57%, Test accuracy: 50.39%\n",
      "Epoch 23/50, Training Loss: 1.2588, Training Accuracy: 54.46%, Test accuracy: 48.54%\n",
      "Epoch 24/50, Training Loss: 1.2629, Training Accuracy: 54.89%, Test accuracy: 50.19%\n",
      "Epoch 25/50, Training Loss: 1.0883, Training Accuracy: 61.02%, Test accuracy: 52.68%\n",
      "Epoch 26/50, Training Loss: 1.0553, Training Accuracy: 62.31%, Test accuracy: 52.99%\n",
      "Epoch 27/50, Training Loss: 1.0598, Training Accuracy: 62.14%, Test accuracy: 51.66%\n",
      "Epoch 28/50, Training Loss: 1.0846, Training Accuracy: 61.07%, Test accuracy: 51.68%\n",
      "Epoch 29/50, Training Loss: 1.1355, Training Accuracy: 59.23%, Test accuracy: 50.58%\n",
      "Epoch 30/50, Training Loss: 1.1440, Training Accuracy: 58.76%, Test accuracy: 50.76%\n",
      "Epoch 31/50, Training Loss: 1.1588, Training Accuracy: 58.37%, Test accuracy: 51.54%\n",
      "Epoch 32/50, Training Loss: 1.1473, Training Accuracy: 58.76%, Test accuracy: 49.87%\n",
      "Epoch 33/50, Training Loss: 0.9654, Training Accuracy: 65.64%, Test accuracy: 52.80%\n",
      "Epoch 34/50, Training Loss: 0.9307, Training Accuracy: 66.54%, Test accuracy: 52.61%\n",
      "Epoch 35/50, Training Loss: 0.9345, Training Accuracy: 66.46%, Test accuracy: 51.19%\n",
      "Epoch 36/50, Training Loss: 0.9755, Training Accuracy: 64.75%, Test accuracy: 51.12%\n",
      "Epoch 37/50, Training Loss: 1.0050, Training Accuracy: 63.90%, Test accuracy: 50.14%\n",
      "Epoch 38/50, Training Loss: 1.0425, Training Accuracy: 62.40%, Test accuracy: 49.90%\n",
      "Epoch 39/50, Training Loss: 1.0621, Training Accuracy: 61.59%, Test accuracy: 49.95%\n",
      "Epoch 40/50, Training Loss: 1.0775, Training Accuracy: 61.15%, Test accuracy: 49.56%\n",
      "Epoch 41/50, Training Loss: 0.8639, Training Accuracy: 69.32%, Test accuracy: 51.85%\n",
      "Epoch 42/50, Training Loss: 0.8185, Training Accuracy: 71.03%, Test accuracy: 52.28%\n",
      "Epoch 43/50, Training Loss: 0.8282, Training Accuracy: 70.43%, Test accuracy: 51.74%\n",
      "Epoch 44/50, Training Loss: 0.8696, Training Accuracy: 68.97%, Test accuracy: 50.37%\n",
      "Epoch 45/50, Training Loss: 0.9124, Training Accuracy: 67.09%, Test accuracy: 50.95%\n",
      "Epoch 46/50, Training Loss: 0.9452, Training Accuracy: 65.66%, Test accuracy: 50.13%\n",
      "Epoch 47/50, Training Loss: 0.9853, Training Accuracy: 64.28%, Test accuracy: 50.21%\n",
      "Epoch 48/50, Training Loss: 0.9911, Training Accuracy: 64.32%, Test accuracy: 49.72%\n",
      "Epoch 49/50, Training Loss: 0.7672, Training Accuracy: 72.81%, Test accuracy: 51.79%\n",
      "Epoch 50/50, Training Loss: 0.7228, Training Accuracy: 74.67%, Test accuracy: 51.61%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "\n",
    "def extract_activations_layers(layers):\n",
    "    \"\"\" Extract for each layer the activations\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (layer_activation, batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array([np.array([np.array(h) for h in l]) for l in layers])\n",
    "\n",
    "def extract_activations_per_sample(layers, mask = False):\n",
    "    \"\"\" Extract for each sample the activations \n",
    "    for each layer and store them in a list.\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    if mask == True:\n",
    "        # mask the activations to remove zeros\n",
    "        mask = layers != 0\n",
    "        layers = [[np.array(h[m]) for h, m in zip(l,sm)] \n",
    "                for l, sm in zip(layers, mask)]\n",
    "        \n",
    "    return np.array([layers[:,i,:].flatten().reshape(-1, 1) for i in range(layers.shape[1])])\n",
    "\n",
    "\n",
    "def get_sampled_activations(activations, bandwidth = 0.2):\n",
    "    \"\"\" Sample the activations using KDE\n",
    "\n",
    "    Args:\n",
    "        activations (np.array): shape (batch_size, number_of_activations)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.stack([\n",
    "                    torch.tensor(KernelDensity(kernel=\"gaussian\", bandwidth=bandwidth).fit(a).sample([1]), dtype=torch.float32).squeeze(0) \n",
    "                    for a in activations], dim=0)\n",
    "\n",
    "def wd(layers: list()):\n",
    "    \"\"\" Compute the weight decay for each layer\n",
    "\n",
    "    Args:\n",
    "        layers (list): list of layers\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: weight decay\n",
    "\n",
    "    \"\"\"\n",
    "    return get_sampled_activations(\n",
    "                list(torch.from_numpy(extract_activations_layers(layers)).permute(1,0,2).numpy()), \n",
    "                bandwidth=0.2\n",
    "            )\n",
    "    \n",
    "    \n",
    "# N_layer = 5\n",
    "# prev = [torch.randn(12,64) for i in range(N_layer)]\n",
    "# wd(prev).shape\n",
    "\n",
    "n_neurons = 120\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, n_neurons)\n",
    "        self.l2 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.l3 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.l4 = nn.Linear(n_neurons, n_neurons)\n",
    "        # self.l2 = LinW(in_features=n_neurons, out_features=n_neurons)\n",
    "        # self.l5 = LinW(in_features=n_neurons, out_features=n_neurons)\n",
    "        self.l6 = nn.Linear(n_neurons, 10)\n",
    "        self.layer_norm = nn.LayerNorm(n_neurons)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l2(x))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l4(x))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        # x = self.gelu(self.l5(x, repr))\n",
    "        # x = self.layer_norm(x)\n",
    "        x = self.l6(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "\n",
    "    def forward(self, input, prev=[]):\n",
    "        # weight_decay = wd(prev)\n",
    "        # weight = self.weight * weight_decay.to('cuda:0')\n",
    "        # return F.linear(input, weight, self.bias)\n",
    "        # print(wd(prev).shape)\n",
    "        return F.linear(input * wd(prev).to(device), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# check if mps are available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = 8, T_mult=1, eta_min=5e-3)\n",
    "# StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(model)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "MLPWD(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (l1): Linear(in_features=3072, out_features=32, bias=True)\n",
      "  (l2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (l3): LinW(in_features=32, out_features=32, bias=True)\n",
      "  (l4): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (l5): LinW(in_features=32, out_features=32, bias=True)\n",
      "  (l6): Linear(in_features=32, out_features=10, bias=True)\n",
      "  (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (gelu): GELU(approximate='none')\n",
      ")\n",
      "Epoch 1/50, Training Loss: 2.2789, Training Accuracy: 13.29%, Test accuracy: 18.11%\n",
      "Epoch 2/50, Training Loss: 2.0588, Training Accuracy: 22.38%, Test accuracy: 26.33%\n",
      "Epoch 3/50, Training Loss: 1.9576, Training Accuracy: 27.87%, Test accuracy: 30.61%\n",
      "Epoch 4/50, Training Loss: 1.9080, Training Accuracy: 30.32%, Test accuracy: 30.30%\n",
      "Epoch 5/50, Training Loss: 1.8719, Training Accuracy: 31.44%, Test accuracy: 31.98%\n",
      "Epoch 6/50, Training Loss: 1.8492, Training Accuracy: 32.62%, Test accuracy: 32.98%\n",
      "Epoch 7/50, Training Loss: 1.8185, Training Accuracy: 33.61%, Test accuracy: 32.67%\n",
      "Epoch 8/50, Training Loss: 1.8087, Training Accuracy: 33.87%, Test accuracy: 34.27%\n",
      "Epoch 9/50, Training Loss: 1.7248, Training Accuracy: 37.24%, Test accuracy: 36.60%\n",
      "Epoch 10/50, Training Loss: 1.7066, Training Accuracy: 38.01%, Test accuracy: 37.48%\n",
      "Epoch 11/50, Training Loss: 1.7047, Training Accuracy: 37.84%, Test accuracy: 36.63%\n",
      "Epoch 12/50, Training Loss: 1.7094, Training Accuracy: 37.94%, Test accuracy: 37.36%\n",
      "Epoch 13/50, Training Loss: 1.7189, Training Accuracy: 37.45%, Test accuracy: 38.45%\n",
      "Epoch 14/50, Training Loss: 1.7338, Training Accuracy: 37.16%, Test accuracy: 37.98%\n",
      "Epoch 15/50, Training Loss: 1.7171, Training Accuracy: 37.86%, Test accuracy: 38.42%\n",
      "Epoch 16/50, Training Loss: 1.7173, Training Accuracy: 37.70%, Test accuracy: 36.13%\n",
      "Epoch 17/50, Training Loss: 1.6463, Training Accuracy: 40.35%, Test accuracy: 40.88%\n",
      "Epoch 18/50, Training Loss: 1.6250, Training Accuracy: 41.16%, Test accuracy: 40.55%\n",
      "Epoch 19/50, Training Loss: 1.6272, Training Accuracy: 41.03%, Test accuracy: 41.61%\n",
      "Epoch 20/50, Training Loss: 1.6385, Training Accuracy: 41.06%, Test accuracy: 40.68%\n",
      "Epoch 21/50, Training Loss: 1.6530, Training Accuracy: 40.08%, Test accuracy: 38.37%\n",
      "Epoch 22/50, Training Loss: 1.6552, Training Accuracy: 39.71%, Test accuracy: 39.37%\n",
      "Epoch 23/50, Training Loss: 1.6845, Training Accuracy: 39.28%, Test accuracy: 36.87%\n",
      "Epoch 24/50, Training Loss: 1.6789, Training Accuracy: 39.31%, Test accuracy: 39.13%\n",
      "Epoch 25/50, Training Loss: 1.5987, Training Accuracy: 42.28%, Test accuracy: 41.63%\n",
      "Epoch 26/50, Training Loss: 1.5836, Training Accuracy: 42.70%, Test accuracy: 41.47%\n",
      "Epoch 27/50, Training Loss: 1.5931, Training Accuracy: 42.49%, Test accuracy: 41.55%\n",
      "Epoch 28/50, Training Loss: 1.6052, Training Accuracy: 42.19%, Test accuracy: 41.50%\n",
      "Epoch 29/50, Training Loss: 1.6130, Training Accuracy: 41.49%, Test accuracy: 41.65%\n",
      "Epoch 30/50, Training Loss: 1.6291, Training Accuracy: 41.26%, Test accuracy: 40.40%\n",
      "Epoch 31/50, Training Loss: 1.6486, Training Accuracy: 40.73%, Test accuracy: 40.75%\n",
      "Epoch 32/50, Training Loss: 1.6354, Training Accuracy: 41.09%, Test accuracy: 40.11%\n",
      "Epoch 33/50, Training Loss: 1.5745, Training Accuracy: 43.39%, Test accuracy: 41.90%\n",
      "Epoch 34/50, Training Loss: 1.5649, Training Accuracy: 43.72%, Test accuracy: 42.63%\n",
      "Epoch 35/50, Training Loss: 1.5735, Training Accuracy: 43.22%, Test accuracy: 41.81%\n",
      "Epoch 36/50, Training Loss: 1.5860, Training Accuracy: 43.11%, Test accuracy: 42.01%\n",
      "Epoch 37/50, Training Loss: 1.6005, Training Accuracy: 42.51%, Test accuracy: 41.51%\n",
      "Epoch 38/50, Training Loss: 1.6176, Training Accuracy: 41.79%, Test accuracy: 41.69%\n",
      "Epoch 39/50, Training Loss: 1.6274, Training Accuracy: 41.35%, Test accuracy: 40.56%\n",
      "Epoch 40/50, Training Loss: 1.6353, Training Accuracy: 41.18%, Test accuracy: 41.52%\n",
      "Epoch 41/50, Training Loss: 1.5659, Training Accuracy: 43.49%, Test accuracy: 42.42%\n",
      "Epoch 42/50, Training Loss: 1.5552, Training Accuracy: 43.99%, Test accuracy: 42.79%\n",
      "Epoch 43/50, Training Loss: 1.5654, Training Accuracy: 43.84%, Test accuracy: 42.67%\n",
      "Epoch 44/50, Training Loss: 1.5687, Training Accuracy: 43.61%, Test accuracy: 41.20%\n",
      "Epoch 45/50, Training Loss: 1.5940, Training Accuracy: 42.67%, Test accuracy: 42.00%\n",
      "Epoch 46/50, Training Loss: 1.5948, Training Accuracy: 42.73%, Test accuracy: 41.05%\n",
      "Epoch 47/50, Training Loss: 1.6131, Training Accuracy: 42.07%, Test accuracy: 39.90%\n",
      "Epoch 48/50, Training Loss: 1.6171, Training Accuracy: 41.84%, Test accuracy: 36.64%\n",
      "Epoch 49/50, Training Loss: 1.5611, Training Accuracy: 44.06%, Test accuracy: 43.23%\n",
      "Epoch 50/50, Training Loss: 1.5472, Training Accuracy: 44.40%, Test accuracy: 43.02%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "\n",
    "def extract_activations_layers(layers):\n",
    "    \"\"\" Extract for each layer the activations\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (layer_activation, batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array([np.array([np.array(h) for h in l]) for l in layers])\n",
    "\n",
    "def extract_activations_per_sample(layers, mask = False):\n",
    "    \"\"\" Extract for each sample the activations \n",
    "    for each layer and store them in a list.\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    if mask == True:\n",
    "        # mask the activations to remove zeros\n",
    "        mask = layers != 0\n",
    "        layers = [[np.array(h[m]) for h, m in zip(l,sm)] \n",
    "                for l, sm in zip(layers, mask)]\n",
    "        \n",
    "    return np.array([layers[:,i,:].flatten().reshape(-1, 1) for i in range(layers.shape[1])])\n",
    "\n",
    "\n",
    "def get_sampled_activations(activations, bandwidth = 0.2):\n",
    "    \"\"\" Sample the activations using KDE\n",
    "\n",
    "    Args:\n",
    "        activations (np.array): shape (batch_size, number_of_activations)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.stack([\n",
    "                    torch.tensor(KernelDensity(kernel=\"gaussian\", bandwidth=bandwidth).fit(a).sample([1]), dtype=torch.float32).squeeze(0) \n",
    "                    for a in activations], dim=0)\n",
    "\n",
    "def wd(layers: list()):\n",
    "    \"\"\" Compute the weight decay for each layer\n",
    "\n",
    "    Args:\n",
    "        layers (list): list of layers\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: weight decay\n",
    "\n",
    "    \"\"\"\n",
    "    return get_sampled_activations(\n",
    "                list(torch.from_numpy(extract_activations_layers(layers)).permute(1,0,2).numpy()), \n",
    "                bandwidth=0.2\n",
    "            )\n",
    "    \n",
    "    \n",
    "# N_layer = 5\n",
    "# prev = [torch.randn(12,64) for i in range(N_layer)]\n",
    "# wd(prev).shape\n",
    "\n",
    "n_neurons = 32\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, n_neurons)\n",
    "        self.l2 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.l3 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.l4 = nn.Linear(n_neurons, n_neurons)\n",
    "        # self.l2 = LinW(in_features=n_neurons, out_features=n_neurons)\n",
    "        self.l3 = LinW(in_features=n_neurons, out_features=n_neurons)\n",
    "        self.l5 = LinW(in_features=n_neurons, out_features=n_neurons)\n",
    "        self.l6 = nn.Linear(n_neurons, 10)\n",
    "        self.layer_norm = nn.LayerNorm(n_neurons)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l2(x))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l4(x))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l5(x, repr))\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.l6(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "\n",
    "    def forward(self, input, prev=[]):\n",
    "        # weight_decay = wd(prev)\n",
    "        # weight = self.weight * weight_decay.to('cuda:0')\n",
    "        # return F.linear(input, weight, self.bias)\n",
    "        # print(wd(prev).shape)\n",
    "        return F.linear(input * wd(prev).to(device), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# check if mps are available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = 8, T_mult=1, eta_min=5e-3)\n",
    "# StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(model)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "MLPWD(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (l1): Linear(in_features=3072, out_features=256, bias=True)\n",
      "  (l2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (l3): LinW(in_features=256, out_features=256, bias=True)\n",
      "  (l4): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (l5): LinW(in_features=256, out_features=256, bias=True)\n",
      "  (l6): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (gelu): GELU(approximate='none')\n",
      ")\n",
      "Epoch 1/50, Training Loss: 2.1420, Training Accuracy: 17.96%, Test accuracy: 21.17%\n",
      "Epoch 2/50, Training Loss: 1.9382, Training Accuracy: 27.77%, Test accuracy: 29.20%\n",
      "Epoch 3/50, Training Loss: 1.8583, Training Accuracy: 31.81%, Test accuracy: 32.32%\n",
      "Epoch 4/50, Training Loss: 1.8294, Training Accuracy: 32.86%, Test accuracy: 34.87%\n",
      "Epoch 5/50, Training Loss: 1.7890, Training Accuracy: 34.61%, Test accuracy: 36.40%\n",
      "Epoch 6/50, Training Loss: 1.7592, Training Accuracy: 35.81%, Test accuracy: 35.62%\n",
      "Epoch 7/50, Training Loss: 1.7330, Training Accuracy: 36.81%, Test accuracy: 37.78%\n",
      "Epoch 8/50, Training Loss: 1.7043, Training Accuracy: 38.28%, Test accuracy: 39.94%\n",
      "Epoch 9/50, Training Loss: 1.5902, Training Accuracy: 42.52%, Test accuracy: 42.68%\n",
      "Epoch 10/50, Training Loss: 1.5718, Training Accuracy: 43.35%, Test accuracy: 43.42%\n",
      "Epoch 11/50, Training Loss: 1.5697, Training Accuracy: 43.38%, Test accuracy: 43.04%\n",
      "Epoch 12/50, Training Loss: 1.5887, Training Accuracy: 42.78%, Test accuracy: 41.43%\n",
      "Epoch 13/50, Training Loss: 1.6052, Training Accuracy: 42.22%, Test accuracy: 42.50%\n",
      "Epoch 14/50, Training Loss: 1.6308, Training Accuracy: 41.29%, Test accuracy: 41.78%\n",
      "Epoch 15/50, Training Loss: 1.6305, Training Accuracy: 41.53%, Test accuracy: 38.36%\n",
      "Epoch 16/50, Training Loss: 1.6178, Training Accuracy: 41.91%, Test accuracy: 40.92%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 162\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mprint\u001b[39m(model)\n\u001b[1;32m    161\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m--> 162\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(model, train_loader, criterion, optimizer, device)\n\u001b[1;32m    163\u001b[0m     test_accuracy \u001b[39m=\u001b[39m evaluate(device, model, test_loader)\n\u001b[1;32m    164\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mEPOCHS\u001b[39m}\u001b[39;00m\u001b[39m, Training Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Training Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%, Test accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtest_accuracy\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[31], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m images, labels \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> 9\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     10\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     11\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/mambaforge/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[63], line 110\u001b[0m, in \u001b[0;36mMLPWD.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39mrepr\u001b[39m\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m    109\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml3(x, \u001b[39mrepr\u001b[39m))\n\u001b[0;32m--> 110\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer_norm(x)\n\u001b[1;32m    111\u001b[0m \u001b[39mrepr\u001b[39m\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m    112\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml4(x))\n",
      "File \u001b[0;32m~/mambaforge/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/ml/lib/python3.10/site-packages/torch/nn/modules/normalization.py:190\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlayer_norm(\n\u001b[1;32m    191\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalized_shape, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
      "File \u001b[0;32m~/mambaforge/envs/ml/lib/python3.10/site-packages/torch/nn/functional.py:2515\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2511\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[1;32m   2512\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2513\u001b[0m         layer_norm, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, normalized_shape, weight\u001b[39m=\u001b[39mweight, bias\u001b[39m=\u001b[39mbias, eps\u001b[39m=\u001b[39meps\n\u001b[1;32m   2514\u001b[0m     )\n\u001b[0;32m-> 2515\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mlayer_norm(\u001b[39minput\u001b[39;49m, normalized_shape, weight, bias, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "\n",
    "def extract_activations_layers(layers):\n",
    "    \"\"\" Extract for each layer the activations\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (layer_activation, batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array([np.array([np.array(h) for h in l]) for l in layers])\n",
    "\n",
    "def extract_activations_per_sample(layers, mask = False):\n",
    "    \"\"\" Extract for each sample the activations \n",
    "    for each layer and store them in a list.\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    if mask == True:\n",
    "        # mask the activations to remove zeros\n",
    "        mask = layers != 0\n",
    "        layers = [[np.array(h[m]) for h, m in zip(l,sm)] \n",
    "                for l, sm in zip(layers, mask)]\n",
    "        \n",
    "    return np.array([layers[:,i,:].flatten().reshape(-1, 1) for i in range(layers.shape[1])])\n",
    "\n",
    "\n",
    "def get_sampled_activations(activations, bandwidth = 0.2):\n",
    "    \"\"\" Sample the activations using KDE\n",
    "\n",
    "    Args:\n",
    "        activations (np.array): shape (batch_size, number_of_activations)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.stack([\n",
    "                    torch.tensor(KernelDensity(kernel=\"gaussian\", bandwidth=bandwidth).fit(a).sample([1]), dtype=torch.float32).squeeze(0) \n",
    "                    for a in activations], dim=0)\n",
    "\n",
    "def wd(layers: list()):\n",
    "    \"\"\" Compute the weight decay for each layer\n",
    "\n",
    "    Args:\n",
    "        layers (list): list of layers\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: weight decay\n",
    "\n",
    "    \"\"\"\n",
    "    return get_sampled_activations(\n",
    "                list(torch.from_numpy(extract_activations_layers(layers)).permute(1,0,2).numpy()), \n",
    "                bandwidth=0.2\n",
    "            )\n",
    "    \n",
    "    \n",
    "# N_layer = 5\n",
    "# prev = [torch.randn(12,64) for i in range(N_layer)]\n",
    "# wd(prev).shape\n",
    "\n",
    "n_neurons = 256\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, n_neurons)\n",
    "        self.l2 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.l3 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.l4 = nn.Linear(n_neurons, n_neurons)\n",
    "        # self.l2 = LinW(in_features=n_neurons, out_features=n_neurons)\n",
    "        self.l3 = LinW(in_features=n_neurons, out_features=n_neurons)\n",
    "        self.l5 = LinW(in_features=n_neurons, out_features=n_neurons)\n",
    "        self.l6 = nn.Linear(n_neurons, 10)\n",
    "        self.layer_norm = nn.LayerNorm(n_neurons)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l2(x))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l3(x, repr))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l4(x))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l5(x, repr))\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.l6(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "\n",
    "    def forward(self, input, prev=[]):\n",
    "        # weight_decay = wd(prev)\n",
    "        # weight = self.weight * weight_decay.to('cuda:0')\n",
    "        # return F.linear(input, weight, self.bias)\n",
    "        # print(wd(prev).shape)\n",
    "        return F.linear(input * wd(prev).to(device), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# check if mps are available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = 8, T_mult=1, eta_min=5e-3)\n",
    "# StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(model)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "MLPWD(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (l1): Linear(in_features=3072, out_features=64, bias=True)\n",
      "  (t_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (l2): LinW(in_features=64, out_features=64, bias=True)\n",
      "  (l3): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  (gelu): GELU(approximate='none')\n",
      ")\n",
      "Epoch 1/20, Training Loss: 2.0313, Training Accuracy: 24.17%, Test accuracy: 34.07%\n",
      "Epoch 2/20, Training Loss: 1.7823, Training Accuracy: 35.74%, Test accuracy: 38.88%\n",
      "Epoch 3/20, Training Loss: 1.6900, Training Accuracy: 39.28%, Test accuracy: 41.36%\n",
      "Epoch 4/20, Training Loss: 1.6555, Training Accuracy: 40.69%, Test accuracy: 43.01%\n",
      "Epoch 5/20, Training Loss: 1.6231, Training Accuracy: 42.06%, Test accuracy: 42.66%\n",
      "Epoch 6/20, Training Loss: 1.6027, Training Accuracy: 42.93%, Test accuracy: 44.84%\n",
      "Epoch 7/20, Training Loss: 1.5895, Training Accuracy: 43.29%, Test accuracy: 44.02%\n",
      "Epoch 8/20, Training Loss: 1.5751, Training Accuracy: 44.16%, Test accuracy: 45.30%\n",
      "Epoch 9/20, Training Loss: 1.4456, Training Accuracy: 48.54%, Test accuracy: 47.90%\n",
      "Epoch 10/20, Training Loss: 1.4151, Training Accuracy: 49.73%, Test accuracy: 48.91%\n",
      "Epoch 11/20, Training Loss: 1.4071, Training Accuracy: 50.08%, Test accuracy: 48.34%\n",
      "Epoch 12/20, Training Loss: 1.4225, Training Accuracy: 49.46%, Test accuracy: 48.86%\n",
      "Epoch 13/20, Training Loss: 1.4399, Training Accuracy: 48.98%, Test accuracy: 46.54%\n",
      "Epoch 14/20, Training Loss: 1.4575, Training Accuracy: 48.22%, Test accuracy: 48.12%\n",
      "Epoch 15/20, Training Loss: 1.4535, Training Accuracy: 48.41%, Test accuracy: 47.05%\n",
      "Epoch 16/20, Training Loss: 1.4628, Training Accuracy: 48.25%, Test accuracy: 47.76%\n",
      "Epoch 17/20, Training Loss: 1.3254, Training Accuracy: 53.18%, Test accuracy: 50.65%\n",
      "Epoch 18/20, Training Loss: 1.2859, Training Accuracy: 54.59%, Test accuracy: 50.99%\n",
      "Epoch 19/20, Training Loss: 1.2879, Training Accuracy: 54.64%, Test accuracy: 50.78%\n",
      "Epoch 20/20, Training Loss: 1.3033, Training Accuracy: 54.02%, Test accuracy: 50.13%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "\n",
    "def extract_activations_layers(layers):\n",
    "    \"\"\" Extract for each layer the activations\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (layer_activation, batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array([np.array([np.array(h) for h in l]) for l in layers])\n",
    "\n",
    "def extract_activations_per_sample(layers, mask = False):\n",
    "    \"\"\" Extract for each sample the activations \n",
    "    for each layer and store them in a list.\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    if mask == True:\n",
    "        # mask the activations to remove zeros\n",
    "        mask = layers != 0\n",
    "        layers = [[np.array(h[m]) for h, m in zip(l,sm)] \n",
    "                for l, sm in zip(layers, mask)]\n",
    "        \n",
    "    return np.array([layers[:,i,:].flatten().reshape(-1, 1) for i in range(layers.shape[1])])\n",
    "\n",
    "\n",
    "def get_sampled_activations(activations, bandwidth = 0.2):\n",
    "    \"\"\" Sample the activations using KDE\n",
    "\n",
    "    Args:\n",
    "        activations (np.array): shape (batch_size, number_of_activations)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.stack([\n",
    "                    torch.tensor(KernelDensity(kernel=\"gaussian\", bandwidth=bandwidth).fit(a).sample([1]), dtype=torch.float32).squeeze(0) \n",
    "                    for a in activations], dim=0)\n",
    "\n",
    "def wd(layers: list()):\n",
    "    \"\"\" Compute the weight decay for each layer\n",
    "\n",
    "    Args:\n",
    "        layers (list): list of layers\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: weight decay\n",
    "\n",
    "    \"\"\"\n",
    "    return get_sampled_activations(\n",
    "                list(torch.from_numpy(extract_activations_layers(layers)).permute(1,0,2).numpy()), \n",
    "                bandwidth=0.2\n",
    "            )\n",
    "    \n",
    "    \n",
    "# N_layer = 5\n",
    "# prev = [torch.randn(12,64) for i in range(N_layer)]\n",
    "# wd(prev).shape\n",
    "\n",
    "n_neurons = 64\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self, n_head=2):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, n_neurons)\n",
    "        self.t_encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=n_neurons, nhead=n_head), num_layers=2)\n",
    "        self.l2 = LinW(in_features=n_neurons, out_features=n_neurons)\n",
    "        self.l3 = nn.Linear(n_neurons, 10)\n",
    "        self.layer_norm = nn.LayerNorm(n_neurons)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.t_encoder(x))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l2(x, repr))\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "\n",
    "    def forward(self, input, prev=[]):\n",
    "        # weight_decay = wd(prev)\n",
    "        # weight = self.weight * weight_decay.to('cuda:0')\n",
    "        # return F.linear(input, weight, self.bias)\n",
    "        # print(wd(prev).shape)\n",
    "        return F.linear(input * wd(prev).to(device), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# check if mps are available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = 8, T_mult=1, eta_min=5e-3)\n",
    "# StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(model)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "MLPWD(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (l1): Linear(in_features=3072, out_features=64, bias=True)\n",
      "  (t_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (l2): LinW(in_features=64, out_features=64, bias=True)\n",
      "  (l3): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  (gelu): GELU(approximate='none')\n",
      ")\n",
      "Epoch 1/20, Training Loss: 2.0129, Training Accuracy: 25.07%, Test accuracy: 35.37%\n",
      "Epoch 2/20, Training Loss: 1.7541, Training Accuracy: 37.09%, Test accuracy: 40.42%\n",
      "Epoch 3/20, Training Loss: 1.6689, Training Accuracy: 40.51%, Test accuracy: 42.66%\n",
      "Epoch 4/20, Training Loss: 1.6256, Training Accuracy: 41.92%, Test accuracy: 43.24%\n",
      "Epoch 5/20, Training Loss: 1.5949, Training Accuracy: 43.31%, Test accuracy: 44.55%\n",
      "Epoch 6/20, Training Loss: 1.5670, Training Accuracy: 44.16%, Test accuracy: 44.30%\n",
      "Epoch 7/20, Training Loss: 1.5372, Training Accuracy: 45.29%, Test accuracy: 45.58%\n",
      "Epoch 8/20, Training Loss: 1.5165, Training Accuracy: 46.32%, Test accuracy: 46.83%\n",
      "Epoch 9/20, Training Loss: 1.3851, Training Accuracy: 50.75%, Test accuracy: 49.26%\n",
      "Epoch 10/20, Training Loss: 1.3572, Training Accuracy: 51.78%, Test accuracy: 50.24%\n",
      "Epoch 11/20, Training Loss: 1.3531, Training Accuracy: 51.90%, Test accuracy: 49.72%\n",
      "Epoch 12/20, Training Loss: 1.3607, Training Accuracy: 51.60%, Test accuracy: 48.83%\n",
      "Epoch 13/20, Training Loss: 1.3677, Training Accuracy: 51.40%, Test accuracy: 49.35%\n",
      "Epoch 14/20, Training Loss: 1.3871, Training Accuracy: 50.54%, Test accuracy: 48.47%\n",
      "Epoch 15/20, Training Loss: 1.3889, Training Accuracy: 50.65%, Test accuracy: 49.70%\n",
      "Epoch 16/20, Training Loss: 1.3665, Training Accuracy: 51.45%, Test accuracy: 49.19%\n",
      "Epoch 17/20, Training Loss: 1.2270, Training Accuracy: 56.56%, Test accuracy: 52.40%\n",
      "Epoch 18/20, Training Loss: 1.1925, Training Accuracy: 57.93%, Test accuracy: 52.82%\n",
      "Epoch 19/20, Training Loss: 1.1878, Training Accuracy: 57.95%, Test accuracy: 52.26%\n",
      "Epoch 20/20, Training Loss: 1.2029, Training Accuracy: 57.34%, Test accuracy: 52.15%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "\n",
    "def extract_activations_layers(layers):\n",
    "    \"\"\" Extract for each layer the activations\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (layer_activation, batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array([np.array([np.array(h) for h in l]) for l in layers])\n",
    "\n",
    "def extract_activations_per_sample(layers, mask = False):\n",
    "    \"\"\" Extract for each sample the activations \n",
    "    for each layer and store them in a list.\n",
    "\n",
    "    Args:\n",
    "        layers (np.array): shape (layer_activation, batch_size, number_of_neurons)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    if mask == True:\n",
    "        # mask the activations to remove zeros\n",
    "        mask = layers != 0\n",
    "        layers = [[np.array(h[m]) for h, m in zip(l,sm)] \n",
    "                for l, sm in zip(layers, mask)]\n",
    "        \n",
    "    return np.array([layers[:,i,:].flatten().reshape(-1, 1) for i in range(layers.shape[1])])\n",
    "\n",
    "\n",
    "def get_sampled_activations(activations, bandwidth = 0.2):\n",
    "    \"\"\" Sample the activations using KDE\n",
    "\n",
    "    Args:\n",
    "        activations (np.array): shape (batch_size, number_of_activations)\n",
    "\n",
    "    Returns:\n",
    "        np.array: shape (batch_size, number_of_activations)\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.stack([\n",
    "                    torch.tensor(KernelDensity(kernel=\"gaussian\", bandwidth=bandwidth).fit(a).sample([1]), dtype=torch.float32).squeeze(0) \n",
    "                    for a in activations], dim=0)\n",
    "\n",
    "def wd(layers: list()):\n",
    "    \"\"\" Compute the weight decay for each layer\n",
    "\n",
    "    Args:\n",
    "        layers (list): list of layers\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: weight decay\n",
    "\n",
    "    \"\"\"\n",
    "    return get_sampled_activations(\n",
    "                list(torch.from_numpy(extract_activations_layers(layers)).permute(1,0,2).numpy()), \n",
    "                bandwidth=0.2\n",
    "            )\n",
    "    \n",
    "    \n",
    "# N_layer = 5\n",
    "# prev = [torch.randn(12,64) for i in range(N_layer)]\n",
    "# wd(prev).shape\n",
    "\n",
    "n_neurons = 64\n",
    "\n",
    "class MLPWD(nn.Module):\n",
    "    def __init__(self, n_head=2):\n",
    "        super(MLPWD, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(3072, n_neurons)\n",
    "        self.t_encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=n_neurons, nhead=n_head), num_layers=2)\n",
    "        self.l2 = LinW(in_features=n_neurons, out_features=n_neurons)\n",
    "        self.l3 = nn.Linear(n_neurons, 10)\n",
    "        self.layer_norm = nn.LayerNorm(n_neurons)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        repr = []\n",
    "        x = self.flatten(x)\n",
    "        x = self.gelu(self.l1(x))\n",
    "        x = self.layer_norm(x)\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.t_encoder(x))\n",
    "        repr.append(x.detach().cpu().numpy())\n",
    "        x = self.gelu(self.l2(x, repr))\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "\n",
    "class LinW(nn.Linear):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinW, self).__init__(in_features=in_features, out_features=out_features)\n",
    "\n",
    "    def forward(self, input, prev=[]):\n",
    "        # weight_decay = wd(prev)\n",
    "        # weight = self.weight * weight_decay.to('cuda:0')\n",
    "        # return F.linear(input, weight, self.bias)\n",
    "        # print(wd(prev).shape)\n",
    "        return F.linear(input + wd(prev).to(device), self.weight, self.bias)\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# check if mps are available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLPWD().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = 8, T_mult=1, eta_min=5e-3)\n",
    "# StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(model)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "f3163cfb8aa3549ad3f5400bc3427ee7a4002d2a0d6d7ead52f641c6a7636395"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
