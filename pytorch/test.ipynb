{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "from jax.scipy.special import logsumexp\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import time\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_layer_params(m, n, key, scale=1e-2):\n",
    "    \"\"\"Initialize the weights and the biases\n",
    "\n",
    "            Input:\n",
    "                m:\n",
    "                n:\n",
    "                key:\n",
    "                scale:\n",
    "\n",
    "            Output:\n",
    "                weights:\n",
    "                biases:\n",
    "\n",
    "    \"\"\"\n",
    "    w_key, b_key = random.split(key)\n",
    "    return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
    "\n",
    "\n",
    "def init_network_params(sizes, key):\n",
    "    \"\"\"Initialize the weights and the biases\n",
    "        of the fully-connected network.\n",
    "\n",
    "            Input:\n",
    "                size:\n",
    "                key:\n",
    "\n",
    "            Output:\n",
    "                weights, biases: \n",
    "\n",
    "    \"\"\"\n",
    "    keys = random.split(key, len(sizes))\n",
    "    return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    # activation function\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "def flattenList(l):\n",
    "    # this function flatten a list\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def predict(params, image):\n",
    "    # prediction given a sample\n",
    "    activations = image\n",
    "    hist_activation = []\n",
    "    \n",
    "    for w, b in params[:-1]:\n",
    "        log_likelihood = KernelDensity(kernel='gaussian', bandwidth=0.6).fit(X).score_samples(X[:3])\n",
    "        outputs = jnp.dot(w, activations) + b\n",
    "        activations = relu(outputs)\n",
    "        hist_activation.append(activations)\n",
    "\n",
    "    final_w, final_b = params[-1]\n",
    "    logits = jnp.dot(final_w, activations) + final_b\n",
    "    return logits - logsumexp(logits)\n",
    "\n",
    "\n",
    "def one_hot(x, k, dtype=jnp.float32):\n",
    "    # Produce one-hot encoding representation\n",
    "    return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "\n",
    "\n",
    "def accuracy(params, images, targets):\n",
    "    # measure the accuracy averaging\n",
    "    target_class = jnp.argmax(targets, axis=1)\n",
    "    predicted_class = jnp.argmax(batched_predict(params, images), axis=1)\n",
    "    return jnp.mean(predicted_class == target_class)\n",
    "\n",
    "\n",
    "def loss(params, images, targets):\n",
    "    preds = batched_predict(params, images)\n",
    "    return -jnp.mean(preds * targets)\n",
    "\n",
    "@jit\n",
    "def update(params, x, y):\n",
    "    # \n",
    "    grads = grad(loss)(params, x, y)\n",
    "    # \n",
    "    return [(w - step_size * dw, b - step_size * db)\n",
    "                for (w, b), (dw, db) in zip(params, grads)]\n",
    "\n",
    "def get_train_batches():\n",
    "    data_dir = '/tmp/tfds'\n",
    "    # get (image, label) as a tuple instead of a dict\n",
    "    ds = tfds.load(name=dataset, split='train', as_supervised=True, data_dir=data_dir)\n",
    "\n",
    "    # initialize number of pixel\n",
    "#     _, info = tfds.load(name=dataset, batch_size=-1, data_dir=data_dir, with_info=True)\n",
    "#     num_labels = info.features['label'].num_classes\n",
    "#     h, w, c = info.features['image'].shape\n",
    "#     self.num_pixels = h * w * c\n",
    "\n",
    "    # generate batches\n",
    "    ds = ds.batch(batch_size).prefetch(1)\n",
    "    # converts the tf.data.Dataset into an iterable of NumPy arrays\n",
    "    return tfds.as_numpy(ds)\n",
    "\n",
    "def load_data():\n",
    "    # fetch the full dataset\n",
    "    data_dir = '/tmp/tfds'\n",
    "    mnist_data, info = tfds.load(name=dataset, batch_size=-1, data_dir=data_dir, with_info=True)\n",
    "    mnist_data = tfds.as_numpy(mnist_data)\n",
    "    train_data, test_data = mnist_data['train'], mnist_data['test']\n",
    "    num_labels = info.features['label'].num_classes\n",
    "    h, w, c = info.features['image'].shape\n",
    "    num_pixels = h * w * c\n",
    "\n",
    "    # traingset\n",
    "    train_images, train_labels = train_data['image'], train_data['label']\n",
    "    train_images = jnp.reshape(train_images, (len(train_images), num_pixels))\n",
    "    train_labels = one_hot(train_labels, num_labels)\n",
    "\n",
    "    # testset\n",
    "    test_images, test_labels = test_data['image'], test_data['label']\n",
    "    test_images = jnp.reshape(test_images, (len(test_images), num_pixels))\n",
    "    test_labels = one_hot(test_labels, num_labels)\n",
    "\n",
    "    return train_images, train_labels, test_images, test_labels\n",
    "\n",
    "\n",
    "\n",
    "# def train():\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "\n",
    "#         start_time = time.time()\n",
    "#         for x, y in get_train_batches():\n",
    "#             x = jnp.reshape(x, (len(x), num_pixels))\n",
    "#             y = one_hot(y, num_labels)\n",
    "#             params = update(params, x, y)\n",
    "#         epoch_time = time.time() - start_time\n",
    "\n",
    "#         train_images, train_labels, test_images, test_labels = load_data()\n",
    "\n",
    "#         # measure the accuracy\n",
    "#         train_acc = accuracy(params, train_images, train_labels)\n",
    "#         test_acc = accuracy(params, test_images, test_labels)\n",
    "#         print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "#         print(\"Training set accuracy {}\".format(train_acc))\n",
    "#         print(\"Test set accuracy {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "\n",
    "layer_sizes = [784, 512, 512, 10]\n",
    "step_size: int = 0.01\n",
    "num_epochs: int = 10\n",
    "batch_size: int = 128\n",
    "n_targets: int = 10\n",
    "dataset = 'mnist'\n",
    "\n",
    "# initialize parameter\n",
    "params = init_network_params(layer_sizes, random.PRNGKey(0))\n",
    "\n",
    "# Batched version of the `predict` function\n",
    "batched_predict = vmap(predict, in_axes=(None, 0))\n",
    "\n",
    "# import dataset info\n",
    "data_dir = '/tmp/tfds'\n",
    "_, info = tfds.load(name=dataset, batch_size=-1, data_dir=data_dir, with_info=True)\n",
    "\n",
    "# number of pixel\n",
    "h, w, c = info.features['image'].shape\n",
    "num_pixels = h * w * c\n",
    "\n",
    "# number of label\n",
    "num_labels = info.features['label'].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
=======
   "execution_count": 1,
>>>>>>> origin/andrea
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 0 in 2.25 sec\n",
      "Training set accuracy 0.9427500367164612\n",
      "Test set accuracy 0.941100001335144\n",
      "Epoch 1 in 2.20 sec\n",
      "Training set accuracy 0.9531333446502686\n",
      "Test set accuracy 0.9511999487876892\n",
      "Epoch 2 in 1.96 sec\n",
      "Training set accuracy 0.9600000381469727\n",
      "Test set accuracy 0.9554999470710754\n",
      "Epoch 3 in 1.98 sec\n",
      "Training set accuracy 0.9651166796684265\n",
      "Test set accuracy 0.9602999687194824\n",
      "Epoch 4 in 2.03 sec\n",
      "Training set accuracy 0.9691666960716248\n",
      "Test set accuracy 0.9628999829292297\n",
      "Epoch 5 in 1.96 sec\n",
      "Training set accuracy 0.9726166725158691\n",
      "Test set accuracy 0.9651999473571777\n",
      "Epoch 6 in 2.02 sec\n",
      "Training set accuracy 0.9754166603088379\n",
      "Test set accuracy 0.9666000008583069\n",
      "Epoch 7 in 2.02 sec\n",
      "Training set accuracy 0.9780499935150146\n",
      "Test set accuracy 0.9679999947547913\n",
      "Epoch 8 in 2.12 sec\n",
      "Training set accuracy 0.980316698551178\n",
      "Test set accuracy 0.9693999886512756\n",
      "Epoch 9 in 2.34 sec\n",
      "Training set accuracy 0.98211669921875\n",
      "Test set accuracy 0.9703999757766724\n"
=======
      "LinW layers:\n",
      "\n",
      "Depth 0: LinW(in_features=3, out_features=3, bias=True)\n",
      "Depth 1: LinW(in_features=3, out_features=3, bias=True)\n"
>>>>>>> origin/andrea
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "train_images, train_labels, test_images, test_labels = load_data()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "    for x, y in get_train_batches():\n",
    "        x = jnp.reshape(x, (len(x), num_pixels))\n",
    "        y = one_hot(y, num_labels)\n",
    "        params = update(params, x, y)\n",
    "    epoch_time = time.time() - start_time\n",
    "\n",
    "    # measure the accuracy\n",
    "    train_acc = accuracy(params, train_images, train_labels)\n",
    "    test_acc = accuracy(params, test_images, test_labels)\n",
    "    print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "    print(\"Training set accuracy {}\".format(train_acc))\n",
    "    print(\"Test set accuracy {}\".format(test_acc))"
=======
    "from model import MLP\n",
    "\n",
    "model = MLP()\n",
    "\n",
    "print(\"LinW layers:\", \"\\n\".join([f\"Depth {model[i].depth}: {model[i]}\" for i in range(len(model))]), sep=\"\\n\\n\")"
>>>>>>> origin/andrea
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def flattenList(l):\n",
    "    # this function flatten a list\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "\n",
    "hist_activation=[]\n",
    "for i in range(1):\n",
    "#     hist_activation = flattenList(hist_activation.append(np.random.randn(10)))   \n",
    "    hist_activation.append(list([np.random.randn(10)]))\n"
=======
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / total\n",
    "    return train_loss, accuracy"
>>>>>>> origin/andrea
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3282723674761134,\n",
       " -0.06700344442769444,\n",
       " -0.9976463965047163,\n",
       " -1.2494449119033997,\n",
       " 0.4316570733309302,\n",
       " 0.2719909308559955,\n",
       " 0.2693416349819199,\n",
       " 0.5841085526483356,\n",
       " 1.1045208595753788,\n",
       " -0.24438789681719417]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattenList(hist_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.53545484, -0.75405626, -0.58566053, -0.53437566, -0.64215413])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "import numpy as np\n",
    "rng = np.random.RandomState(42)\n",
    "X = rng.random_sample((1000, 1))\n",
    "# X array-like of shape (n_samples, n_features)\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=0.6).fit(X)\n",
    "log_density = kde.score_samples(X[:5])\n",
    "log_density"
=======
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def evaluate(device, model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
>>>>>>> origin/andrea
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Incompatible shapes for broadcasting: shapes=[(512, 784), (512,)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/mambaforge/envs/jax/lib/python3.11/site-packages/jax/_src/util.py:254\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 254\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trace_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/jax/lib/python3.11/site-packages/jax/_src/util.py:247\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached\u001b[39m(_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 247\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/jax/lib/python3.11/site-packages/jax/_src/lax/lax.py:145\u001b[0m, in \u001b[0;36m_broadcast_shapes_cached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;129m@cache\u001b[39m()\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_broadcast_shapes_cached\u001b[39m(\u001b[38;5;241m*\u001b[39mshapes: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m--> 145\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_broadcast_shapes_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/jax/lib/python3.11/site-packages/jax/_src/lax/lax.py:161\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(512, 784), (512,)]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[173], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m y \u001b[38;5;241m=\u001b[39m one_hot(y, num_labels)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w, b \u001b[38;5;129;01min\u001b[39;00m params[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m---> 10\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_density\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n\u001b[1;32m     11\u001b[0m     activations \u001b[38;5;241m=\u001b[39m relu(outputs)\n\u001b[1;32m     12\u001b[0m     hist_activation\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlist\u001b[39m(activations))\n",
      "File \u001b[0;32m~/mambaforge/envs/jax/lib/python3.11/site-packages/jax/_src/numpy/lax_numpy.py:5112\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   5110\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m   5111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m-> 5112\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _rejected_binop_types):\n\u001b[1;32m   5114\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsupported operand type(s) for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopchar\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5115\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(args[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(args[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "File \u001b[0;32m~/mambaforge/envs/jax/lib/python3.11/site-packages/jax/_src/numpy/ufuncs.py:95\u001b[0m, in \u001b[0;36m_maybe_bool_binop.<locals>.fn\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(x1, x2, \u001b[38;5;241m/\u001b[39m):\n\u001b[0;32m---> 95\u001b[0m   x1, x2 \u001b[38;5;241m=\u001b[39m \u001b[43m_promote_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumpy_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m lax_fn(x1, x2) \u001b[38;5;28;01mif\u001b[39;00m x1\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_ \u001b[38;5;28;01melse\u001b[39;00m bool_lax_fn(x1, x2)\n",
      "File \u001b[0;32m~/mambaforge/envs/jax/lib/python3.11/site-packages/jax/_src/numpy/util.py:363\u001b[0m, in \u001b[0;36m_promote_args\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    361\u001b[0m _check_arraylike(fun_name, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    362\u001b[0m _check_no_float0s(fun_name, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_promote_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_promote_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/jax/lib/python3.11/site-packages/jax/_src/numpy/util.py:256\u001b[0m, in \u001b[0;36m_promote_shapes\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mjax_numpy_rank_promotion \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    255\u001b[0m   _rank_promotion_warning_or_error(fun_name, shapes)\n\u001b[0;32m--> 256\u001b[0m result_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [_broadcast_to(arg, (\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m*\u001b[39m (result_rank \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(shp)) \u001b[38;5;241m+\u001b[39m shp)\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m arg, shp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, shapes)]\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/mambaforge/envs/jax/lib/python3.11/site-packages/jax/_src/lax/lax.py:161\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    159\u001b[0m result_shape \u001b[38;5;241m=\u001b[39m _try_broadcast_shapes(shape_list)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(512, 784), (512,)]"
=======
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 1.8727, Training Accuracy: 26.86%, Test accuracy: 32.50%\n",
      "Epoch 2/10, Training Loss: 1.5264, Training Accuracy: 33.51%, Test accuracy: 36.25%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m lr_scheduler \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mStepLR(optimizer, step_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m---> 26\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(model, train_loader, criterion, optimizer, device)\n\u001b[1;32m     27\u001b[0m     test_accuracy \u001b[39m=\u001b[39m evaluate(device, model, test_loader)\n\u001b[1;32m     28\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mEPOCHS\u001b[39m}\u001b[39;00m\u001b[39m, Training Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Training Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%, Test accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtest_accuracy\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      5\u001b[0m total \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfor\u001b[39;00m images, labels \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m      7\u001b[0m     images, labels \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/dlea/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/dlea/lib/python3.9/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/dlea/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/dlea/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/dlea/lib/python3.9/site-packages/torchvision/datasets/mnist.py:142\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    138\u001b[0m img, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[index], \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtargets[index])\n\u001b[1;32m    140\u001b[0m \u001b[39m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39m# to return a PIL Image\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mfromarray(img\u001b[39m.\u001b[39;49mnumpy(), mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mL\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(img)\n",
      "File \u001b[0;32m~/anaconda3/envs/dlea/lib/python3.9/site-packages/PIL/Image.py:3103\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3101\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mtostring()\n\u001b[0;32m-> 3103\u001b[0m \u001b[39mreturn\u001b[39;00m frombuffer(mode, size, obj, \u001b[39m\"\u001b[39;49m\u001b[39mraw\u001b[39;49m\u001b[39m\"\u001b[39;49m, rawmode, \u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dlea/lib/python3.9/site-packages/PIL/Image.py:3008\u001b[0m, in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2973\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrombuffer\u001b[39m(mode, size, data, decoder_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[1;32m   2974\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2975\u001b[0m \u001b[39m    Creates an image memory referencing pixel data in a byte buffer.\u001b[39;00m\n\u001b[1;32m   2976\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3005\u001b[0m \u001b[39m    .. versionadded:: 1.1.4\u001b[39;00m\n\u001b[1;32m   3006\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3008\u001b[0m     _check_size(size)\n\u001b[1;32m   3010\u001b[0m     \u001b[39m# may pass tuple instead of argument list\u001b[39;00m\n\u001b[1;32m   3011\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], \u001b[39mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/dlea/lib/python3.9/site-packages/PIL/Image.py:2886\u001b[0m, in \u001b[0;36m_check_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2884\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSize must be a tuple\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2885\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m-> 2886\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39;49m(size) \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m   2887\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSize must be a tuple of length 2\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2888\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
>>>>>>> origin/andrea
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "x, y = list(get_train_batches())[0]\n",
    "activations = jnp.reshape(x, (len(x), num_pixels))[0]\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=0.6).fit(activations.reshape(-1, 1))\n",
    "log_density = jnp.array([kde.score_samples(activations.reshape(-1, 1)[i].reshape(1, -1)) for i in range(activations.shape[0])])\n",
    "hist_activation = []\n",
    "hist_activation.append(list(activations))\n",
    "y = one_hot(y, num_labels)\n",
    "\n",
    "for w, b in params[:-1]:\n",
    "    outputs = jnp.dot(w, jnp.multiply(activations, jnp.exp(log_density))) + b\n",
    "    activations = relu(outputs)\n",
    "    hist_activation.append(list(activations))\n",
    "    print(log_density.shape)\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=0.6).fit(jnp.array(flattenList(hist_activation)).reshape(-1, 1))\n",
    "    log_density = jnp.array([kde.score_samples(activations.reshape(-1, 1)[i].reshape(1, -1)) for i in range(activations.shape[0])])\n",
    "    \n",
    "    \n",
    "\n",
    "final_w, final_b = params[-1]\n",
    "logits = jnp.dot(final_w, activations) + final_b\n",
    "logits - logsumexp(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 1)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations.reshape(-1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattenList(l):\n",
    "    # this function flatten a list\n",
    "    return [item for sublist in l for item in sublist]\n",
    "np.array(flattenList(hist_activation)).shape\n",
    "\n",
    "\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=0.6).fit(np.array(flattenList(hist_activation)).reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = 512\n",
    "log_density = [kde.score_samples(np.array(flattenList(hist_activation)).reshape(-1, 1)[i].reshape(-1, 1)) for i in range(neuron)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 1)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations.reshape(-1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b57134f0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAaxklEQVR4nO3df2xU97nn8c8Y7AESexxj7LGLIYaEkAZwVAqul4SSYgGOxEJAV/klXchFsBATFdw0WVdJCG21bskVjZJ1ye6qhWYVQooUYMNeoZs4sblpbSIcWMS2tbCvG+Bim4YrzxgTG4O/+webaSbY0DPM8NjD+yUdCc+cr8+Tk1HeOZ7h2OeccwIA4CZLsR4AAHBrIkAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMDESOsBvq6/v19nzpxRenq6fD6f9TgAAI+cc+rq6lJ+fr5SUga/zhlyATpz5owKCgqsxwAA3KBTp05p/Pjxgz4/5AKUnp4uSXpAD2ukUo2nAQB4dUl9+lj/FPnv+WASFqDq6mq98soram9vV1FRkV5//XXNnj37uuu+/LHbSKVqpI8AAcCw8//vMHq9t1ES8iGEd955RxUVFdq0aZM+/fRTFRUVaeHChTp79mwiDgcAGIYSEqCtW7dq9erVeuqpp/TNb35Tb7zxhsaMGaNf//rXiTgcAGAYinuALl68qMbGRpWWlv71ICkpKi0tVX19/VX79/b2KhwOR20AgOQX9wB9/vnnunz5snJzc6Mez83NVXt7+1X7V1VVKRAIRDY+AQcAtwbzv4haWVmpUCgU2U6dOmU9EgDgJoj7p+Cys7M1YsQIdXR0RD3e0dGhYDB41f5+v19+vz/eYwAAhri4XwGlpaVp5syZqqmpiTzW39+vmpoalZSUxPtwAIBhKiF/D6iiokIrVqzQt7/9bc2ePVuvvvqquru79dRTTyXicACAYSghAXr00Uf1l7/8RS+99JLa29t1//3368CBA1d9MAEAcOvyOeec9RBfFQ6HFQgENE9LuBMCAAxDl1yfarVPoVBIGRkZg+5n/ik4AMCtiQABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmIh7gF5++WX5fL6oberUqfE+DABgmBuZiG9633336YMPPvjrQUYm5DAAgGEsIWUYOXKkgsFgIr41ACBJJOQ9oBMnTig/P1+TJk3Sk08+qZMnTw66b29vr8LhcNQGAEh+cQ9QcXGxduzYoQMHDmjbtm1qbW3Vgw8+qK6urgH3r6qqUiAQiGwFBQXxHgkAMAT5nHMukQfo7OzUxIkTtXXrVq1ateqq53t7e9Xb2xv5OhwOq6CgQPO0RCN9qYkcDQCQAJdcn2q1T6FQSBkZGYPul/BPB2RmZmrKlClqbm4e8Hm/3y+/35/oMQAAQ0zC/x7Q+fPn1dLSory8vEQfCgAwjMQ9QM8++6zq6ur05z//Wb///e/1yCOPaMSIEXr88cfjfSgAwDAW9x/BnT59Wo8//rjOnTuncePG6YEHHlBDQ4PGjRsX70MBAIaxuAdo165d8f6WAIAkxL3gAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATCf+FdMCN8qWmeV+U4ovpWCOy7vC8pm1poec1nff2e17jbrvsec2mOf/L8xpJWplx1vOa++qf9Lxm/PL/63kNkgdXQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADDB3bCTzIg7vN/N+cI7gQRMEj+vTdnlec19sdxB+ya6JO93tv73y70JmGRgfW605zV/P+UTz2tqR431vKa/p8fzGgxNXAEBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACa4GWmySUv1vGTL3btjOtR/63jI85of5R2I6Vhe/fnSpZjWnejzfnPM7+/6B89rgg3eb0Y6ar/3m32OvHOC5zWS9I+13m8A2xjyfqz+nnOe1yB5cAUEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJjgZqRJ5nLHWc9rXiycFePRznte8bQeiPFYQ9edqrceYVDhb+XFtG5K6ijPa873+WM6Fm5dXAEBAEwQIACACc8BOnjwoBYvXqz8/Hz5fD7t3bs36nnnnF566SXl5eVp9OjRKi0t1YkTJ+I1LwAgSXgOUHd3t4qKilRdXT3g81u2bNFrr72mN954Q4cOHdJtt92mhQsXqqen54aHBQAkD88fQigrK1NZWdmAzznn9Oqrr+qFF17QkiVLJElvvvmmcnNztXfvXj322GM3Ni0AIGnE9T2g1tZWtbe3q7S0NPJYIBBQcXGx6usH/qRQb2+vwuFw1AYASH5xDVB7e7skKTc3N+rx3NzcyHNfV1VVpUAgENkKCgriORIAYIgy/xRcZWWlQqFQZDt16pT1SACAmyCuAQoGg5Kkjo6OqMc7Ojoiz32d3+9XRkZG1AYASH5xDVBhYaGCwaBqamoij4XDYR06dEglJSXxPBQAYJjz/Cm48+fPq7m5OfJ1a2urjh49qqysLE2YMEEbNmzQT3/6U919990qLCzUiy++qPz8fC1dujSecwMAhjnPATp8+LAeeuihyNcVFRWSpBUrVmjHjh167rnn1N3drTVr1qizs1MPPPCADhw4oFGjvN9bCgCQvHzOOWc9xFeFw2EFAgHN0xKN9KVajwMMGT6/95t9lnzSFdOxNoxt9Lzm4Y0bPK+5ffchz2sw9F1yfarVPoVCoWu+r2/+KTgAwK2JAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJjz/OgYANjpWz/S85oXs/xrTse79l//keU0hd7aGR1wBAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmuBkpYCBl1CjPa5aurvW8puYLv+c1kjT5pxc9r+mP6Ui4lXEFBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCY4GakgIGL/+E+z2teyP4fntc8/W9zPK+RpP7jf4ppHeAFV0AAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAluRgoYOFWadlOOU/vP98e07k7Vx3cQYABcAQEATBAgAIAJzwE6ePCgFi9erPz8fPl8Pu3duzfq+ZUrV8rn80VtixYtite8AIAk4TlA3d3dKioqUnV19aD7LFq0SG1tbZHt7bffvqEhAQDJx/OHEMrKylRWVnbNffx+v4LBYMxDAQCSX0LeA6qtrVVOTo7uuecerVu3TufOnRt0397eXoXD4agNAJD84h6gRYsW6c0331RNTY1+/vOfq66uTmVlZbp8+fKA+1dVVSkQCES2goKCeI8EABiC4v73gB577LHIn6dPn64ZM2Zo8uTJqq2t1fz586/av7KyUhUVFZGvw+EwEQKAW0DCP4Y9adIkZWdnq7m5ecDn/X6/MjIyojYAQPJLeIBOnz6tc+fOKS8vL9GHAgAMI55/BHf+/Pmoq5nW1lYdPXpUWVlZysrK0ubNm7V8+XIFg0G1tLToueee01133aWFCxfGdXAAwPDmOUCHDx/WQw89FPn6y/dvVqxYoW3btunYsWP6zW9+o87OTuXn52vBggX6yU9+Ir/fH7+pAQDDns8556yH+KpwOKxAIKB5WqKRvlTrcYCEuP+I9zVZI7s9r6mdNdb7gST19/TEtA6QpEuuT7Xap1AodM339bkXHADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEzE/VdyA7ca37eneV7zn7P/u+c1m8/O9byGu1pjKOMKCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwwc1IgRv0r8vSPa/pdv2e1/z+tVme19yhes9rgJuFKyAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQ3IwW+YuSdEzyv2fJ3/9Pzmg2fLfW85o7fcGNRJBeugAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAE9yMFPiKS8FMz2sWjwl7XvPD393tec0kfe55DTCUcQUEADBBgAAAJjwFqKqqSrNmzVJ6erpycnK0dOlSNTU1Re3T09Oj8vJyjR07VrfffruWL1+ujo6OuA4NABj+PAWorq5O5eXlamho0Pvvv6++vj4tWLBA3d3dkX02btyo9957T7t371ZdXZ3OnDmjZcuWxX1wAMDw5ulDCAcOHIj6eseOHcrJyVFjY6Pmzp2rUCikX/3qV9q5c6e+973vSZK2b9+ue++9Vw0NDfrOd74Tv8kBAMPaDb0HFAqFJElZWVmSpMbGRvX19am0tDSyz9SpUzVhwgTV1w/864R7e3sVDoejNgBA8os5QP39/dqwYYPmzJmjadOmSZLa29uVlpamzMzMqH1zc3PV3t4+4PepqqpSIBCIbAUFBbGOBAAYRmIOUHl5uY4fP65du3bd0ACVlZUKhUKR7dSpUzf0/QAAw0NMfxF1/fr12r9/vw4ePKjx48dHHg8Gg7p48aI6OzujroI6OjoUDAYH/F5+v19+vz+WMQAAw5inKyDnnNavX689e/boww8/VGFhYdTzM2fOVGpqqmpqaiKPNTU16eTJkyopKYnPxACApODpCqi8vFw7d+7Uvn37lJ6eHnlfJxAIaPTo0QoEAlq1apUqKiqUlZWljIwMPfPMMyopKeETcACAKJ4CtG3bNknSvHnzoh7fvn27Vq5cKUn6xS9+oZSUFC1fvly9vb1auHChfvnLX8ZlWABA8vAUIOfcdfcZNWqUqqurVV1dHfNQgJXW/3ib9QjALYN7wQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMBETL8RFUhWl77R63nNgS/GeF4zZdtpz2sueV4BDG1cAQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJrgZKZLSiMxATOv+y3f2eF7zfy5M9Lzm0menPK8Bkg1XQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACW5GiqTUP3l8TOv+7vaPPK/5eW9OTMcCbnVcAQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJrgZKZJSymftMa373xduj/MkAAbDFRAAwAQBAgCY8BSgqqoqzZo1S+np6crJydHSpUvV1NQUtc+8efPk8/mitrVr18Z1aADA8OcpQHV1dSovL1dDQ4Pef/999fX1acGCBeru7o7ab/Xq1Wpra4tsW7ZsievQAIDhz9OHEA4cOBD19Y4dO5STk6PGxkbNnTs38viYMWMUDAbjMyEAICnd0HtAoVBIkpSVlRX1+FtvvaXs7GxNmzZNlZWVunDhwqDfo7e3V+FwOGoDACS/mD+G3d/frw0bNmjOnDmaNm1a5PEnnnhCEydOVH5+vo4dO6bnn39eTU1Nevfddwf8PlVVVdq8eXOsYwAAhqmYA1ReXq7jx4/r448/jnp8zZo1kT9Pnz5deXl5mj9/vlpaWjR58uSrvk9lZaUqKioiX4fDYRUUFMQ6FgBgmIgpQOvXr9f+/ft18OBBjR8//pr7FhcXS5Kam5sHDJDf75ff749lDADAMOYpQM45PfPMM9qzZ49qa2tVWFh43TVHjx6VJOXl5cU0IAAgOXkKUHl5uXbu3Kl9+/YpPT1d7e1XbncSCAQ0evRotbS0aOfOnXr44Yc1duxYHTt2TBs3btTcuXM1Y8aMhPwDAACGJ08B2rZtm6Qrf9n0q7Zv366VK1cqLS1NH3zwgV599VV1d3eroKBAy5cv1wsvvBC3gQEAycHzj+CupaCgQHV1dTc0EADg1sDdsJGcrvM/S4M50On9R8X/2jU2hiP9WwxrgOTCzUgBACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABPcjBRJ6fK5f49pXcusWFZxY1EgFlwBAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMDHk7gXnnJMkXVKf5IyHAQB4dkl9kv763/PBDLkAdXV1SZI+1j8ZTwIAuBFdXV0KBAKDPu9z10vUTdbf368zZ84oPT1dPp8v6rlwOKyCggKdOnVKGRkZRhPa4zxcwXm4gvNwBefhiqFwHpxz6urqUn5+vlJSBn+nZ8hdAaWkpGj8+PHX3CcjI+OWfoF9ifNwBefhCs7DFZyHK6zPw7WufL7EhxAAACYIEADAxLAKkN/v16ZNm+T3+61HMcV5uILzcAXn4QrOwxXD6TwMuQ8hAABuDcPqCggAkDwIEADABAECAJggQAAAE8MmQNXV1brzzjs1atQoFRcX65NPPrEe6aZ7+eWX5fP5orapU6daj5VwBw8e1OLFi5Wfny+fz6e9e/dGPe+c00svvaS8vDyNHj1apaWlOnHihM2wCXS987By5cqrXh+LFi2yGTZBqqqqNGvWLKWnpysnJ0dLly5VU1NT1D49PT0qLy/X2LFjdfvtt2v58uXq6Ogwmjgx/pbzMG/evKteD2vXrjWaeGDDIkDvvPOOKioqtGnTJn366acqKirSwoULdfbsWevRbrr77rtPbW1tke3jjz+2Hinhuru7VVRUpOrq6gGf37Jli1577TW98cYbOnTokG677TYtXLhQPT09N3nSxLreeZCkRYsWRb0+3n777Zs4YeLV1dWpvLxcDQ0Nev/999XX16cFCxaou7s7ss/GjRv13nvvaffu3aqrq9OZM2e0bNkyw6nj7285D5K0evXqqNfDli1bjCYehBsGZs+e7crLyyNfX7582eXn57uqqirDqW6+TZs2uaKiIusxTElye/bsiXzd39/vgsGge+WVVyKPdXZ2Or/f795++22DCW+Or58H55xbsWKFW7Jkick8Vs6ePeskubq6OufclX/3qampbvfu3ZF9/vjHPzpJrr6+3mrMhPv6eXDOue9+97vu+9//vt1Qf4MhfwV08eJFNTY2qrS0NPJYSkqKSktLVV9fbziZjRMnTig/P1+TJk3Sk08+qZMnT1qPZKq1tVXt7e1Rr49AIKDi4uJb8vVRW1urnJwc3XPPPVq3bp3OnTtnPVJChUIhSVJWVpYkqbGxUX19fVGvh6lTp2rChAlJ/Xr4+nn40ltvvaXs7GxNmzZNlZWVunDhgsV4gxpyNyP9us8//1yXL19Wbm5u1OO5ubn605/+ZDSVjeLiYu3YsUP33HOP2tratHnzZj344IM6fvy40tPTrccz0d7eLkkDvj6+fO5WsWjRIi1btkyFhYVqaWnRj370I5WVlam+vl4jRoywHi/u+vv7tWHDBs2ZM0fTpk2TdOX1kJaWpszMzKh9k/n1MNB5kKQnnnhCEydOVH5+vo4dO6bnn39eTU1Nevfddw2njTbkA4S/Kisri/x5xowZKi4u1sSJE/Xb3/5Wq1atMpwMQ8Fjjz0W+fP06dM1Y8YMTZ48WbW1tZo/f77hZIlRXl6u48eP3xLvg17LYOdhzZo1kT9Pnz5deXl5mj9/vlpaWjR58uSbPeaAhvyP4LKzszVixIirPsXS0dGhYDBoNNXQkJmZqSlTpqi5udl6FDNfvgZ4fVxt0qRJys7OTsrXx/r167V//3599NFHUb++JRgM6uLFi+rs7IzaP1lfD4Odh4EUFxdL0pB6PQz5AKWlpWnmzJmqqamJPNbf36+amhqVlJQYTmbv/PnzamlpUV5envUoZgoLCxUMBqNeH+FwWIcOHbrlXx+nT5/WuXPnkur14ZzT+vXrtWfPHn344YcqLCyMen7mzJlKTU2Nej00NTXp5MmTSfV6uN55GMjRo0claWi9Hqw/BfG32LVrl/P7/W7Hjh3uD3/4g1uzZo3LzMx07e3t1qPdVD/4wQ9cbW2ta21tdb/73e9caWmpy87OdmfPnrUeLaG6urrckSNH3JEjR5wkt3XrVnfkyBH32WefOeec+9nPfuYyMzPdvn373LFjx9ySJUtcYWGh++KLL4wnj69rnYeuri737LPPuvr6etfa2uo++OAD961vfcvdfffdrqenx3r0uFm3bp0LBAKutrbWtbW1RbYLFy5E9lm7dq2bMGGC+/DDD93hw4ddSUmJKykpMZw6/q53Hpqbm92Pf/xjd/jwYdfa2ur27dvnJk2a5ObOnWs8ebRhESDnnHv99dfdhAkTXFpamps9e7ZraGiwHumme/TRR11eXp5LS0tz3/jGN9yjjz7qmpubrcdKuI8++shJumpbsWKFc+7KR7FffPFFl5ub6/x+v5s/f75ramqyHToBrnUeLly44BYsWODGjRvnUlNT3cSJE93q1auT7n/SBvrnl+S2b98e2eeLL75wTz/9tLvjjjvcmDFj3COPPOLa2trshk6A652HkydPurlz57qsrCzn9/vdXXfd5X74wx+6UChkO/jX8OsYAAAmhvx7QACA5ESAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmPh/WG9cuq7k9OkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(train_images[109].reshape(32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
=======
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLP().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = evaluate(device, model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Test accuracy: {test_accuracy:.2f}%')\n",
    "    lr_scheduler.step()\n"
   ]
>>>>>>> origin/andrea
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "Python 3 (ipykernel)",
=======
   "display_name": "dlea",
>>>>>>> origin/andrea
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "747470a23b547734b54ad941aca0c642f18c60741ffaa8ef37d60afac2b472eb"
   }
  }
=======
   "version": "3.9.16"
  },
  "orig_nbformat": 4
>>>>>>> origin/andrea
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
